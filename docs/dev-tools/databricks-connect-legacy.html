

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en-US" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en-US"> <!--<![endif]-->

<head>
  <!-- cookie consent -->
  
    <!-- Combined Onetrust and Rudderstack Implementation Scripts -->
    <!-- Onetrust Initialization -->
    <script type="text/javascript" src="https://cdn.cookielaw.org/consent/92466579-1717-44d3-809d-a05fb02843ed-test/OtAutoBlock.js"></script>
    <script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="92466579-1717-44d3-809d-a05fb02843ed-test"></script>
    <link rel="stylesheet" id="db-onetrust-style" href="https://www.databricks.com/wp-content/uploads/db_onetrust.css" media="all" />
    <!-- Setting Rudderstack Write Key -->
    <script>window.rudderstackKey = "2SOR9fvSr5Fi6tN2ihPbVHnX1SZ" </script>
    <!-- Rudderstack Initialization + Onetrust Integration + Rudderstack Custom Events -->
    <script type="text/javascript" src="https://www.databricks.com/sites/default/files/rudderstack/v1/db-rudderstack-events.js"></script>

  <!-- cookie consent -->

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="X-UA-Compatible" content="IE=9" />
  <meta content="Learn how to use Databricks Connect to connect your favorite IDE, notebook server, or custom applications to Databricks clusters." name="description" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0">
  <meta property="og:image" content="https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:title" content="Databricks Connect for Databricks Runtime 12.2 LTS and below">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://docs.databricks.com">
  <meta property="og:description" content="" id="og-description">
  <meta name="twitter:image" content="https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png">
  <meta name="twitter:site" content="@databricks">
  <meta name="twitter:creator" content="@databricks">
  <meta property="twitter:description" content="">
  
  <title>Databricks Connect for Databricks Runtime 12.2 LTS and below &#124; Databricks on AWS</title>
  
  
  <link rel="canonical" href="https://docs.databricks.com/en/dev-tools/databricks-connect-legacy.html">
  <!-- Start hreflang tag -->
  <link rel="alternate" hreflang="en" href="https://docs.databricks.com/en/dev-tools/databricks-connect-legacy.html" />
<link rel="alternate" hreflang="x-default" href="https://docs.databricks.com/en/dev-tools/databricks-connect-legacy.html" />
  <!-- End hreflang tag -->
  
  
  <link rel="shortcut icon" href="../_static/favicon.ico" />
  

  

  

  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;
j.setAttributeNode(d.createAttribute('data-ot-ignore'));
f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-T85FQ33');</script>
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;
j.setAttributeNode(d.createAttribute('data-ot-ignore'));
f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-TWTKQQ');</script>
    
  <!-- End Google Tag Manager -->


  <!-- MaxMind / GEO IP -->
  <script src="//js.maxmind.com/js/apis/geoip2/v2.1/geoip2.js" type="text/javascript"></script>
  <!-- End MaxMind / GEO IP -->

  
  
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600&display=swap" rel="stylesheet">
  <link rel="preload" href="../_static/fonts/DMSans-Bold.ttf" as="font">
  <link rel="preload" href="../_static/fonts/DMSans-Regular.ttf" as="font">
  <link rel="preload" href="../_static/fonts/DMMono-Regular.ttf" as="font">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/cloud-provider-selector.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/translation-selector.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/searchunify/main.css" type="text/css" />

  
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="top" title="Databricks on AWS" href="../index.html" /> 
</head>

<body class="wy-body-for-nav" role="document">

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T85FQ33"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TWTKQQ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  
  <nav class="wy-nav-top header su_header" role="navigation" aria-label="top navigation">
    
<nav class="wy-nav-top header su_header" role="navigation" aria-label="top navigation">
  <div class="container-logo">
    <ul class="mobile-menu-toggle">
        <li class="menu-toggle">
            <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
            
            <a href="https://www.databricks.com/" class="wy-nav-top-logo"><img src="../_static/small-scale-lockup-full-color-rgb.svg" width="137" height="21"
              alt="Databricks" /></a>   
               
              </li>
    </ul>
    <ul class="su_nav-menu">
      <li class="menu-toggle">
        <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
        
          
        
        <a href="https://www.databricks.com/" class="wy-nav-top-logo"><img src="../_static/small-scale-lockup-full-color-rgb.svg" width="137" height="21"
            alt="Databricks" /></a></li>
        <!-- 
<li><a href="https://help.databricks.com/s/">Help Center</a></li>
<li class="active"><a href="https://docs.databricks.com/en/">Documentation</a></li>
<li><a href="https://kb.databricks.com/">Knowledge Base</a></li>
 -->
    </ul>
  </div>
  <div class="su_nav-right">
    <ul class="su_link-mobile">
  <!-- Mobile header code can go here -->
</ul>
<ul class="right-try-list">
   
</ul>
  </div>
</nav>
  </nav>

  <div class="su_sub-header">
    <div class="container">
      <div class="su_sub-header-inner">
        <!-- <div class="su_subnav-menu-right">
  <div id="auto" style="width: 100%;">
    <div ng-controller="SearchautoController">
      <div bind-html-compile="autocompleteHtml">
        <form class="su__search-box-1" disabled="disabled">
          <input class="su__search-input" type="search" name="Search box" id="su__search-b" placeholder="Search Documentation" disabled="disabled"/>
          <button class="su__search-button" type="submit" class="button button-success" disabled="disabled">
            <svg width="24" height="24" viewBox="0 0 24 24">
              <path
                d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"
                fill="#333"></path>
            </svg>
          </button>
        </form>
      </div>
    </div>
  </div>
</div> -->
        <div class="search-lng-gap"></div>
        <div style="margin-left: 16px; margin-right: 16px;">
          <!-- <select name="lng selector" id="lng-selector">
    <option value="../../en/dev-tools/databricks-connect-legacy.html" class="notranslate">English</option>
    <option value="../../ja/dev-tools/databricks-connect-legacy.html" class="notranslate">日本語</option>
    <option value="../../pt/dev-tools/databricks-connect-legacy.html" class="notranslate">Português (Brasil)</option>
</select> -->
        </div>
        <div class="cloud-selector-container">
          <!-- <select name="cloud provider selector" id="cloud-provider-selector">
    <option value="aws" selected class="notranslate">
        Amazon Web Services
    </option>
    <option value="azure"  class="notranslate">
        Microsoft Azure
    </option>
    <option value="gcp"  class="notranslate">
        Google Cloud Platform
    </option>
</select> -->
        </div>
      </div>
    </div>
  </div>
  <page class="js-page-container">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side su_nav-side">
<div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    

    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../index.html" class="main-navigation-home">Databricks on AWS</a>
    

    
      

      
        <p class="caption"><span class="caption-text">Load &amp; manage data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../rag-temp/index.html">RAG Studio</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    
  <p class="build_info notranslate"data-last-edit="December 23, 2023">
    Updated Jan 11, 2024
  </p>
<script>
  window.addEventListener('DOMContentLoaded',function(){
    var h1=document.querySelector('h1');
    var bi=document.querySelector('[data-last-edit]');
    if(h1 && bi){
      var ver = document.createElement('p');
      ver.className = 'version_info';
      ver.textContent = bi.getAttribute('data-last-edit');
      h1.parentElement.insertBefore(ver, h1.nextElementSibling);
    }
  });
</script>

    <p>
      
        <a id='feedbacklink' href="mailto:doc-feedback@databricks.com?subject=Documentation Feedback">Send us feedback</a>
      
    </p>
  </div>
</div>
</nav>
    
    
<main class="wy-grid-for-nav su_nav-grid">
  <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
    <div class="wy-nav-content su__nav_content">
      <div class="rst-content">
        





<div role="navigation" aria-label="breadcrumbs navigation" class="wy-breadcrumbs-wrapper">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>Databricks Connect for Databricks Runtime 12.2 LTS and below</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>
</div>
        
        <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
          <div itemprop="articleBody">
            
    
  <div class="section" id="databricks-connect-for-databricks-runtime-122-lts-and-below">
<span id="databricks-connect-v1"></span><h1>Databricks Connect for Databricks Runtime 12.2 LTS and below<a class="headerlink" href="#databricks-connect-for-databricks-runtime-122-lts-and-below" title="Permalink to this headline"> </a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Databricks Connect recommends that you use <a class="reference internal" href="databricks-connect/index.html"><span class="doc">Databricks Connect for Databricks Runtime 13.0 and above</span></a> instead.</p>
<p>Databricks plans no new feature work for Databricks Connect for Databricks Runtime 12.2 LTS and below.</p>
</div>
<p>Databricks Connect allows you to connect popular IDEs such as Visual Studio Code and PyCharm, notebook servers, and other custom applications to Databricks clusters.</p>
<p>This article explains how Databricks Connect works, walks you through the steps to get started with Databricks Connect, explains how to troubleshoot issues that may arise when using Databricks Connect, and differences between running using Databricks Connect versus running in a Databricks notebook.</p>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"> </a></h2>
<p>Databricks Connect is a client library for the Databricks Runtime. It allows you to write jobs using Spark APIs and run them remotely on a Databricks cluster instead of in the local Spark session.</p>
<p>For example, when you run the DataFrame command <code class="docutils literal notranslate"><span class="pre">spark.read.format(...).load(...).groupBy(...).agg(...).show()</span></code> using Databricks Connect, the logical representation of the command is sent to the Spark server running in Databricks for execution on the remote cluster.</p>
<p>With Databricks Connect, you can:</p>
<ul class="simple">
<li><p>Run large-scale Spark jobs from any Python, R, Scala, or Java application. Anywhere you can <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">pyspark</span></code>, <code class="docutils literal notranslate"><span class="pre">require(SparkR)</span></code> or <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">org.apache.spark</span></code>, you can now run Spark jobs directly from your application, without needing to install any IDE plugins or use Spark submission scripts.</p></li>
<li><p>Step through and debug code in your IDE even when working with a remote cluster.</p></li>
<li><p>Iterate quickly when developing libraries. You do not need to restart the cluster after changing Python or Java library dependencies in Databricks Connect, because each client session is isolated from each other in the cluster.</p></li>
<li><p>Shut down idle clusters without losing work. Because the client application is decoupled from the cluster, it is unaffected by cluster restarts or upgrades, which would normally cause you to lose all the variables, RDDs, and DataFrame objects defined in a notebook.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For Python development with SQL queries, Databricks recommends that you use the <a class="reference internal" href="python-sql-connector.html"><span class="doc">Databricks SQL Connector for Python</span></a> instead of Databricks Connect. the Databricks SQL Connector for Python is easier to set up than Databricks Connect. Also, Databricks Connect parses and plans jobs runs on your local machine, while jobs run on remote compute resources. This can make it especially difficult to debug runtime errors. The Databricks SQL Connector for Python submits SQL queries directly to remote compute resources and fetches results.</p>
</div>
</div>
<div class="section" id="requirements">
<span id="requirements-legacy"></span><h2>Requirements<a class="headerlink" href="#requirements" title="Permalink to this headline"> </a></h2>
<p>This section lists the requirements for Databricks Connect.</p>
<ul>
<li><p>Only the following Databricks Runtime versions are supported:</p>
<ul class="simple">
<li><p>Databricks Runtime 12.2 LTS ML, Databricks Runtime 12.2 LTS</p></li>
<li><p>Databricks Runtime 11.3 LTS ML, Databricks Runtime 11.3 LTS</p></li>
<li><p>Databricks Runtime 10.4 LTS ML, Databricks Runtime 10.4 LTS</p></li>
<li><p>Databricks Runtime 9.1 LTS ML, Databricks Runtime 9.1 LTS</p></li>
<li><p>Databricks Runtime 7.3 LTS</p></li>
</ul>
</li>
<li><p>You must install Python 3 on your development machine, and the minor version of your client Python installation must be the same as the minor Python version of your Databricks cluster. The following table shows the Python version installed with each Databricks Runtime.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 60%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Databricks Runtime version</p></th>
<th class="head"><p>Python version</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p> 12.2 LTS ML, 12.2 LTS</p></td>
<td><p> 3.9</p></td>
</tr>
<tr class="row-odd"><td><p> 11.3 LTS ML, 11.3 LTS</p></td>
<td><p> 3.9</p></td>
</tr>
<tr class="row-even"><td><p> 10.4 LTS ML, 10.4 LTS</p></td>
<td><p> 3.8</p></td>
</tr>
<tr class="row-odd"><td><p> 9.1 LTS ML, 9.1 LTS</p></td>
<td><p> 3.8</p></td>
</tr>
<tr class="row-even"><td><p> 7.3 LTS</p></td>
<td><p>  3.7</p></td>
</tr>
</tbody>
</table>
<p>Databricks strongly recommends that you have a Python <em>virtual environment</em> activated for each Python version that you use with Databricks Connect. Python virtual environments help to make sure that you are using the correct versions of Python and Databricks Connect together. This can help to reduce the time spent resolving related technical issues.</p>
<p>For example, if you’re using <a class="reference external" href="https://docs.python.org/3/library/venv.html">venv</a> on your development machine and your cluster is running Python 3.9, you must create a <code class="docutils literal notranslate"><span class="pre">venv</span></code> environment with that version. The following example command generates the scripts to activate a <code class="docutils literal notranslate"><span class="pre">venv</span></code> environment with Python 3.9, and this command then places those scripts within a hidden folder named <code class="docutils literal notranslate"><span class="pre">.venv</span></code> within the current working directory:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Linux and macOS</span>
python3.9<span class="w"> </span>-m<span class="w"> </span>venv<span class="w"> </span>./.venv

<span class="c1"># Windows</span>
python3.9<span class="w"> </span>-m<span class="w"> </span>venv<span class="w"> </span>.<span class="se">\.</span>venv
</pre></div>
</div>
<p>To use these scripts to activate this <code class="docutils literal notranslate"><span class="pre">venv</span></code> environment, see <a class="reference external" href="https://docs.python.org/3/library/venv.html#how-venvs-work">How venvs work</a>.</p>
<p>As another example, if you’re using <a class="reference external" href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html">Conda</a> on your development machine and your cluster is running Python 3.9, you must create a Conda environment with that version, for example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>create<span class="w"> </span>--name<span class="w"> </span>dbconnect<span class="w"> </span><span class="nv">python</span><span class="o">=</span><span class="m">3</span>.9
</pre></div>
</div>
<p>To activate the Conda environment with this environment name, run <code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">activate</span> <span class="pre">dbconnect</span></code>.</p>
</li>
<li><p>The Databricks Connect major and minor package version must always match your Databricks Runtime version. Databricks recommends that you always use the most recent package of Databricks Connect that matches your Databricks Runtime version. For example, when you use a Databricks Runtime 12.2 LTS cluster, you must also use the <code class="docutils literal notranslate"><span class="pre">databricks-connect==12.2.*</span></code> package.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference internal" href="../release-notes/dbconnect/index.html"><span class="doc">Databricks Connect release notes</span></a> for a list of available Databricks Connect releases and maintenance updates.</p>
</div>
</li>
<li><p>Java Runtime Environment (JRE) 8. The client has been tested with the OpenJDK 8 JRE. The client does not support Java 11.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>On Windows, if you see an error that Databricks Connect cannot find <code class="docutils literal notranslate"><span class="pre">winutils.exe</span></code>, see <a class="reference internal" href="#cannot-find-winutilsexe-on-windows-legacy"><span class="std std-ref">Cannot find winutils.exe on Windows</span></a>.</p>
</div>
</div>
<div class="section" id="set-up-the-client">
<span id="set-up-legacy"></span><h2>Set up the client<a class="headerlink" href="#set-up-the-client" title="Permalink to this headline"> </a></h2>
<p>Complete the following steps to set up the local client for Databricks Connect.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Before you begin to set up the local Databricks Connect client, you must <a class="reference internal" href="#requirements-legacy"><span class="std std-ref">meet the requirements</span></a> for Databricks Connect.</p>
</div>
<div class="section" id="step-1-install-the-databricks-connect-client">
<h3>Step 1: Install the Databricks Connect client<a class="headerlink" href="#step-1-install-the-databricks-connect-client" title="Permalink to this headline"> </a></h3>
<ol class="arabic">
<li><p>With your virtual environment activated, uninstall PySpark, if it is already installed, by running the <code class="docutils literal notranslate"><span class="pre">uninstall</span></code> command. This is required because the <code class="docutils literal notranslate"><span class="pre">databricks-connect</span></code> package conflicts with PySpark. For details, see <a class="reference internal" href="#conflicting-pyspark-installations-legacy"><span class="std std-ref">Conflicting PySpark installations</span></a>. To check whether PySpark is already installed, run the <code class="docutils literal notranslate"><span class="pre">show</span></code> command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Is PySpark already installed?</span>
pip3<span class="w"> </span>show<span class="w"> </span>pyspark

<span class="c1"># Uninstall PySpark</span>
pip3<span class="w"> </span>uninstall<span class="w"> </span>pyspark
</pre></div>
</div>
</li>
<li><p>With your virtual environment still activated, install the Databricks Connect client by running the <code class="docutils literal notranslate"><span class="pre">install</span></code> command. Use the <code class="docutils literal notranslate"><span class="pre">--upgrade</span></code> option to upgrade any existing client installation to the specified version.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip3<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span><span class="s2">&quot;databricks-connect==12.2.*&quot;</span><span class="w">  </span><span class="c1"># Or X.Y.* to match your cluster version.</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Databricks recommends that you append the “dot-asterisk” notation to specify <code class="docutils literal notranslate"><span class="pre">databricks-connect==X.Y.*</span></code> instead of <code class="docutils literal notranslate"><span class="pre">databricks-connect=X.Y</span></code>, to make sure that the most recent package is installed.</p>
</div>
</li>
</ol>
</div>
<div class="section" id="step-2-configure-connection-properties">
<h3>Step 2: Configure connection properties<a class="headerlink" href="#step-2-configure-connection-properties" title="Permalink to this headline"> </a></h3>
<ol class="arabic">
<li><p>Collect the following configuration properties.</p>
<ul>
<li><p>The Databricks <a class="reference internal" href="../workspace/workspace-details.html#workspace-url"><span class="std std-ref">workspace URL</span></a>.</p></li>
<li><p>Your Databricks <a class="reference internal" href="auth/pat.html"><span class="doc">personal access token</span></a>.</p></li>
<li><p>The ID of your cluster. You can obtain the cluster ID from the URL. Here the cluster ID is <code class="docutils literal notranslate"><span class="pre">0304-201045-hoary804</span></code>.</p>
<div class="figure align-default">
<img alt="Cluster ID 2" src="../_images/cluster-id-aws.png" />
</div>
</li>
<li><p>The port that Databricks Connect connects to on your cluster. The default port is <code class="docutils literal notranslate"><span class="pre">15001</span></code>.</p></li>
</ul>
</li>
<li><p>Configure the connection as follows.</p>
<p>You can use the CLI, SQL configs, or environment variables. The precedence of configuration methods from highest to lowest is: SQL config keys, CLI, and environment variables.</p>
<ul>
<li><p>CLI</p>
<ol class="loweralpha">
<li><p>Run <code class="docutils literal notranslate"><span class="pre">databricks-connect</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>databricks-connect<span class="w"> </span>configure
</pre></div>
</div>
<p>The license displays:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Copyright</span> <span class="p">(</span><span class="mi">2018</span><span class="p">)</span> <span class="n">Databricks</span><span class="p">,</span> <span class="n">Inc</span><span class="o">.</span>

<span class="n">This</span> <span class="n">library</span> <span class="p">(</span><span class="n">the</span> <span class="s2">&quot;Software&quot;</span><span class="p">)</span> <span class="n">may</span> <span class="ow">not</span> <span class="n">be</span> <span class="n">used</span> <span class="k">except</span> <span class="ow">in</span> <span class="n">connection</span> <span class="k">with</span> <span class="n">the</span>
<span class="n">Licensee</span><span class="s1">&#39;s use of the Databricks Platform Services pursuant to an Agreement</span>
  <span class="o">...</span>
</pre></div>
</div>
</li>
<li><p>Accept the license and supply configuration values. For <strong>Databricks Host</strong> and <strong>Databricks Token</strong>, enter the workspace URL and the personal access token you noted in Step 1.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Do you accept the above agreement? [y/N] y
Set new config values (leave input empty to accept default):
Databricks Host [no current value, must start with https://]: &lt;databricks-url&gt;
Databricks Token [no current value]: &lt;databricks-token&gt;
Cluster ID (e.g., 0921-001415-jelly628) [no current value]: &lt;cluster-id&gt;
Org ID (Azure-only, see ?o=orgId in URL) [0]: &lt;org-id&gt;
Port [15001]: &lt;port&gt;
</pre></div>
</div>
</li>
</ol>
</li>
<li><p>SQL configs or environment variables. The following table shows the SQL config keys and the environment variables that correspond to the configuration properties you noted in Step 1. To set a SQL config key, use <code class="docutils literal notranslate"><span class="pre">sql(&quot;set</span> <span class="pre">config=value&quot;)</span></code>. For example: <code class="docutils literal notranslate"><span class="pre">sql(&quot;set</span> <span class="pre">spark.databricks.service.clusterId=0304-201045-abcdefgh&quot;)</span></code>.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 21%" />
<col style="width: 40%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>SQL config key</p></th>
<th class="head"><p>Environment variable name</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Databricks Host</p></td>
<td><p>spark.databricks.service.address</p></td>
<td><p>DATABRICKS_ADDRESS</p></td>
</tr>
<tr class="row-odd"><td><p>Databricks Token</p></td>
<td><p>spark.databricks.service.token</p></td>
<td><p>DATABRICKS_API_TOKEN</p></td>
</tr>
<tr class="row-even"><td><p>Cluster ID</p></td>
<td><p>spark.databricks.service.clusterId</p></td>
<td><p>DATABRICKS_CLUSTER_ID</p></td>
</tr>
<tr class="row-odd"><td><p>Org ID</p></td>
<td><p>spark.databricks.service.orgId</p></td>
<td><p>DATABRICKS_ORG_ID</p></td>
</tr>
<tr class="row-even"><td><p>Port</p></td>
<td><p>spark.databricks.service.port</p></td>
<td><p>DATABRICKS_PORT</p></td>
</tr>
</tbody>
</table>
</li>
</ul>
</li>
<li><p>With your virtual environment still activated, test connectivity to Databricks as follows.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>databricks-connect<span class="w"> </span><span class="nb">test</span>
</pre></div>
</div>
<p>If the cluster you configured is not running, the test starts the cluster which will remain running until its configured autotermination time. The output should look similar to the following:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>* PySpark is installed at /.../.../pyspark
* Checking java version
java version &quot;1.8...&quot;
Java(TM) SE Runtime Environment (build 1.8...)
Java HotSpot(TM) 64-Bit Server VM (build 25..., mixed mode)
* Testing scala command
../../.. ..:..:.. WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark&#39;s default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to &quot;WARN&quot;.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
../../.. ..:..:.. WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
../../.. ..:..:.. WARN SparkServiceRPCClient: Now tracking server state for 5ab..., invalidating prev state
../../.. ..:..:.. WARN SparkServiceRPCClient: Syncing 129 files (176036 bytes) took 3003 ms
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  &#39;_/
   /___/ .__/\_,_/_/ /_/\_\   version 2...
      /_/

Using Scala version 2.... (Java HotSpot(TM) 64-Bit Server VM, Java 1.8...)
Type in expressions to have them evaluated.
Type :help for more information.

scala&gt; spark.range(100).reduce(_ + _)
Spark context Web UI available at https://...
Spark context available as &#39;sc&#39; (master = local[*], app id = local-...).
Spark session available as &#39;spark&#39;.
View job details at &lt;databricks-url&gt;/?o=0#/setting/clusters/&lt;cluster-id&gt;/sparkUi
View job details at &lt;databricks-url&gt;?o=0#/setting/clusters/&lt;cluster-id&gt;/sparkUi
res0: Long = 4950

scala&gt; :quit

* Testing python command
../../.. ..:..:.. WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark&#39;s default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to &quot;WARN&quot;.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
../../.. ..:..:.. WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
../../.. ..:..:.. WARN SparkServiceRPCClient: Now tracking server state for 5ab.., invalidating prev state
View job details at &lt;databricks-url&gt;/?o=0#/setting/clusters/&lt;cluster-id&gt;/sparkUi
</pre></div>
</div>
</li>
<li><p>If no connection-related errors are shown (<code class="docutils literal notranslate"><span class="pre">WARN</span></code> messages are okay), then you have successfully connected.</p></li>
</ol>
</div>
</div>
<div class="section" id="use-databricks-connect">
<span id="use-legacy"></span><h2>Use Databricks Connect<a class="headerlink" href="#use-databricks-connect" title="Permalink to this headline"> </a></h2>
<p>The section describes how to configure your preferred IDE or notebook server to use the client for Databricks Connect.</p>
<div class="contents local topic" id="in-this-section">
<p class="topic-title first">In this section:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#jupyterlab" id="id4">JupyterLab</a></p></li>
<li><p><a class="reference internal" href="#classic-jupyter-notebook" id="id5">Classic Jupyter Notebook</a></p></li>
<li><p><a class="reference internal" href="#pycharm" id="id6">PyCharm</a></p></li>
<li><p><a class="reference internal" href="#sparkr-and-rstudio-desktop" id="id7">SparkR and RStudio Desktop</a></p></li>
<li><p><a class="reference internal" href="#sparklyr-and-rstudio-desktop" id="id8">sparklyr and RStudio Desktop</a></p></li>
<li><p><a class="reference internal" href="#intellij-scala-or-java" id="id9">IntelliJ (Scala or Java)</a></p></li>
<li><p><a class="reference internal" href="#pydev-with-eclipse" id="id10">PyDev with Eclipse</a></p></li>
<li><p><a class="reference internal" href="#eclipse" id="id11">Eclipse</a></p></li>
<li><p><a class="reference internal" href="#sbt" id="id12">SBT</a></p></li>
<li><p><a class="reference internal" href="#spark-shell" id="id13">Spark shell</a></p></li>
</ul>
</div>
<div class="section" id="jupyterlab">
<h3><a class="toc-backref" href="#id4">JupyterLab</a><a class="headerlink" href="#jupyterlab" title="Permalink to this headline"> </a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Before you begin to use Databricks Connect, you must <a class="reference internal" href="#requirements-legacy"><span class="std std-ref">meet the requirements</span></a> and <a class="reference internal" href="#set-up-legacy"><span class="std std-ref">set up the client</span></a> for Databricks Connect.</p>
</div>
<p>To use Databricks Connect with <a class="reference external" href="https://jupyterlab.readthedocs.io/en/stable/">JupyterLab</a> and Python, follow these instructions.</p>
<ol class="arabic">
<li><p>To install JupyterLab, with your Python virtual environment activated, run the following command from your terminal or Command Prompt:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip3<span class="w"> </span>install<span class="w"> </span>jupyterlab
</pre></div>
</div>
</li>
<li><p>To start JupyterLab in your web browser, run the following command from your activated Python virtual environment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>jupyter<span class="w"> </span>lab
</pre></div>
</div>
<p>If JupyterLab does not appear in your web browser, copy the URL that starts with <code class="docutils literal notranslate"><span class="pre">localhost</span></code> or <code class="docutils literal notranslate"><span class="pre">127.0.0.1</span></code> from your virtual environment, and enter it in your web browser’s address bar.</p>
</li>
<li><p>Create a new notebook: in JupyterLab, click <strong>File &gt; New &gt; Notebook</strong> on the main menu, select <strong>Python 3 (ipykernel)</strong> and click <strong>Select</strong>.</p></li>
<li><p>In the notebook’s first cell, enter either the <a class="reference internal" href="#example-code-legacy"><span class="std std-ref">example code</span></a> or your own code. If you use your own code, at minimum you must instantiate an instance of <code class="docutils literal notranslate"><span class="pre">SparkSession.builder.getOrCreate()</span></code>, as shown in the <a class="reference internal" href="#example-code-legacy"><span class="std std-ref">example code</span></a>.</p></li>
<li><p>To run the notebook, click <strong>Run &gt; Run All Cells</strong>.</p></li>
<li><p>To debug the notebook, click the bug (<strong>Enable Debugger</strong>) icon next to <strong>Python 3 (ipykernel)</strong> in the notebook’s toolbar. Set one or more breakpoints, and then click <strong>Run &gt; Run All Cells</strong>.</p></li>
<li><p>To shut down JupyterLab, click <strong>File &gt; Shut Down</strong>. If the JupyterLab process is still running in your terminal or Command Prompt, stop this process by pressing <code class="docutils literal notranslate"><span class="pre">Ctrl</span> <span class="pre">+</span> <span class="pre">c</span></code> and then entering <code class="docutils literal notranslate"><span class="pre">y</span></code> to confirm.</p></li>
</ol>
<p>For more specific debug instructions, see <a class="reference external" href="https://jupyterlab.readthedocs.io/en/stable/user/debugger.html">Debugger</a>.</p>
</div>
<div class="section" id="classic-jupyter-notebook">
<span id="jupyter-notebook"></span><h3><a class="toc-backref" href="#id5">Classic Jupyter Notebook</a><a class="headerlink" href="#classic-jupyter-notebook" title="Permalink to this headline"> </a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Before you begin to use Databricks Connect, you must <a class="reference internal" href="#requirements-legacy"><span class="std std-ref">meet the requirements</span></a> and <a class="reference internal" href="#set-up-legacy"><span class="std std-ref">set up the client</span></a> for Databricks Connect.</p>
</div>
<p>The configuration script for Databricks Connect automatically adds the package to your project configuration. To get started in a Python kernel, run:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</pre></div>
</div>
<p>To enable the <code class="docutils literal notranslate"><span class="pre">%sql</span></code> shorthand for running and visualizing SQL queries, use the following snippet:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.core.magic</span> <span class="kn">import</span> <span class="n">line_magic</span><span class="p">,</span> <span class="n">line_cell_magic</span><span class="p">,</span> <span class="n">Magics</span><span class="p">,</span> <span class="n">magics_class</span>

<span class="nd">@magics_class</span>
<span class="k">class</span> <span class="nc">DatabricksConnectMagics</span><span class="p">(</span><span class="n">Magics</span><span class="p">):</span>

   <span class="nd">@line_cell_magic</span>
   <span class="k">def</span> <span class="nf">sql</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">line</span><span class="p">,</span> <span class="n">cell</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
       <span class="k">if</span> <span class="n">cell</span> <span class="ow">and</span> <span class="n">line</span><span class="p">:</span>
           <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Line must be empty for cell magic&quot;</span><span class="p">,</span> <span class="n">line</span><span class="p">)</span>
       <span class="k">try</span><span class="p">:</span>
           <span class="kn">from</span> <span class="nn">autovizwidget.widget.utils</span> <span class="kn">import</span> <span class="n">display_dataframe</span>
       <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
           <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Please run `pip install autovizwidget` to enable the visualization widget.&quot;</span><span class="p">)</span>
           <span class="n">display_dataframe</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span>
       <span class="k">return</span> <span class="n">display_dataframe</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_spark</span><span class="p">()</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="n">cell</span> <span class="ow">or</span> <span class="n">line</span><span class="p">)</span><span class="o">.</span><span class="n">toPandas</span><span class="p">())</span>

   <span class="k">def</span> <span class="nf">get_spark</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
       <span class="n">user_ns</span> <span class="o">=</span> <span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">user_ns</span>
       <span class="k">if</span> <span class="s2">&quot;spark&quot;</span> <span class="ow">in</span> <span class="n">user_ns</span><span class="p">:</span>
           <span class="k">return</span> <span class="n">user_ns</span><span class="p">[</span><span class="s2">&quot;spark&quot;</span><span class="p">]</span>
       <span class="k">else</span><span class="p">:</span>
           <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
           <span class="n">user_ns</span><span class="p">[</span><span class="s2">&quot;spark&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
           <span class="k">return</span> <span class="n">user_ns</span><span class="p">[</span><span class="s2">&quot;spark&quot;</span><span class="p">]</span>

<span class="n">ip</span> <span class="o">=</span> <span class="n">get_ipython</span><span class="p">()</span>
<span class="n">ip</span><span class="o">.</span><span class="n">register_magics</span><span class="p">(</span><span class="n">DatabricksConnectMagics</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="visual-studio-code">
<h4>Visual Studio Code<a class="headerlink" href="#visual-studio-code" title="Permalink to this headline"> </a></h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Before you begin to use Databricks Connect, you must <a class="reference internal" href="#requirements-legacy"><span class="std std-ref">meet the requirements</span></a> and <a class="reference internal" href="#set-up-legacy"><span class="std std-ref">set up the client</span></a> for Databricks Connect.</p>
</div>
<p>To use Databricks Connect with Visual Studio Code, do the following:</p>
<ol class="arabic">
<li><p>Verify that the <a class="reference external" href="https://marketplace.visualstudio.com/items?itemName=ms-python.python">Python extension</a> is installed.</p></li>
<li><p>Open the Command Palette (<strong>Command+Shift+P</strong> on macOS and <strong>Ctrl+Shift+P</strong> on Windows/Linux).</p></li>
<li><p>Select a Python interpreter. Go to <strong>Code &gt; Preferences &gt; Settings</strong>, and choose <strong>python settings</strong>.</p></li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">databricks-connect</span> <span class="pre">get-jar-dir</span></code>.</p></li>
<li><p>Add the directory returned from the command to the User Settings JSON under <code class="docutils literal notranslate"><span class="pre">python.venvPath</span></code>. This should be added to the Python Configuration.</p></li>
<li><p>Disable the linter. Click the <strong>…</strong> on the right side and <strong>edit json settings</strong>.   The modified settings are as follows:</p>
<div class="figure align-default">
<img alt="VS Code configuration" src="../_images/vscode.png" />
</div>
</li>
<li><p>If running with a virtual environment, which is the recommended way to develop for Python in VS Code, in the Command Palette type <code class="docutils literal notranslate"><span class="pre">select</span> <span class="pre">python</span> <span class="pre">interpreter</span></code> and point to your environment that <em>matches</em> your cluster Python version.</p>
<div class="figure align-default">
<img alt="Select Python interpreter" src="../_images/select-intepreter.png" />
</div>
<p>For example, if your cluster is Python 3.9, your development environment should be Python 3.9.</p>
<div class="figure align-default">
<img alt="Python version" src="../_images/python35.png" />
</div>
</li>
</ol>
</div>
</div>
<div class="section" id="pycharm">
<h3><a class="toc-backref" href="#id6">PyCharm</a><a class="headerlink" href="#pycharm" title="Permalink to this headline"> </a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Before you begin to use Databricks Connect, you must <a class="reference internal" href="#requirements-legacy"><span class="std std-ref">meet the requirements</span></a> and <a class="reference internal" href="#set-up-legacy"><span class="std std-ref">set up the client</span></a> for Databricks Connect.</p>
</div>
<p>The configuration script for Databricks Connect automatically adds the package to your project configuration.</p>
<div class="section" id="python-3-clusters">
<h4>Python 3 clusters<a class="headerlink" href="#python-3-clusters" title="Permalink to this headline"> </a></h4>
<ol class="arabic">
<li><p>When you create a PyCharm project, select <strong>Existing Interpreter</strong>. From the drop-down menu, select the Conda environment you created (see <a class="reference internal" href="#requirements"><span class="std std-ref">Requirements</span></a>).</p>
<div class="figure align-default">
<img alt="Select interpreter" src="../_images/interpreter.png" />
</div>
</li>
<li><p>Go to <strong>Run &gt; Edit Configurations</strong>.</p></li>
<li><p>Add <code class="docutils literal notranslate"><span class="pre">PYSPARK_PYTHON=python3</span></code> as an environment variable.</p>
<div class="figure align-default">
<img alt="Python 3 cluster configuration" src="../_images/python3-env.png" />
</div>
</li>
</ol>
</div>
</div>
<div class="section" id="sparkr-and-rstudio-desktop">
<span id="sparkr-rstudio"></span><h3><a class="toc-backref" href="#id7">SparkR and RStudio Desktop</a><a class="headerlink" href="#sparkr-and-rstudio-desktop" title="Permalink to this headline"> </a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Before you begin to use Databricks Connect, you must <a class="reference internal" href="#requirements-legacy"><span class="std std-ref">meet the requirements</span></a> and <a class="reference internal" href="#set-up-legacy"><span class="std std-ref">set up the client</span></a> for Databricks Connect.</p>
</div>
<p>To use Databricks Connect with SparkR and RStudio Desktop, do the following:</p>
<ol class="arabic">
<li><p>Download and unpack the <a class="reference external" href="https://spark.apache.org/downloads.html">open source Spark</a> distribution onto your development machine. Choose the same version as in your Databricks cluster (Hadoop 2.7).</p></li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">databricks-connect</span> <span class="pre">get-jar-dir</span></code>. This command returns a path like <code class="docutils literal notranslate"><span class="pre">/usr/local/lib/python3.5/dist-packages/pyspark/jars</span></code>. Copy the file path of <em>one directory above</em> the JAR directory file path, for example, <code class="docutils literal notranslate"><span class="pre">/usr/local/lib/python3.5/dist-packages/pyspark</span></code>, which is the <code class="docutils literal notranslate"><span class="pre">SPARK_HOME</span></code> directory.</p></li>
<li><p>Configure the Spark lib path and Spark home by adding them to the top of your R script. Set <code class="docutils literal notranslate"><span class="pre">&lt;spark-lib-path&gt;</span></code> to the directory where you unpacked the open source Spark package in step 1. Set <code class="docutils literal notranslate"><span class="pre">&lt;spark-home-path&gt;</span></code> to the Databricks Connect directory from step 2.</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Point to the OSS package path, e.g., /path/to/.../spark-2.4.0-bin-hadoop2.7</span>
<span class="nf">library</span><span class="p">(</span><span class="n">SparkR</span><span class="p">,</span><span class="w"> </span><span class="n">lib.loc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">.libPaths</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="nf">file.path</span><span class="p">(</span><span class="s">&#39;&lt;spark-lib-path&gt;&#39;</span><span class="p">,</span><span class="w"> </span><span class="s">&#39;R&#39;</span><span class="p">,</span><span class="w"> </span><span class="s">&#39;lib&#39;</span><span class="p">),</span><span class="w"> </span><span class="nf">.libPaths</span><span class="p">())))</span>

<span class="c1"># Point to the Databricks Connect PySpark installation, e.g., /path/to/.../pyspark</span>
<span class="nf">Sys.setenv</span><span class="p">(</span><span class="n">SPARK_HOME</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&lt;spark-home-path&gt;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Initiate a Spark session and start running SparkR commands.</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">sparkR.session</span><span class="p">()</span>

<span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">as.DataFrame</span><span class="p">(</span><span class="n">faithful</span><span class="p">)</span>
<span class="nf">head</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="n">df1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">dapply</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="w"> </span><span class="nf">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="p">},</span><span class="w"> </span><span class="nf">schema</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
<span class="nf">collect</span><span class="p">(</span><span class="n">df1</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="sparklyr-and-rstudio-desktop">
<span id="sparklyr-rstudio"></span><h3><a class="toc-backref" href="#id8">sparklyr and RStudio Desktop</a><a class="headerlink" href="#sparklyr-and-rstudio-desktop" title="Permalink to this headline"> </a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Before you begin to use Databricks Connect, you must <a class="reference internal" href="#requirements-legacy"><span class="std std-ref">meet the requirements</span></a> and <a class="reference internal" href="#set-up-legacy"><span class="std std-ref">set up the client</span></a> for Databricks Connect.</p>
</div>
<div class="preview admonition">
<p class="admonition-title">Preview</p>
<p>This feature is in <a class="reference internal" href="../release-notes/release-types.html"><span class="doc">Public Preview</span></a>.</p>
</div>
<p>You can copy sparklyr-dependent code that you’ve developed locally using Databricks Connect and run it in a Databricks notebook or hosted RStudio Server in your Databricks workspace with minimal or no code changes.</p>
<div class="contents local topic" id="id1">
<p class="topic-title first">In this section:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#requirements" id="id14">Requirements</a></p></li>
<li><p><a class="reference internal" href="#install-configure-and-use-sparklyr" id="id15">Install, configure, and use sparklyr</a></p></li>
<li><p><a class="reference internal" href="#resources" id="id16">Resources</a></p></li>
<li><p><a class="reference internal" href="#sparklyr-and-rstudio-desktop-limitations" id="id17">sparklyr and RStudio Desktop limitations</a></p></li>
</ul>
</div>
<div class="section" id="requirements">
<span id="requirements-1"></span><h4><a class="toc-backref" href="#id14">Requirements</a><a class="headerlink" href="#requirements" title="Permalink to this headline"> </a></h4>
<ul class="simple">
<li><p>sparklyr 1.2 or above.</p></li>
<li><p>Databricks Runtime 7.3 LTS or above with the matching version of Databricks Connect.</p></li>
</ul>
</div>
<div class="section" id="install-configure-and-use-sparklyr">
<h4><a class="toc-backref" href="#id15">Install, configure, and use sparklyr</a><a class="headerlink" href="#install-configure-and-use-sparklyr" title="Permalink to this headline"> </a></h4>
<ol class="arabic">
<li><p>In RStudio Desktop, install sparklyr 1.2 or above from CRAN or install the latest master version from GitHub.</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install from CRAN</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;sparklyr&quot;</span><span class="p">)</span>

<span class="c1"># Or install the latest master version from GitHub</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;devtools&quot;</span><span class="p">)</span>
<span class="n">devtools</span><span class="o">::</span><span class="nf">install_github</span><span class="p">(</span><span class="s">&quot;sparklyr/sparklyr&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Activate the Python environment with the correct version of Databricks Connect installed and run the following command in the terminal to get the <code class="docutils literal notranslate"><span class="pre">&lt;spark-home-path&gt;</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>databricks-connect<span class="w"> </span>get-spark-home
</pre></div>
</div>
</li>
<li><p>Initiate a Spark session and start running sparklyr commands.</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">sparklyr</span><span class="p">)</span>
<span class="n">sc</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">spark_connect</span><span class="p">(</span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;databricks&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">spark_home</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&lt;spark-home-path&gt;&quot;</span><span class="p">)</span>

<span class="n">iris_tbl</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">copy_to</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span><span class="w"> </span><span class="n">iris</span><span class="p">,</span><span class="w"> </span><span class="n">overwrite</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span>

<span class="nf">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span>
<span class="nf">src_tbls</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>

<span class="n">iris_tbl</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">count</span>
</pre></div>
</div>
</li>
<li><p>Close the connection.</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">spark_disconnect</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="resources">
<h4><a class="toc-backref" href="#id16">Resources</a><a class="headerlink" href="#resources" title="Permalink to this headline"> </a></h4>
<p>For more information, see the sparklyr GitHub <a class="reference external" href="https://github.com/sparklyr/sparklyr#connecting-through-databricks-connect">README</a>.</p>
<p>For code examples, see <a class="reference internal" href="../sparkr/sparklyr.html"><span class="doc">sparklyr</span></a>.</p>
</div>
<div class="section" id="sparklyr-and-rstudio-desktop-limitations">
<h4><a class="toc-backref" href="#id17">sparklyr and RStudio Desktop limitations</a><a class="headerlink" href="#sparklyr-and-rstudio-desktop-limitations" title="Permalink to this headline"> </a></h4>
<p>The following features are unsupported:</p>
<ul class="simple">
<li><p>sparklyr streaming APIs</p></li>
<li><p>sparklyr ML APIs</p></li>
<li><p>broom APIs</p></li>
<li><p>csv_file serialization mode</p></li>
<li><p>spark submit</p></li>
</ul>
</div>
</div>
<div class="section" id="intellij-scala-or-java">
<h3><a class="toc-backref" href="#id9">IntelliJ (Scala or Java)</a><a class="headerlink" href="#intellij-scala-or-java" title="Permalink to this headline"> </a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Before you begin to use Databricks Connect, you must <a class="reference internal" href="#requirements-legacy"><span class="std std-ref">meet the requirements</span></a> and <a class="reference internal" href="#set-up-legacy"><span class="std std-ref">set up the client</span></a> for Databricks Connect.</p>
</div>
<p>To use Databricks Connect with IntelliJ (Scala or Java), do the following:</p>
<ol class="arabic">
<li><p>Run <code class="docutils literal notranslate"><span class="pre">databricks-connect</span> <span class="pre">get-jar-dir</span></code>.</p></li>
<li><p>Point the dependencies to the directory returned from the command. Go to <strong>File &gt; Project Structure &gt; Modules &gt; Dependencies &gt; ‘+’ sign &gt; JARs or Directories</strong>.</p>
<div class="figure align-default">
<img alt="IntelliJ JARs" src="../_images/intelli-j-jars.png" />
</div>
<p>To avoid conflicts, we strongly recommend removing any other Spark installations from your classpath. If this is not possible, make sure that the JARs you add are at the front of the classpath. In particular, they must be ahead of any other installed version of Spark (otherwise you will either use one of those other Spark versions and run locally or throw a <code class="docutils literal notranslate"><span class="pre">ClassDefNotFoundError</span></code>).</p>
</li>
<li><p>Check the setting of the breakout option in IntelliJ. The default is <strong>All</strong> and will cause network timeouts if you set breakpoints for debugging.  Set it to <strong>Thread</strong> to avoid stopping the background network threads.</p>
<div class="figure align-default">
<img alt="IntelliJ Thread" src="../_images/intelli-j-thread.png" />
</div>
</li>
</ol>
</div>
<div class="section" id="pydev-with-eclipse">
<span id="pydev"></span><h3><a class="toc-backref" href="#id10">PyDev with Eclipse</a><a class="headerlink" href="#pydev-with-eclipse" title="Permalink to this headline"> </a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Before you begin to use Databricks Connect, you must <a class="reference internal" href="#requirements-legacy"><span class="std std-ref">meet the requirements</span></a> and <a class="reference internal" href="#set-up-legacy"><span class="std std-ref">set up the client</span></a> for Databricks Connect.</p>
</div>
<p>To use Databricks Connect and <a class="reference external" href="https://www.pydev.org/manual_101_install.html">PyDev</a> with <a class="reference external" href="https://www.eclipse.org/downloads">Eclipse</a>, follow these instructions.</p>
<ol class="arabic simple">
<li><p>Start Eclipse.</p></li>
<li><p>Create a project: click <strong>File &gt; New &gt; Project &gt; PyDev &gt; PyDev Project</strong>, and then click <strong>Next</strong>.</p></li>
<li><p>Specify a <strong>Project name</strong>.</p></li>
<li><p>For <strong>Project contents</strong>, specify the path to your Python virtual environment.</p></li>
<li><p>Click <strong>Please configure an interpreter before proceeding</strong>.</p></li>
<li><p>Click <strong>Manual config</strong>.</p></li>
<li><p>Click <strong>New &gt; Browse for python/pypy exe</strong>.</p></li>
<li><p>Browse to and select select the full path to the Python interpreter that is referenced from the virtual environment, and then click <strong>Open</strong>.</p></li>
<li><p>In the <strong>Select interpreter</strong> dialog, click <strong>OK</strong>.</p></li>
<li><p>In the <strong>Selection needed</strong> dialog, click <strong>OK</strong>.</p></li>
<li><p>In the <strong>Preferences</strong> dialog, click <strong>Apply and Close</strong>.</p></li>
<li><p>In the <strong>PyDev Project</strong> dialog, click <strong>Finish</strong>.</p></li>
<li><p>Click <strong>Open Perspective</strong>.</p></li>
<li><p>Add to the project a Python code (<code class="docutils literal notranslate"><span class="pre">.py</span></code>) file that contains either the <a class="reference internal" href="#example-code-legacy"><span class="std std-ref">example code</span></a> or your own code. If you use your own code, at minimum you must instantiate an instance of <code class="docutils literal notranslate"><span class="pre">SparkSession.builder.getOrCreate()</span></code>, as shown in the <a class="reference internal" href="#example-code-legacy"><span class="std std-ref">example code</span></a>.</p></li>
<li><p>With the Python code file open, set any breakpoints where you want your code to pause while running.</p></li>
<li><p>Click <strong>Run &gt; Run</strong> or <strong>Run &gt; Debug</strong>.</p></li>
</ol>
<p>For more specific run and debug instructions, see <a class="reference external" href="https://www.pydev.org/manual_101_run.html">Running a Program</a>.</p>
</div>
<div class="section" id="eclipse">
<h3><a class="toc-backref" href="#id11">Eclipse</a><a class="headerlink" href="#eclipse" title="Permalink to this headline"> </a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Before you begin to use Databricks Connect, you must <a class="reference internal" href="#requirements-legacy"><span class="std std-ref">meet the requirements</span></a> and <a class="reference internal" href="#set-up-legacy"><span class="std std-ref">set up the client</span></a> for Databricks Connect.</p>
</div>
<p>To use Databricks Connect and Eclipse, do the following:</p>
<ol class="arabic">
<li><p>Run <code class="docutils literal notranslate"><span class="pre">databricks-connect</span> <span class="pre">get-jar-dir</span></code>.</p></li>
<li><p>Point the external JARs configuration to the directory returned from the command. Go to <strong>Project menu &gt; Properties &gt; Java Build Path &gt; Libraries &gt; Add External Jars</strong>.</p>
<div class="figure align-default">
<img alt="Eclipse external JAR configuration" src="../_images/eclipse.png" />
</div>
<p>To avoid conflicts, we strongly recommend removing any other Spark installations from your classpath. If this is not possible, make sure that the JARs you add are at the front of the classpath. In particular, they must be ahead of any other installed version of Spark (otherwise you will either use one of those other Spark versions and run locally or throw a <code class="docutils literal notranslate"><span class="pre">ClassDefNotFoundError</span></code>).</p>
<div class="figure align-default">
<img alt="Eclipse Spark configuration" src="../_images/eclipse2.png" />
</div>
</li>
</ol>
</div>
<div class="section" id="sbt">
<h3><a class="toc-backref" href="#id12">SBT</a><a class="headerlink" href="#sbt" title="Permalink to this headline"> </a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Before you begin to use Databricks Connect, you must <a class="reference internal" href="#requirements-legacy"><span class="std std-ref">meet the requirements</span></a> and <a class="reference internal" href="#set-up-legacy"><span class="std std-ref">set up the client</span></a> for Databricks Connect.</p>
</div>
<p>To use Databricks Connect with SBT, you must configure your <code class="docutils literal notranslate"><span class="pre">build.sbt</span></code> file to link against the Databricks Connect JARs instead of the usual Spark library dependency. You do this with the <code class="docutils literal notranslate"><span class="pre">unmanagedBase</span></code> directive in the following example build file, which assumes a Scala app that has a <code class="docutils literal notranslate"><span class="pre">com.example.Test</span></code> main object:</p>
<div class="section" id="buildsbt">
<h4><code class="docutils literal notranslate"><span class="pre">build.sbt</span></code><a class="headerlink" href="#buildsbt" title="Permalink to this headline"> </a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>name := &quot;hello-world&quot;
version := &quot;1.0&quot;
scalaVersion := &quot;2.11.6&quot;
// this should be set to the path returned by ``databricks-connect get-jar-dir``
unmanagedBase := new java.io.File(&quot;/usr/local/lib/python2.7/dist-packages/pyspark/jars&quot;)
mainClass := Some(&quot;com.example.Test&quot;)
</pre></div>
</div>
</div>
</div>
<div class="section" id="spark-shell">
<h3><a class="toc-backref" href="#id13">Spark shell</a><a class="headerlink" href="#spark-shell" title="Permalink to this headline"> </a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Before you begin to use Databricks Connect, you must <a class="reference internal" href="#requirements-legacy"><span class="std std-ref">meet the requirements</span></a> and <a class="reference internal" href="#set-up-legacy"><span class="std std-ref">set up the client</span></a> for Databricks Connect.</p>
</div>
<p>To use Databricks Connect with the Spark shell and Python or Scala, follow these instructions.</p>
<ol class="arabic">
<li><p>With your virtual environment activated, make sure that the <code class="docutils literal notranslate"><span class="pre">databricks-connect</span> <span class="pre">test</span></code> command ran successfully in <a class="reference internal" href="#set-up-legacy"><span class="std std-ref">Set up the client</span></a>.</p></li>
<li><p>With your virtual environment activated, start the Spark shell. For Python, run the <code class="docutils literal notranslate"><span class="pre">pyspark</span></code> command. For Scala, run the <code class="docutils literal notranslate"><span class="pre">spark-shell</span></code> command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># For Python:</span>
pyspark
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># For Scala:</span>
spark-shell
</pre></div>
</div>
</li>
<li><p>The Spark shell appears, for example for Python:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Python 3... (v3...)
[Clang 6... (clang-6...)] on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
Setting default log level to &quot;WARN&quot;.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
../../.. ..:..:.. WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
       ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  &#39;_/
   /__ / .__/\_,_/_/ /_/\_\   version 3....
      /_/

Using Python version 3... (v3...)
Spark context Web UI available at http://...:...
Spark context available as &#39;sc&#39; (master = local[*], app id = local-...).
SparkSession available as &#39;spark&#39;.
&gt;&gt;&gt;
</pre></div>
</div>
<p>For Scala:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Setting default log level to &quot;WARN&quot;.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
../../.. ..:..:.. WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context Web UI available at http://...
Spark context available as &#39;sc&#39; (master = local[*], app id = local-...).
Spark session available as &#39;spark&#39;.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  &#39;_/
   /___/ .__/\_,_/_/ /_/\_\   version 3...
      /_/

Using Scala version 2... (OpenJDK 64-Bit Server VM, Java 1.8...)
Type in expressions to have them evaluated.
Type :help for more information.

scala&gt;
</pre></div>
</div>
</li>
<li><p>Refer to <a class="reference external" href="https://spark.apache.org/docs/latest/quick-start.html#interactive-analysis-with-the-spark-shell">Interactive Analysis with the Spark Shell</a> for information about how to use the Spark shell with Python or Scala to run commands on your cluster.</p>
<p>Use the built-in <code class="docutils literal notranslate"><span class="pre">spark</span></code> variable to represent the <code class="docutils literal notranslate"><span class="pre">SparkSession</span></code> on your running cluster, for example for Python:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; df = spark.read.table(&quot;samples.nyctaxi.trips&quot;)
&gt;&gt;&gt; df.show(5)
+--------------------+---------------------+-------------+-----------+----------+-----------+
|tpep_pickup_datetime|tpep_dropoff_datetime|trip_distance|fare_amount|pickup_zip|dropoff_zip|
+--------------------+---------------------+-------------+-----------+----------+-----------+
| 2016-02-14 16:52:13|  2016-02-14 17:16:04|         4.94|       19.0|     10282|      10171|
| 2016-02-04 18:44:19|  2016-02-04 18:46:00|         0.28|        3.5|     10110|      10110|
| 2016-02-17 17:13:57|  2016-02-17 17:17:55|          0.7|        5.0|     10103|      10023|
| 2016-02-18 10:36:07|  2016-02-18 10:41:45|          0.8|        6.0|     10022|      10017|
| 2016-02-22 14:14:41|  2016-02-22 14:31:52|         4.51|       17.0|     10110|      10282|
+--------------------+---------------------+-------------+-----------+----------+-----------+
only showing top 5 rows
</pre></div>
</div>
<p>For Scala:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; val df = spark.read.table(&quot;samples.nyctaxi.trips&quot;)
&gt;&gt;&gt; df.show(5)
+--------------------+---------------------+-------------+-----------+----------+-----------+
|tpep_pickup_datetime|tpep_dropoff_datetime|trip_distance|fare_amount|pickup_zip|dropoff_zip|
+--------------------+---------------------+-------------+-----------+----------+-----------+
| 2016-02-14 16:52:13|  2016-02-14 17:16:04|         4.94|       19.0|     10282|      10171|
| 2016-02-04 18:44:19|  2016-02-04 18:46:00|         0.28|        3.5|     10110|      10110|
| 2016-02-17 17:13:57|  2016-02-17 17:17:55|          0.7|        5.0|     10103|      10023|
| 2016-02-18 10:36:07|  2016-02-18 10:41:45|          0.8|        6.0|     10022|      10017|
| 2016-02-22 14:14:41|  2016-02-22 14:31:52|         4.51|       17.0|     10110|      10282|
+--------------------+---------------------+-------------+-----------+----------+-----------+
only showing top 5 rows
</pre></div>
</div>
</li>
<li><p>To stop the Spark shell, press <code class="docutils literal notranslate"><span class="pre">Ctrl</span> <span class="pre">+</span> <span class="pre">d</span></code>  or <code class="docutils literal notranslate"><span class="pre">Ctrl</span> <span class="pre">+</span> <span class="pre">z</span></code>, or run the command <code class="docutils literal notranslate"><span class="pre">quit()</span></code> or <code class="docutils literal notranslate"><span class="pre">exit()</span></code> for Python or <code class="docutils literal notranslate"><span class="pre">:q</span></code> or <code class="docutils literal notranslate"><span class="pre">:quit</span></code> for Scala.</p></li>
</ol>
</div>
</div>
<div class="section" id="code-examples">
<span id="example-code-legacy"></span><h2>Code examples<a class="headerlink" href="#code-examples" title="Permalink to this headline"> </a></h2>
<p>This simple code example queries the specified table and then shows the specified table’s first 5 rows. To use a different table, adjust the call to <code class="docutils literal notranslate"><span class="pre">spark.read.table</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.session</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">&quot;samples.nyctaxi.trips&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<p>This longer code example does the following:</p>
<ol class="arabic simple">
<li><p>Creates an in-memory DataFrame.</p></li>
<li><p>Creates a table with the name <code class="docutils literal notranslate"><span class="pre">zzz_demo_temps_table</span></code> within the <code class="docutils literal notranslate"><span class="pre">default</span></code> schema. If the table with this name already exists, the table is deleted first. To use a different schema or table, adjust the calls to <code class="docutils literal notranslate"><span class="pre">spark.sql</span></code>, <code class="docutils literal notranslate"><span class="pre">temps.write.saveAsTable</span></code>, or both.</p></li>
<li><p>Saves the DataFrame’s contents to the table.</p></li>
<li><p>Runs a <code class="docutils literal notranslate"><span class="pre">SELECT</span></code> query on the table’s contents.</p></li>
<li><p>Shows the query’s result.</p></li>
<li><p>Deletes the table.</p></li>
</ol>
<div class="js-code-language-tabs js-code-language-tabs--literal compound">
<div class="compound-first highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">date</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s1">&#39;temps-demo&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="c1"># Create a Spark DataFrame consisting of high and low temperatures</span>
<span class="c1"># by airport code and date.</span>
<span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span>
    <span class="n">StructField</span><span class="p">(</span><span class="s1">&#39;AirportCode&#39;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">False</span><span class="p">),</span>
    <span class="n">StructField</span><span class="p">(</span><span class="s1">&#39;Date&#39;</span><span class="p">,</span> <span class="n">DateType</span><span class="p">(),</span> <span class="kc">False</span><span class="p">),</span>
    <span class="n">StructField</span><span class="p">(</span><span class="s1">&#39;TempHighF&#39;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">False</span><span class="p">),</span>
    <span class="n">StructField</span><span class="p">(</span><span class="s1">&#39;TempLowF&#39;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">False</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span> <span class="s1">&#39;BLI&#39;</span><span class="p">,</span> <span class="n">date</span><span class="p">(</span><span class="mi">2021</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="mi">52</span><span class="p">,</span> <span class="mi">43</span><span class="p">],</span>
    <span class="p">[</span> <span class="s1">&#39;BLI&#39;</span><span class="p">,</span> <span class="n">date</span><span class="p">(</span><span class="mi">2021</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">38</span><span class="p">],</span>
    <span class="p">[</span> <span class="s1">&#39;BLI&#39;</span><span class="p">,</span> <span class="n">date</span><span class="p">(</span><span class="mi">2021</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">52</span><span class="p">,</span> <span class="mi">41</span><span class="p">],</span>
    <span class="p">[</span> <span class="s1">&#39;PDX&#39;</span><span class="p">,</span> <span class="n">date</span><span class="p">(</span><span class="mi">2021</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">45</span><span class="p">],</span>
    <span class="p">[</span> <span class="s1">&#39;PDX&#39;</span><span class="p">,</span> <span class="n">date</span><span class="p">(</span><span class="mi">2021</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">61</span><span class="p">,</span> <span class="mi">41</span><span class="p">],</span>
    <span class="p">[</span> <span class="s1">&#39;PDX&#39;</span><span class="p">,</span> <span class="n">date</span><span class="p">(</span><span class="mi">2021</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">66</span><span class="p">,</span> <span class="mi">39</span><span class="p">],</span>
    <span class="p">[</span> <span class="s1">&#39;SEA&#39;</span><span class="p">,</span> <span class="n">date</span><span class="p">(</span><span class="mi">2021</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="mi">57</span><span class="p">,</span> <span class="mi">43</span><span class="p">],</span>
    <span class="p">[</span> <span class="s1">&#39;SEA&#39;</span><span class="p">,</span> <span class="n">date</span><span class="p">(</span><span class="mi">2021</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">54</span><span class="p">,</span> <span class="mi">39</span><span class="p">],</span>
    <span class="p">[</span> <span class="s1">&#39;SEA&#39;</span><span class="p">,</span> <span class="n">date</span><span class="p">(</span><span class="mi">2021</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">41</span><span class="p">]</span>
<span class="p">]</span>

<span class="n">temps</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>

<span class="c1"># Create a table on the Databricks cluster and then fill</span>
<span class="c1"># the table with the DataFrame&#39;s contents.</span>
<span class="c1"># If the table already exists from a previous run,</span>
<span class="c1"># delete it first.</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s1">&#39;USE default&#39;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s1">&#39;DROP TABLE IF EXISTS zzz_demo_temps_table&#39;</span><span class="p">)</span>
<span class="n">temps</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">saveAsTable</span><span class="p">(</span><span class="s1">&#39;zzz_demo_temps_table&#39;</span><span class="p">)</span>

<span class="c1"># Query the table on the Databricks cluster, returning rows</span>
<span class="c1"># where the airport code is not BLI and the date is later</span>
<span class="c1"># than 2021-04-01. Group the results and order by high</span>
<span class="c1"># temperature in descending order.</span>
<span class="n">df_temps</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM zzz_demo_temps_table &quot;</span> \
    <span class="s2">&quot;WHERE AirportCode != &#39;BLI&#39; AND Date &gt; &#39;2021-04-01&#39; &quot;</span> \
    <span class="s2">&quot;GROUP BY AirportCode, Date, TempHighF, TempLowF &quot;</span> \
    <span class="s2">&quot;ORDER BY TempHighF DESC&quot;</span><span class="p">)</span>
<span class="n">df_temps</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Results:</span>
<span class="c1">#</span>
<span class="c1"># +-----------+----------+---------+--------+</span>
<span class="c1"># |AirportCode|      Date|TempHighF|TempLowF|</span>
<span class="c1"># +-----------+----------+---------+--------+</span>
<span class="c1"># |        PDX|2021-04-03|       64|      45|</span>
<span class="c1"># |        PDX|2021-04-02|       61|      41|</span>
<span class="c1"># |        SEA|2021-04-03|       57|      43|</span>
<span class="c1"># |        SEA|2021-04-02|       54|      39|</span>
<span class="c1"># +-----------+----------+---------+--------+</span>

<span class="c1"># Clean up by deleting the table from the Databricks cluster.</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s1">&#39;DROP TABLE zzz_demo_temps_table&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="compound-middle highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="k">import</span><span class="w"> </span><span class="nn">org</span><span class="p">.</span><span class="nn">apache</span><span class="p">.</span><span class="nn">spark</span><span class="p">.</span><span class="nn">sql</span><span class="p">.</span><span class="nc">SparkSession</span>
<span class="k">import</span><span class="w"> </span><span class="nn">org</span><span class="p">.</span><span class="nn">apache</span><span class="p">.</span><span class="nn">spark</span><span class="p">.</span><span class="nn">sql</span><span class="p">.</span><span class="nn">types</span><span class="p">.</span><span class="n">_</span>
<span class="k">import</span><span class="w"> </span><span class="nn">org</span><span class="p">.</span><span class="nn">apache</span><span class="p">.</span><span class="nn">spark</span><span class="p">.</span><span class="nn">sql</span><span class="p">.</span><span class="nc">Row</span>
<span class="k">import</span><span class="w"> </span><span class="nn">java</span><span class="p">.</span><span class="nn">sql</span><span class="p">.</span><span class="nc">Date</span>

<span class="k">object</span><span class="w"> </span><span class="nc">Demo</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">(</span><span class="n">args</span><span class="p">:</span><span class="w"> </span><span class="nc">Array</span><span class="p">[</span><span class="nc">String</span><span class="p">])</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="kd">val</span><span class="w"> </span><span class="n">spark</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="n">master</span><span class="p">(</span><span class="s">&quot;local&quot;</span><span class="p">).</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="w">      </span><span class="c1">// Create a Spark DataFrame consisting of high and low temperatures</span>
<span class="w">      </span><span class="c1">// by airport code and date.</span>
<span class="w">      </span><span class="kd">val</span><span class="w"> </span><span class="n">schema</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">StructType</span><span class="p">(</span><span class="nc">Array</span><span class="p">(</span>
<span class="w">        </span><span class="nc">StructField</span><span class="p">(</span><span class="s">&quot;AirportCode&quot;</span><span class="p">,</span><span class="w"> </span><span class="nc">StringType</span><span class="p">,</span><span class="w"> </span><span class="kc">false</span><span class="p">),</span>
<span class="w">        </span><span class="nc">StructField</span><span class="p">(</span><span class="s">&quot;Date&quot;</span><span class="p">,</span><span class="w"> </span><span class="nc">DateType</span><span class="p">,</span><span class="w"> </span><span class="kc">false</span><span class="p">),</span>
<span class="w">        </span><span class="nc">StructField</span><span class="p">(</span><span class="s">&quot;TempHighF&quot;</span><span class="p">,</span><span class="w"> </span><span class="nc">IntegerType</span><span class="p">,</span><span class="w"> </span><span class="kc">false</span><span class="p">),</span>
<span class="w">        </span><span class="nc">StructField</span><span class="p">(</span><span class="s">&quot;TempLowF&quot;</span><span class="p">,</span><span class="w"> </span><span class="nc">IntegerType</span><span class="p">,</span><span class="w"> </span><span class="kc">false</span><span class="p">)</span>
<span class="w">      </span><span class="p">))</span>

<span class="w">      </span><span class="kd">val</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">List</span><span class="p">(</span>
<span class="w">        </span><span class="nc">Row</span><span class="p">(</span><span class="s">&quot;BLI&quot;</span><span class="p">,</span><span class="w"> </span><span class="nc">Date</span><span class="p">.</span><span class="n">valueOf</span><span class="p">(</span><span class="s">&quot;2021-04-03&quot;</span><span class="p">),</span><span class="w"> </span><span class="mi">52</span><span class="p">,</span><span class="w"> </span><span class="mi">43</span><span class="p">),</span>
<span class="w">        </span><span class="nc">Row</span><span class="p">(</span><span class="s">&quot;BLI&quot;</span><span class="p">,</span><span class="w"> </span><span class="nc">Date</span><span class="p">.</span><span class="n">valueOf</span><span class="p">(</span><span class="s">&quot;2021-04-02&quot;</span><span class="p">),</span><span class="w"> </span><span class="mi">50</span><span class="p">,</span><span class="w"> </span><span class="mi">38</span><span class="p">),</span>
<span class="w">        </span><span class="nc">Row</span><span class="p">(</span><span class="s">&quot;BLI&quot;</span><span class="p">,</span><span class="w"> </span><span class="nc">Date</span><span class="p">.</span><span class="n">valueOf</span><span class="p">(</span><span class="s">&quot;2021-04-01&quot;</span><span class="p">),</span><span class="w"> </span><span class="mi">52</span><span class="p">,</span><span class="w"> </span><span class="mi">41</span><span class="p">),</span>
<span class="w">        </span><span class="nc">Row</span><span class="p">(</span><span class="s">&quot;PDX&quot;</span><span class="p">,</span><span class="w"> </span><span class="nc">Date</span><span class="p">.</span><span class="n">valueOf</span><span class="p">(</span><span class="s">&quot;2021-04-03&quot;</span><span class="p">),</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">45</span><span class="p">),</span>
<span class="w">        </span><span class="nc">Row</span><span class="p">(</span><span class="s">&quot;PDX&quot;</span><span class="p">,</span><span class="w"> </span><span class="nc">Date</span><span class="p">.</span><span class="n">valueOf</span><span class="p">(</span><span class="s">&quot;2021-04-02&quot;</span><span class="p">),</span><span class="w"> </span><span class="mi">61</span><span class="p">,</span><span class="w"> </span><span class="mi">41</span><span class="p">),</span>
<span class="w">        </span><span class="nc">Row</span><span class="p">(</span><span class="s">&quot;PDX&quot;</span><span class="p">,</span><span class="w"> </span><span class="nc">Date</span><span class="p">.</span><span class="n">valueOf</span><span class="p">(</span><span class="s">&quot;2021-04-01&quot;</span><span class="p">),</span><span class="w"> </span><span class="mi">66</span><span class="p">,</span><span class="w"> </span><span class="mi">39</span><span class="p">),</span>
<span class="w">        </span><span class="nc">Row</span><span class="p">(</span><span class="s">&quot;SEA&quot;</span><span class="p">,</span><span class="w"> </span><span class="nc">Date</span><span class="p">.</span><span class="n">valueOf</span><span class="p">(</span><span class="s">&quot;2021-04-03&quot;</span><span class="p">),</span><span class="w"> </span><span class="mi">57</span><span class="p">,</span><span class="w"> </span><span class="mi">43</span><span class="p">),</span>
<span class="w">        </span><span class="nc">Row</span><span class="p">(</span><span class="s">&quot;SEA&quot;</span><span class="p">,</span><span class="w"> </span><span class="nc">Date</span><span class="p">.</span><span class="n">valueOf</span><span class="p">(</span><span class="s">&quot;2021-04-02&quot;</span><span class="p">),</span><span class="w"> </span><span class="mi">54</span><span class="p">,</span><span class="w"> </span><span class="mi">39</span><span class="p">),</span>
<span class="w">        </span><span class="nc">Row</span><span class="p">(</span><span class="s">&quot;SEA&quot;</span><span class="p">,</span><span class="w"> </span><span class="nc">Date</span><span class="p">.</span><span class="n">valueOf</span><span class="p">(</span><span class="s">&quot;2021-04-01&quot;</span><span class="p">),</span><span class="w"> </span><span class="mi">56</span><span class="p">,</span><span class="w"> </span><span class="mi">41</span><span class="p">)</span>
<span class="w">      </span><span class="p">)</span>

<span class="w">      </span><span class="kd">val</span><span class="w"> </span><span class="n">rdd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="n">makeRDD</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="w">      </span><span class="kd">val</span><span class="w"> </span><span class="n">temps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span><span class="w"> </span><span class="n">schema</span><span class="p">)</span>

<span class="w">      </span><span class="c1">// Create a table on the Databricks cluster and then fill</span>
<span class="w">      </span><span class="c1">// the table with the DataFrame&#39;s contents.</span>
<span class="w">      </span><span class="c1">// If the table already exists from a previous run,</span>
<span class="w">      </span><span class="c1">// delete it first.</span>
<span class="w">      </span><span class="n">spark</span><span class="p">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;USE default&quot;</span><span class="p">)</span>
<span class="w">      </span><span class="n">spark</span><span class="p">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;DROP TABLE IF EXISTS zzz_demo_temps_table&quot;</span><span class="p">)</span>
<span class="w">      </span><span class="n">temps</span><span class="p">.</span><span class="n">write</span><span class="p">.</span><span class="n">saveAsTable</span><span class="p">(</span><span class="s">&quot;zzz_demo_temps_table&quot;</span><span class="p">)</span>

<span class="w">      </span><span class="c1">// Query the table on the Databricks cluster, returning rows</span>
<span class="w">      </span><span class="c1">// where the airport code is not BLI and the date is later</span>
<span class="w">      </span><span class="c1">// than 2021-04-01. Group the results and order by high</span>
<span class="w">      </span><span class="c1">// temperature in descending order.</span>
<span class="w">      </span><span class="kd">val</span><span class="w"> </span><span class="n">df_temps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">spark</span><span class="p">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;SELECT * FROM zzz_demo_temps_table &quot;</span><span class="w"> </span><span class="o">+</span>
<span class="w">        </span><span class="s">&quot;WHERE AirportCode != &#39;BLI&#39; AND Date &gt; &#39;2021-04-01&#39; &quot;</span><span class="w"> </span><span class="o">+</span>
<span class="w">        </span><span class="s">&quot;GROUP BY AirportCode, Date, TempHighF, TempLowF &quot;</span><span class="w"> </span><span class="o">+</span>
<span class="w">        </span><span class="s">&quot;ORDER BY TempHighF DESC&quot;</span><span class="p">)</span>
<span class="w">      </span><span class="n">df_temps</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="w">      </span><span class="c1">// Results:</span>
<span class="w">      </span><span class="c1">//</span>
<span class="w">      </span><span class="c1">// +-----------+----------+---------+--------+</span>
<span class="w">      </span><span class="c1">// |AirportCode|      Date|TempHighF|TempLowF|</span>
<span class="w">      </span><span class="c1">// +-----------+----------+---------+--------+</span>
<span class="w">      </span><span class="c1">// |        PDX|2021-04-03|       64|      45|</span>
<span class="w">      </span><span class="c1">// |        PDX|2021-04-02|       61|      41|</span>
<span class="w">      </span><span class="c1">// |        SEA|2021-04-03|       57|      43|</span>
<span class="w">      </span><span class="c1">// |        SEA|2021-04-02|       54|      39|</span>
<span class="w">      </span><span class="c1">// +-----------+----------+---------+--------+</span>

<span class="w">      </span><span class="c1">// Clean up by deleting the table from the Databricks cluster.</span>
<span class="w">      </span><span class="n">spark</span><span class="p">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;DROP TABLE zzz_demo_temps_table&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="compound-last highlight-java notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">java.util.ArrayList</span><span class="p">;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">java.util.List</span><span class="p">;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">java.sql.Date</span><span class="p">;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">org.apache.spark.sql.SparkSession</span><span class="p">;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">org.apache.spark.sql.types.*</span><span class="p">;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">org.apache.spark.sql.Row</span><span class="p">;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">org.apache.spark.sql.RowFactory</span><span class="p">;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">org.apache.spark.sql.Dataset</span><span class="p">;</span>

<span class="kd">public</span><span class="w"> </span><span class="kd">class</span> <span class="nc">App</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kd">public</span><span class="w"> </span><span class="kd">static</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="nf">main</span><span class="p">(</span><span class="n">String</span><span class="o">[]</span><span class="w"> </span><span class="n">args</span><span class="p">)</span><span class="w"> </span><span class="kd">throws</span><span class="w"> </span><span class="n">Exception</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">SparkSession</span><span class="w"> </span><span class="n">spark</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">SparkSession</span>
<span class="w">            </span><span class="p">.</span><span class="na">builder</span><span class="p">()</span>
<span class="w">            </span><span class="p">.</span><span class="na">appName</span><span class="p">(</span><span class="s">&quot;Temps Demo&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="p">.</span><span class="na">config</span><span class="p">(</span><span class="s">&quot;spark.master&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;local&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="p">.</span><span class="na">getOrCreate</span><span class="p">();</span>

<span class="w">        </span><span class="c1">// Create a Spark DataFrame consisting of high and low temperatures</span>
<span class="w">        </span><span class="c1">// by airport code and date.</span>
<span class="w">        </span><span class="n">StructType</span><span class="w"> </span><span class="n">schema</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">StructType</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="n">StructField</span><span class="o">[]</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">new</span><span class="w"> </span><span class="n">StructField</span><span class="p">(</span><span class="s">&quot;AirportCode&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">DataTypes</span><span class="p">.</span><span class="na">StringType</span><span class="p">,</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span><span class="w"> </span><span class="n">Metadata</span><span class="p">.</span><span class="na">empty</span><span class="p">()),</span>
<span class="w">            </span><span class="k">new</span><span class="w"> </span><span class="n">StructField</span><span class="p">(</span><span class="s">&quot;Date&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">DataTypes</span><span class="p">.</span><span class="na">DateType</span><span class="p">,</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span><span class="w"> </span><span class="n">Metadata</span><span class="p">.</span><span class="na">empty</span><span class="p">()),</span>
<span class="w">            </span><span class="k">new</span><span class="w"> </span><span class="n">StructField</span><span class="p">(</span><span class="s">&quot;TempHighF&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">DataTypes</span><span class="p">.</span><span class="na">IntegerType</span><span class="p">,</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span><span class="w"> </span><span class="n">Metadata</span><span class="p">.</span><span class="na">empty</span><span class="p">()),</span>
<span class="w">            </span><span class="k">new</span><span class="w"> </span><span class="n">StructField</span><span class="p">(</span><span class="s">&quot;TempLowF&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">DataTypes</span><span class="p">.</span><span class="na">IntegerType</span><span class="p">,</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span><span class="w"> </span><span class="n">Metadata</span><span class="p">.</span><span class="na">empty</span><span class="p">()),</span>
<span class="w">        </span><span class="p">});</span>

<span class="w">        </span><span class="n">List</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dataList</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">ArrayList</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">        </span><span class="n">dataList</span><span class="p">.</span><span class="na">add</span><span class="p">(</span><span class="n">RowFactory</span><span class="p">.</span><span class="na">create</span><span class="p">(</span><span class="s">&quot;BLI&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">Date</span><span class="p">.</span><span class="na">valueOf</span><span class="p">(</span><span class="s">&quot;2021-04-03&quot;</span><span class="p">),</span><span class="w"> </span><span class="mi">52</span><span class="p">,</span><span class="w"> </span><span class="mi">43</span><span class="p">));</span>
<span class="w">        </span><span class="n">dataList</span><span class="p">.</span><span class="na">add</span><span class="p">(</span><span class="n">RowFactory</span><span class="p">.</span><span class="na">create</span><span class="p">(</span><span class="s">&quot;BLI&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">Date</span><span class="p">.</span><span class="na">valueOf</span><span class="p">(</span><span class="s">&quot;2021-04-02&quot;</span><span class="p">),</span><span class="w"> </span><span class="mi">50</span><span class="p">,</span><span class="w"> </span><span class="mi">38</span><span class="p">));</span>
<span class="w">        </span><span class="n">dataList</span><span class="p">.</span><span class="na">add</span><span class="p">(</span><span class="n">RowFactory</span><span class="p">.</span><span class="na">create</span><span class="p">(</span><span class="s">&quot;BLI&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">Date</span><span class="p">.</span><span class="na">valueOf</span><span class="p">(</span><span class="s">&quot;2021-04-01&quot;</span><span class="p">),</span><span class="w"> </span><span class="mi">52</span><span class="p">,</span><span class="w"> </span><span class="mi">41</span><span class="p">));</span>
<span class="w">        </span><span class="n">dataList</span><span class="p">.</span><span class="na">add</span><span class="p">(</span><span class="n">RowFactory</span><span class="p">.</span><span class="na">create</span><span class="p">(</span><span class="s">&quot;PDX&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">Date</span><span class="p">.</span><span class="na">valueOf</span><span class="p">(</span><span class="s">&quot;2021-04-03&quot;</span><span class="p">),</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">45</span><span class="p">));</span>
<span class="w">        </span><span class="n">dataList</span><span class="p">.</span><span class="na">add</span><span class="p">(</span><span class="n">RowFactory</span><span class="p">.</span><span class="na">create</span><span class="p">(</span><span class="s">&quot;PDX&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">Date</span><span class="p">.</span><span class="na">valueOf</span><span class="p">(</span><span class="s">&quot;2021-04-02&quot;</span><span class="p">),</span><span class="w"> </span><span class="mi">61</span><span class="p">,</span><span class="w"> </span><span class="mi">41</span><span class="p">));</span>
<span class="w">        </span><span class="n">dataList</span><span class="p">.</span><span class="na">add</span><span class="p">(</span><span class="n">RowFactory</span><span class="p">.</span><span class="na">create</span><span class="p">(</span><span class="s">&quot;PDX&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">Date</span><span class="p">.</span><span class="na">valueOf</span><span class="p">(</span><span class="s">&quot;2021-04-01&quot;</span><span class="p">),</span><span class="w"> </span><span class="mi">66</span><span class="p">,</span><span class="w"> </span><span class="mi">39</span><span class="p">));</span>
<span class="w">        </span><span class="n">dataList</span><span class="p">.</span><span class="na">add</span><span class="p">(</span><span class="n">RowFactory</span><span class="p">.</span><span class="na">create</span><span class="p">(</span><span class="s">&quot;SEA&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">Date</span><span class="p">.</span><span class="na">valueOf</span><span class="p">(</span><span class="s">&quot;2021-04-03&quot;</span><span class="p">),</span><span class="w"> </span><span class="mi">57</span><span class="p">,</span><span class="w"> </span><span class="mi">43</span><span class="p">));</span>
<span class="w">        </span><span class="n">dataList</span><span class="p">.</span><span class="na">add</span><span class="p">(</span><span class="n">RowFactory</span><span class="p">.</span><span class="na">create</span><span class="p">(</span><span class="s">&quot;SEA&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">Date</span><span class="p">.</span><span class="na">valueOf</span><span class="p">(</span><span class="s">&quot;2021-04-02&quot;</span><span class="p">),</span><span class="w"> </span><span class="mi">54</span><span class="p">,</span><span class="w"> </span><span class="mi">39</span><span class="p">));</span>
<span class="w">        </span><span class="n">dataList</span><span class="p">.</span><span class="na">add</span><span class="p">(</span><span class="n">RowFactory</span><span class="p">.</span><span class="na">create</span><span class="p">(</span><span class="s">&quot;SEA&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">Date</span><span class="p">.</span><span class="na">valueOf</span><span class="p">(</span><span class="s">&quot;2021-04-01&quot;</span><span class="p">),</span><span class="w"> </span><span class="mi">56</span><span class="p">,</span><span class="w"> </span><span class="mi">41</span><span class="p">));</span>

<span class="w">        </span><span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span><span class="w"> </span><span class="n">temps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">spark</span><span class="p">.</span><span class="na">createDataFrame</span><span class="p">(</span><span class="n">dataList</span><span class="p">,</span><span class="w"> </span><span class="n">schema</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// Create a table on the Databricks cluster and then fill</span>
<span class="w">        </span><span class="c1">// the table with the DataFrame&#39;s contents.</span>
<span class="w">        </span><span class="c1">// If the table already exists from a previous run,</span>
<span class="w">        </span><span class="c1">// delete it first.</span>
<span class="w">        </span><span class="n">spark</span><span class="p">.</span><span class="na">sql</span><span class="p">(</span><span class="s">&quot;USE default&quot;</span><span class="p">);</span>
<span class="w">        </span><span class="n">spark</span><span class="p">.</span><span class="na">sql</span><span class="p">(</span><span class="s">&quot;DROP TABLE IF EXISTS zzz_demo_temps_table&quot;</span><span class="p">);</span>
<span class="w">        </span><span class="n">temps</span><span class="p">.</span><span class="na">write</span><span class="p">().</span><span class="na">saveAsTable</span><span class="p">(</span><span class="s">&quot;zzz_demo_temps_table&quot;</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// Query the table on the Databricks cluster, returning rows</span>
<span class="w">        </span><span class="c1">// where the airport code is not BLI and the date is later</span>
<span class="w">        </span><span class="c1">// than 2021-04-01. Group the results and order by high</span>
<span class="w">        </span><span class="c1">// temperature in descending order.</span>
<span class="w">        </span><span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span><span class="w"> </span><span class="n">df_temps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">spark</span><span class="p">.</span><span class="na">sql</span><span class="p">(</span><span class="s">&quot;SELECT * FROM zzz_demo_temps_table &quot;</span><span class="w"> </span><span class="o">+</span>
<span class="w">            </span><span class="s">&quot;WHERE AirportCode != &#39;BLI&#39; AND Date &gt; &#39;2021-04-01&#39; &quot;</span><span class="w"> </span><span class="o">+</span>
<span class="w">            </span><span class="s">&quot;GROUP BY AirportCode, Date, TempHighF, TempLowF &quot;</span><span class="w"> </span><span class="o">+</span>
<span class="w">            </span><span class="s">&quot;ORDER BY TempHighF DESC&quot;</span><span class="p">);</span>
<span class="w">        </span><span class="n">df_temps</span><span class="p">.</span><span class="na">show</span><span class="p">();</span>

<span class="w">        </span><span class="c1">// Results:</span>
<span class="w">        </span><span class="c1">//</span>
<span class="w">        </span><span class="c1">// +-----------+----------+---------+--------+</span>
<span class="w">        </span><span class="c1">// |AirportCode|      Date|TempHighF|TempLowF|</span>
<span class="w">        </span><span class="c1">// +-----------+----------+---------+--------+</span>
<span class="w">        </span><span class="c1">// |        PDX|2021-04-03|       64|      45|</span>
<span class="w">        </span><span class="c1">// |        PDX|2021-04-02|       61|      41|</span>
<span class="w">        </span><span class="c1">// |        SEA|2021-04-03|       57|      43|</span>
<span class="w">        </span><span class="c1">// |        SEA|2021-04-02|       54|      39|</span>
<span class="w">        </span><span class="c1">// +-----------+----------+---------+--------+</span>

<span class="w">        </span><span class="c1">// Clean up by deleting the table from the Databricks cluster.</span>
<span class="w">        </span><span class="n">spark</span><span class="p">.</span><span class="na">sql</span><span class="p">(</span><span class="s">&quot;DROP TABLE zzz_demo_temps_table&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="work-with-dependencies">
<h2>Work with dependencies<a class="headerlink" href="#work-with-dependencies" title="Permalink to this headline"> </a></h2>
<p>Typically your main class or Python file will have other dependency JARs and files. You can add such dependency JARs and files by calling <code class="docutils literal notranslate"><span class="pre">sparkContext.addJar(&quot;path-to-the-jar&quot;)</span></code> or <code class="docutils literal notranslate"><span class="pre">sparkContext.addPyFile(&quot;path-to-the-file&quot;)</span></code>. You can also add Egg files and zip files with the <code class="docutils literal notranslate"><span class="pre">addPyFile()</span></code> interface. Every time you run the code in your IDE, the dependency JARs and files are installed on the cluster.</p>
<div class="js-code-language-tabs compound">
<div class="compound-first compound" lang="python">
<div class="compound-first highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lib</span> <span class="kn">import</span> <span class="n">Foo</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="n">sc</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span>
<span class="c1">#sc.setLogLevel(&quot;INFO&quot;)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing simple count&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing addPyFile isolation&quot;</span><span class="p">)</span>
<span class="n">sc</span><span class="o">.</span><span class="n">addPyFile</span><span class="p">(</span><span class="s2">&quot;lib.py&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">Foo</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>

<span class="k">class</span> <span class="nc">Foo</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
</pre></div>
</div>
<p class="compound-middle"><strong>Python + Java UDFs</strong></p>
<div class="compound-last highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.column</span> <span class="kn">import</span> <span class="n">_to_java_column</span><span class="p">,</span> <span class="n">_to_seq</span><span class="p">,</span> <span class="n">Column</span>

<span class="c1">## In this example, udf.jar contains compiled Java / Scala UDFs:</span>
<span class="c1">#package com.example</span>
<span class="c1">#</span>
<span class="c1">#import org.apache.spark.sql._</span>
<span class="c1">#import org.apache.spark.sql.expressions._</span>
<span class="c1">#import org.apache.spark.sql.functions.udf</span>
<span class="c1">#</span>
<span class="c1">#object Test {</span>
<span class="c1">#  val plusOne: UserDefinedFunction = udf((i: Long) =&gt; i + 1)</span>
<span class="c1">#}</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span> \
  <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.jars&quot;</span><span class="p">,</span> <span class="s2">&quot;/path/to/udf.jar&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span>

<span class="k">def</span> <span class="nf">plus_one_udf</span><span class="p">(</span><span class="n">col</span><span class="p">):</span>
  <span class="n">f</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">com</span><span class="o">.</span><span class="n">example</span><span class="o">.</span><span class="n">Test</span><span class="o">.</span><span class="n">plusOne</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">Column</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">_to_seq</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="p">[</span><span class="n">col</span><span class="p">],</span> <span class="n">_to_java_column</span><span class="p">)))</span>

<span class="n">sc</span><span class="o">.</span><span class="n">_jsc</span><span class="o">.</span><span class="n">addJar</span><span class="p">(</span><span class="s2">&quot;/path/to/udf.jar&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;plusOne&quot;</span><span class="p">,</span> <span class="n">plus_one_udf</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="compound-last compound" lang="scala">
<div class="highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="k">package</span><span class="w"> </span><span class="nn">com</span><span class="p">.</span><span class="n">example</span>

<span class="k">import</span><span class="w"> </span><span class="nn">org</span><span class="p">.</span><span class="nn">apache</span><span class="p">.</span><span class="nn">spark</span><span class="p">.</span><span class="nn">sql</span><span class="p">.</span><span class="nc">SparkSession</span>

<span class="k">case</span><span class="w"> </span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><span class="n">x</span><span class="p">:</span><span class="w"> </span><span class="nc">String</span><span class="p">)</span>

<span class="k">object</span><span class="w"> </span><span class="nc">Test</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">(</span><span class="n">args</span><span class="p">:</span><span class="w"> </span><span class="nc">Array</span><span class="p">[</span><span class="nc">String</span><span class="p">]):</span><span class="w"> </span><span class="nc">Unit</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kd">val</span><span class="w"> </span><span class="n">spark</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">()</span>
<span class="w">      </span><span class="p">...</span>
<span class="w">      </span><span class="p">.</span><span class="n">getOrCreate</span><span class="p">();</span>
<span class="w">    </span><span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="n">setLogLevel</span><span class="p">(</span><span class="s">&quot;INFO&quot;</span><span class="p">)</span>

<span class="w">    </span><span class="n">println</span><span class="p">(</span><span class="s">&quot;Running simple show query...&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">format</span><span class="p">(</span><span class="s">&quot;parquet&quot;</span><span class="p">).</span><span class="n">load</span><span class="p">(</span><span class="s">&quot;/tmp/x&quot;</span><span class="p">).</span><span class="n">show</span><span class="p">()</span>

<span class="w">    </span><span class="n">println</span><span class="p">(</span><span class="s">&quot;Running simple UDF query...&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="n">addJar</span><span class="p">(</span><span class="s">&quot;./target/scala-2.11/hello-world_2.11-1.0.jar&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="n">spark</span><span class="p">.</span><span class="n">udf</span><span class="p">.</span><span class="n">register</span><span class="p">(</span><span class="s">&quot;f&quot;</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="n">x</span><span class="p">:</span><span class="w"> </span><span class="nc">Int</span><span class="p">)</span><span class="w"> </span><span class="o">=&gt;</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span>
<span class="w">    </span><span class="n">spark</span><span class="p">.</span><span class="n">range</span><span class="p">(</span><span class="mi">10</span><span class="p">).</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">&quot;f(id)&quot;</span><span class="p">).</span><span class="n">show</span><span class="p">()</span>

<span class="w">    </span><span class="n">println</span><span class="p">(</span><span class="s">&quot;Running custom objects query...&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="kd">val</span><span class="w"> </span><span class="n">objs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nc">Seq</span><span class="p">(</span><span class="nc">Foo</span><span class="p">(</span><span class="s">&quot;bye&quot;</span><span class="p">),</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><span class="s">&quot;hi&quot;</span><span class="p">))).</span><span class="n">collect</span><span class="p">()</span>
<span class="w">    </span><span class="n">println</span><span class="p">(</span><span class="n">objs</span><span class="p">.</span><span class="n">toSeq</span><span class="p">)</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="access-databricks-utilities">
<span id="dbutils-legacy"></span><h2>Access Databricks Utilities<a class="headerlink" href="#access-databricks-utilities" title="Permalink to this headline"> </a></h2>
<p>This section describes how to use Databricks Connect to access <a class="reference internal" href="databricks-utils.html"><span class="doc">Databricks Utilities</span></a>.</p>
<p>You can use <code class="docutils literal notranslate"><span class="pre">dbutils.fs</span></code> and <code class="docutils literal notranslate"><span class="pre">dbutils.secrets</span></code> utilities of the <a class="reference internal" href="databricks-utils.html"><span class="doc">Databricks Utilities (dbutils) reference</span></a> module.
Supported commands are <code class="docutils literal notranslate"><span class="pre">dbutils.fs.cp</span></code>, <code class="docutils literal notranslate"><span class="pre">dbutils.fs.head</span></code>, <code class="docutils literal notranslate"><span class="pre">dbutils.fs.ls</span></code>, <code class="docutils literal notranslate"><span class="pre">dbutils.fs.mkdirs</span></code>, <code class="docutils literal notranslate"><span class="pre">dbutils.fs.mv</span></code>, <code class="docutils literal notranslate"><span class="pre">dbutils.fs.put</span></code>, <code class="docutils literal notranslate"><span class="pre">dbutils.fs.rm</span></code>, <code class="docutils literal notranslate"><span class="pre">dbutils.secrets.get</span></code>, <code class="docutils literal notranslate"><span class="pre">dbutils.secrets.getBytes</span></code>, <code class="docutils literal notranslate"><span class="pre">dbutils.secrets.list</span></code>, <code class="docutils literal notranslate"><span class="pre">dbutils.secrets.listScopes</span></code>.
See <a class="reference internal" href="databricks-utils.html#dbutils-fs"><span class="std std-ref">File system utility (dbutils.fs)</span></a> or run <code class="docutils literal notranslate"><span class="pre">dbutils.fs.help()</span></code> and <a class="reference internal" href="databricks-utils.html#dbutils-secrets"><span class="std std-ref">Secrets utility (dbutils.secrets)</span></a> or run <code class="docutils literal notranslate"><span class="pre">dbutils.secrets.help()</span></code>.</p>
<div class="js-code-language-tabs compound">
<div class="compound-first compound" lang="python">
<div class="compound-first highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="nn">pyspark.dbutils</span> <span class="kn">import</span> <span class="n">DBUtils</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="n">dbutils</span> <span class="o">=</span> <span class="n">DBUtils</span><span class="p">(</span><span class="n">spark</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dbutils</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">ls</span><span class="p">(</span><span class="s2">&quot;dbfs:/&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dbutils</span><span class="o">.</span><span class="n">secrets</span><span class="o">.</span><span class="n">listScopes</span><span class="p">())</span>
</pre></div>
</div>
<p class="compound-middle">When using Databricks Runtime 7.3 LTS or above, to access the DBUtils module in a way that works both locally and in Databricks clusters, use the following <code class="docutils literal notranslate"><span class="pre">get_dbutils()</span></code>:</p>
<div class="compound-middle highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_dbutils</span><span class="p">(</span><span class="n">spark</span><span class="p">):</span>
  <span class="kn">from</span> <span class="nn">pyspark.dbutils</span> <span class="kn">import</span> <span class="n">DBUtils</span>
  <span class="k">return</span> <span class="n">DBUtils</span><span class="p">(</span><span class="n">spark</span><span class="p">)</span>
</pre></div>
</div>
<p class="compound-middle">Otherwise, use the following <code class="docutils literal notranslate"><span class="pre">get_dbutils()</span></code>:</p>
<div class="compound-last highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_dbutils</span><span class="p">(</span><span class="n">spark</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;spark.databricks.service.client.enabled&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;true&quot;</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">pyspark.dbutils</span> <span class="kn">import</span> <span class="n">DBUtils</span>
    <span class="k">return</span> <span class="n">DBUtils</span><span class="p">(</span><span class="n">spark</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">IPython</span>
    <span class="k">return</span> <span class="n">IPython</span><span class="o">.</span><span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">user_ns</span><span class="p">[</span><span class="s2">&quot;dbutils&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="compound-last compound" lang="scala">
<div class="highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="kd">val</span><span class="w"> </span><span class="n">dbutils</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">com</span><span class="p">.</span><span class="n">databricks</span><span class="p">.</span><span class="n">service</span><span class="p">.</span><span class="nc">DBUtils</span>
<span class="n">println</span><span class="p">(</span><span class="n">dbutils</span><span class="p">.</span><span class="n">fs</span><span class="p">.</span><span class="n">ls</span><span class="p">(</span><span class="s">&quot;dbfs:/&quot;</span><span class="p">))</span>
<span class="n">println</span><span class="p">(</span><span class="n">dbutils</span><span class="p">.</span><span class="n">secrets</span><span class="p">.</span><span class="n">listScopes</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="copying-files-between-local-and-remote-filesystems">
<h3>Copying files between local and remote filesystems<a class="headerlink" href="#copying-files-between-local-and-remote-filesystems" title="Permalink to this headline"> </a></h3>
<p>You can use <code class="docutils literal notranslate"><span class="pre">dbutils.fs</span></code> to copy files between your client and remote filesystems. Scheme <code class="docutils literal notranslate"><span class="pre">file:/</span></code> refers to the local filesystem on the client.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.dbutils</span> <span class="kn">import</span> <span class="n">DBUtils</span>
<span class="n">dbutils</span> <span class="o">=</span> <span class="n">DBUtils</span><span class="p">(</span><span class="n">spark</span><span class="p">)</span>

<span class="n">dbutils</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">cp</span><span class="p">(</span><span class="s1">&#39;file:/home/user/data.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;dbfs:/uploads&#39;</span><span class="p">)</span>
<span class="n">dbutils</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">cp</span><span class="p">(</span><span class="s1">&#39;dbfs:/output/results.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;file:/home/user/downloads/&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The maximum file size that can be transferred that way is 250 MB.</p>
</div>
<div class="section" id="enable-dbutilssecretsget">
<h3>Enable <code class="docutils literal notranslate"><span class="pre">dbutils.secrets.get</span></code><a class="headerlink" href="#enable-dbutilssecretsget" title="Permalink to this headline"> </a></h3>
<p>Because of security restrictions, the ability to call <code class="docutils literal notranslate"><span class="pre">dbutils.secrets.get</span></code> is disabled by default. Contact Databricks support to enable this feature for your workspace.</p>
</div>
</div>
<div class="section" id="set-hadoop-configurations">
<h2>Set Hadoop configurations<a class="headerlink" href="#set-hadoop-configurations" title="Permalink to this headline"> </a></h2>
<p>On the client you can set Hadoop configurations using the <code class="docutils literal notranslate"><span class="pre">spark.conf.set</span></code> API, which applies to SQL and DataFrame operations. Hadoop configurations set on the <code class="docutils literal notranslate"><span class="pre">sparkContext</span></code> must be set in the cluster configuration or using a notebook. This is because configurations set on <code class="docutils literal notranslate"><span class="pre">sparkContext</span></code> are not tied to user sessions but apply to the entire cluster.</p>
</div>
<div class="section" id="troubleshooting">
<h2>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permalink to this headline"> </a></h2>
<p>Run <code class="docutils literal notranslate"><span class="pre">databricks-connect</span> <span class="pre">test</span></code> to check for connectivity issues. This section describes some common issues you may encounter with Databricks Connect and how to resolve them.</p>
<div class="contents local topic" id="id3">
<p class="topic-title first">In this section:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#python-version-mismatch" id="id18">Python version mismatch</a></p></li>
<li><p><a class="reference internal" href="#server-not-enabled" id="id19">Server not enabled</a></p></li>
<li><p><a class="reference internal" href="#conflicting-pyspark-installations" id="id20">Conflicting PySpark installations</a></p></li>
<li><p><a class="reference internal" href="#conflicting-spark_home" id="id21">Conflicting <code class="docutils literal notranslate"><span class="pre">SPARK_HOME</span></code></a></p></li>
<li><p><a class="reference internal" href="#conflicting-or-missing-path-entry-for-binaries" id="id22">Conflicting or Missing <code class="docutils literal notranslate"><span class="pre">PATH</span></code> entry for binaries</a></p></li>
<li><p><a class="reference internal" href="#conflicting-serialization-settings-on-the-cluster" id="id23">Conflicting serialization settings on the cluster</a></p></li>
<li><p><a class="reference internal" href="#cannot-find-winutilsexe-on-windows" id="id24">Cannot find <code class="docutils literal notranslate"><span class="pre">winutils.exe</span></code> on Windows</a></p></li>
<li><p><a class="reference internal" href="#the-filename-directory-name-or-volume-label-syntax-is-incorrect-on-windows" id="id25">The filename, directory name, or volume label syntax is incorrect on Windows</a></p></li>
</ul>
</div>
<div class="section" id="python-version-mismatch">
<h3><a class="toc-backref" href="#id18">Python version mismatch</a><a class="headerlink" href="#python-version-mismatch" title="Permalink to this headline"> </a></h3>
<p>Check the Python version you are using locally has at least the same minor release as the version on the cluster (for example, <code class="docutils literal notranslate"><span class="pre">3.9.16</span></code> versus <code class="docutils literal notranslate"><span class="pre">3.9.15</span></code> is OK, <code class="docutils literal notranslate"><span class="pre">3.9</span></code> versus <code class="docutils literal notranslate"><span class="pre">3.8</span></code> is not).</p>
<p>If you have multiple Python versions installed locally, ensure that Databricks Connect is using the right one by setting the <code class="docutils literal notranslate"><span class="pre">PYSPARK_PYTHON</span></code> environment variable (for example, <code class="docutils literal notranslate"><span class="pre">PYSPARK_PYTHON=python3</span></code>).</p>
</div>
<div class="section" id="server-not-enabled">
<h3><a class="toc-backref" href="#id19">Server not enabled</a><a class="headerlink" href="#server-not-enabled" title="Permalink to this headline"> </a></h3>
<p>Ensure the cluster has the Spark server enabled with <code class="docutils literal notranslate"><span class="pre">spark.databricks.service.server.enabled</span> <span class="pre">true</span></code>. You should see the following lines in the driver log if it is:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>../../.. ..:..:.. INFO SparkConfUtils$: Set spark config:
spark.databricks.service.server.enabled -&gt; true
...
../../.. ..:..:.. INFO SparkContext: Loading Spark Service RPC Server
../../.. ..:..:.. INFO SparkServiceRPCServer:
Starting Spark Service RPC Server
../../.. ..:..:.. INFO Server: jetty-9...
../../.. ..:..:.. INFO AbstractConnector: Started ServerConnector@6a6c7f42
{HTTP/1.1,[http/1.1]}{0.0.0.0:15001}
../../.. ..:..:.. INFO Server: Started @5879ms
</pre></div>
</div>
</div>
<div class="section" id="conflicting-pyspark-installations">
<span id="conflicting-pyspark-installations-legacy"></span><h3><a class="toc-backref" href="#id20">Conflicting PySpark installations</a><a class="headerlink" href="#conflicting-pyspark-installations" title="Permalink to this headline"> </a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">databricks-connect</span></code> package conflicts with PySpark. Having both installed will cause errors when initializing the Spark context in Python. This can manifest in several ways, including “stream corrupted” or “class not found” errors. If you have PySpark installed in your Python environment, ensure it is uninstalled before installing databricks-connect. After uninstalling PySpark, make sure to fully re-install the Databricks Connect package:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip3<span class="w"> </span>uninstall<span class="w"> </span>pyspark
pip3<span class="w"> </span>uninstall<span class="w"> </span>databricks-connect
pip3<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span><span class="s2">&quot;databricks-connect==12.2.*&quot;</span><span class="w">  </span><span class="c1"># or X.Y.* to match your specific cluster version.</span>
</pre></div>
</div>
</div>
<div class="section" id="conflicting-spark_home">
<h3><a class="toc-backref" href="#id21">Conflicting <code class="docutils literal notranslate"><span class="pre">SPARK_HOME</span></code></a><a class="headerlink" href="#conflicting-spark_home" title="Permalink to this headline"> </a></h3>
<p>If you have previously used Spark on your machine, your IDE may be configured to use one of those other versions of Spark rather than the Databricks Connect Spark. This can manifest in several ways, including “stream corrupted” or “class not found” errors. You can see which version of Spark is being used by checking the value of the <code class="docutils literal notranslate"><span class="pre">SPARK_HOME</span></code> environment variable:</p>
<div class="js-code-language-tabs js-code-language-tabs--literal compound">
<div class="compound-first highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="nb">print</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;SPARK_HOME&#39;</span><span class="p">])</span>
</pre></div>
</div>
<div class="compound-middle highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="n">println</span><span class="p">(</span><span class="n">sys</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">&quot;SPARK_HOME&quot;</span><span class="p">))</span>
</pre></div>
</div>
<div class="compound-last highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">System</span><span class="p">.</span><span class="na">out</span><span class="p">.</span><span class="na">println</span><span class="p">(</span><span class="n">System</span><span class="p">.</span><span class="na">getenv</span><span class="p">(</span><span class="s">&quot;SPARK_HOME&quot;</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="section" id="resolution">
<h4>Resolution<a class="headerlink" href="#resolution" title="Permalink to this headline"> </a></h4>
<p>If <code class="docutils literal notranslate"><span class="pre">SPARK_HOME</span></code> is set to a version of Spark other than the one in the client, you should unset the <code class="docutils literal notranslate"><span class="pre">SPARK_HOME</span></code> variable and try again.</p>
<p>Check your IDE environment variable settings, your <code class="docutils literal notranslate"><span class="pre">.bashrc</span></code>, <code class="docutils literal notranslate"><span class="pre">.zshrc</span></code>, or <code class="docutils literal notranslate"><span class="pre">.bash_profile</span></code> file, and anywhere else environment variables might be set. You will most likely have to quit and restart your IDE to purge the old state, and you may even need to create a new project if the problem persists.</p>
<p>You should not need to set <code class="docutils literal notranslate"><span class="pre">SPARK_HOME</span></code> to a new value; unsetting it should be sufficient.</p>
</div>
</div>
<div class="section" id="conflicting-or-missing-path-entry-for-binaries">
<h3><a class="toc-backref" href="#id22">Conflicting or Missing <code class="docutils literal notranslate"><span class="pre">PATH</span></code> entry for binaries</a><a class="headerlink" href="#conflicting-or-missing-path-entry-for-binaries" title="Permalink to this headline"> </a></h3>
<p>It is possible your PATH is configured so that commands like <code class="docutils literal notranslate"><span class="pre">spark-shell</span></code> will be running some other previously installed binary instead of the one provided with Databricks Connect. This can cause <code class="docutils literal notranslate"><span class="pre">databricks-connect</span> <span class="pre">test</span></code> to fail. You should make sure either the Databricks Connect binaries take precedence, or remove the previously installed ones.</p>
<p>If you can’t run commands like <code class="docutils literal notranslate"><span class="pre">spark-shell</span></code>, it is also possible your PATH was not automatically set up by <code class="docutils literal notranslate"><span class="pre">pip3</span> <span class="pre">install</span></code> and you’ll need to add the installation <code class="docutils literal notranslate"><span class="pre">bin</span></code> dir to your PATH manually. It’s possible to use Databricks Connect with IDEs even if this isn’t set up. However, the <code class="docutils literal notranslate"><span class="pre">databricks-connect</span> <span class="pre">test</span></code> command will not work.</p>
</div>
<div class="section" id="conflicting-serialization-settings-on-the-cluster">
<h3><a class="toc-backref" href="#id23">Conflicting serialization settings on the cluster</a><a class="headerlink" href="#conflicting-serialization-settings-on-the-cluster" title="Permalink to this headline"> </a></h3>
<p>If you see “stream corrupted” errors when running <code class="docutils literal notranslate"><span class="pre">databricks-connect</span> <span class="pre">test</span></code>, this may be due to incompatible cluster serialization configs. For example, setting the <code class="docutils literal notranslate"><span class="pre">spark.io.compression.codec</span></code> config can cause this issue. To resolve this issue, consider removing these configs from the cluster settings, or setting the configuration in the Databricks Connect client.</p>
</div>
<div class="section" id="cannot-find-winutilsexe-on-windows">
<span id="cannot-find-winutilsexe-on-windows-legacy"></span><h3><a class="toc-backref" href="#id24">Cannot find <code class="docutils literal notranslate"><span class="pre">winutils.exe</span></code> on Windows</a><a class="headerlink" href="#cannot-find-winutilsexe-on-windows" title="Permalink to this headline"> </a></h3>
<p>If you are using Databricks Connect on Windows and see:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
</pre></div>
</div>
<p>Follow the instructions to <a class="reference external" href="https://cwiki.apache.org/confluence/display/HADOOP2/Hadoop2OnWindows">configure the Hadoop path on Windows</a>.</p>
</div>
<div class="section" id="the-filename-directory-name-or-volume-label-syntax-is-incorrect-on-windows">
<h3><a class="toc-backref" href="#id25">The filename, directory name, or volume label syntax is incorrect on Windows</a><a class="headerlink" href="#the-filename-directory-name-or-volume-label-syntax-is-incorrect-on-windows" title="Permalink to this headline"> </a></h3>
<p>If you are using Windows and Databricks Connect and see:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>The filename, directory name, or volume label syntax is incorrect.
</pre></div>
</div>
<p>Either Java or Databricks Connect was installed into a directory with a <a class="reference external" href="https://stackoverflow.com/questions/47028892/why-does-spark-shell-fail-with-the-filename-directory-name-or-volume-label-sy">space in your path</a>. You can work around this by either installing into a directory path without spaces, or configuring your path using the <a class="reference external" href="https://stackoverflow.com/questions/892555/how-do-i-specify-c-program-files-without-a-space-in-it-for-programs-that-cant">short name form</a>.</p>
</div>
</div>
<div class="section" id="limitations">
<span id="aad-tokens"></span><h2>Limitations<a class="headerlink" href="#limitations" title="Permalink to this headline"> </a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="../data-governance/unity-catalog/index.html"><span class="doc">Unity Catalog</span></a>.</p></li>
</ul>
<ul class="simple">
<li><p>Structured Streaming.</p></li>
<li><p>Running arbitrary code that is not a part of a Spark job on the remote cluster.</p></li>
<li><p>Native Scala, Python, and R APIs for Delta table operations (for example, <code class="docutils literal notranslate"><span class="pre">DeltaTable.forPath</span></code>) are not supported. However, the SQL API (<code class="docutils literal notranslate"><span class="pre">spark.sql(...)</span></code>) with Delta Lake operations and the Spark API (for example, <code class="docutils literal notranslate"><span class="pre">spark.read.load</span></code>) on Delta tables are both supported.</p></li>
<li><p>Copy into.</p></li>
<li><p>Using SQL functions, Python or Scala UDFs which are part of the server’s catalog. However, locally introduced Scala and Python UDFs work.</p></li>
<li><p><a class="reference external" href="https://zeppelin.apache.org/">Apache Zeppelin</a> 0.7.x and below.</p></li>
<li><p>Connecting to clusters with <a class="reference internal" href="../data-governance/table-acls/table-acl.html"><span class="doc">table access control</span></a>.</p></li>
<li><p>Connecting to clusters with process isolation enabled (in other words, where <code class="docutils literal notranslate"><span class="pre">spark.databricks.pyspark.enableProcessIsolation</span></code> is set to <code class="docutils literal notranslate"><span class="pre">true</span></code>).</p></li>
<li><p>Delta <code class="docutils literal notranslate"><span class="pre">CLONE</span></code> SQL command.</p></li>
<li><p>Global temporary views.</p></li>
<li><p><a class="reference internal" href="../archive/legacy/koalas.html"><span class="doc">Koalas</span></a> and <code class="docutils literal notranslate"><span class="pre">pyspark.pandas</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CREATE</span> <span class="pre">TABLE</span> <span class="pre">table</span> <span class="pre">AS</span> <span class="pre">SELECT</span> <span class="pre">...</span></code> SQL commands do not always work. Instead, use <code class="docutils literal notranslate"><span class="pre">spark.sql(&quot;SELECT</span> <span class="pre">...&quot;).write.saveAsTable(&quot;table&quot;)</span></code>.</p></li>
</ul>
<ul class="simple">
<li><p>The following <a class="reference internal" href="databricks-utils.html"><span class="doc">Databricks Utilities (dbutils) reference</span></a>:</p>
<ul>
<li><p><a class="reference internal" href="databricks-utils.html#dbutils-credentials"><span class="std std-ref">credentials</span></a></p></li>
<li><p><a class="reference internal" href="../archive/dev-tools/dbutils-library.html"><span class="doc">library</span></a></p></li>
<li><p><a class="reference internal" href="databricks-utils.html#dbutils-workflow"><span class="std std-ref">notebook workflow</span></a></p></li>
<li><p><a class="reference internal" href="databricks-utils.html#dbutils-widgets"><span class="std std-ref">widgets</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="../archive/external-metastores/aws-glue-metastore.html"><span class="doc">AWS Glue catalog</span></a></p></li>
</ul>
</div>
</div>


    
          </div>
        </div>
        <div  class="suapp-rating">
  <div id="suPageRateApp">
     <su-app></su-app>
   </div> 
 </div>
<hr> 
<footer>
  <div role="contentinfo">
      <p class="copyright">
          &copy; Databricks 2023. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the <a href="http://www.apache.org/">Apache Software Foundation</a>.
      </p>
      <p> 
        
          <a id='feedbacklink' href="mailto:doc-feedback@databricks.com?subject=Documentation Feedback">Send us feedback</a>
        
     | <a href="https://databricks.com/privacy-policy">Privacy Policy</a> | <a href="https://databricks.com/terms-of-use">Terms of Use</a></p>

  </div> 

</footer>
      </div>
    </div>
  </section>
</main>

  </page>
  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT: '../',
      VERSION: '1.0',
      COLLAPSE_INDEX: false,
      FILE_SUFFIX: '.html',
      HAS_SOURCE: 'false'
    };
  </script>
  <script type="text/javascript" src="../_static/jquery.js"></script>
  <script type="text/javascript" src="../_static/underscore.js"></script>
  <script type="text/javascript" src="../_static/doctools.js"></script>
  <script type="text/javascript" src="../_static/language_data.js"></script>
  

  <script type="text/javascript" src="../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../_static/js/jquery.waypoints.min.js"></script>

  <!-- Select2 (https://select2.org/) -->
  <link href="https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/css/select2.min.css" rel="stylesheet" />
  <script src="https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/js/select2.min.js"></script>
  <!-- End Select2 -->

  
  
  <script type="text/javascript" src="../_static/js/localized.js"></script>
  <script type="text/javascript" src="../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  
 



  <script>
  window.__searchunifyLoaderConfig = JSON.parse('{"clients": {"en": "02c2e804-27e9-11ee-aefb-0242ac120011", "ja": "6a42c3f2-2820-11ee-aefb-0242ac120011", "pt": "6a86badd-2821-11ee-aefb-0242ac120011"}}')
</script>
<script type="text/javascript" src="../_static/js/search-loader.js"></script>
</body>
<script type='text/javascript'>
  window.onload = function () {
    var description = document.querySelector('meta[name="description"]').getAttribute("content");
    let titleText = document.querySelector('h1').textContent;
    document.querySelector('meta[property="og:title"]').setAttribute("content", titleText);
    document.querySelector('meta[property="og:description"]').setAttribute("content", description);
    document.querySelector('meta[property="twitter:description"]').setAttribute("content", description);
  };
</script>

</html>