<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>Transforming Complex Data Types - Scala - Databricks</title>
<link rel="canonical" href="https://docs.databricks.com/en/optimizations/complex-types.html" />
<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-T85FQ33');</script>
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T85FQ33"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<meta http-equiv="Content-Type" content="text/html; charset=UTF8"><link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/72e6554be4a818a61d5bc121b20e45d9f00a9eeaaa426472a9acc7750b86817a/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/72e6554be4a818a61d5bc121b20e45d9f00a9eeaaa426472a9acc7750b86817a/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/72e6554be4a818a61d5bc121b20e45d9f00a9eeaaa426472a9acc7750b86817a/css/notebook-main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/72e6554be4a818a61d5bc121b20e45d9f00a9eeaaa426472a9acc7750b86817a/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/72e6554be4a818a61d5bc121b20e45d9f00a9eeaaa426472a9acc7750b86817a/login/favicon.ico"/>
<script>window.settings = {"enableSshKeyUI":true,"enableOnDemandClusterType":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","workspaceFeaturedLinks":[{"linkURI":"https://docs.databricks.com/index.html","displayName":"Documentation","icon":"question"},{"linkURI":"https://docs.databricks.com/release-notes/product/index.html","displayName":"Release Notes","icon":"code"},{"linkURI":"https://docs.databricks.com/spark/latest/training/index.html","displayName":"Training & Tutorials","icon":"graduation-cap"}],"enableClearStateFeature":false,"dbcForumURL":"http://forums.databricks.com/","enableProtoClusterInfoDeltaPublisher":false,"maxCustomTags":45,"enableInstanceProfilesUIInJobs":true,"nodeInfo":{"node_types":[{"support_ssh":true,"num_gpus":0,"spark_heap_memory":20396,"instance_type_id":"r3.xlarge","node_type_id":"r3.xlarge","description":"r3.xlarge","support_cluster_tags":true,"container_memory_mb":25495,"node_instance_type":{"instance_type_id":"r3.xlarge","provider":"AWS","compute_units":13.0,"number_of_ips":15,"local_disks":1,"reserved_compute_units":3.64,"memory_mb":31232,"num_cores":4,"reserved_memory_mb":4800},"memory_mb":31232,"category":"Memory Optimized","num_cores":4.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":44632,"instance_type_id":"r3.2xlarge","node_type_id":"r3.2xlarge","description":"r3.2xlarge","support_cluster_tags":true,"container_memory_mb":55790,"node_instance_type":{"instance_type_id":"r3.2xlarge","provider":"AWS","compute_units":26.0,"number_of_ips":15,"local_disks":1,"reserved_compute_units":3.64,"memory_mb":62464,"num_cores":8,"reserved_memory_mb":4800},"memory_mb":62464,"category":"Memory Optimized","num_cores":8.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":93104,"instance_type_id":"r3.4xlarge","node_type_id":"r3.4xlarge","description":"r3.4xlarge","support_cluster_tags":true,"container_memory_mb":116380,"node_instance_type":{"instance_type_id":"r3.4xlarge","provider":"AWS","compute_units":52.0,"number_of_ips":30,"local_disks":1,"reserved_compute_units":3.64,"memory_mb":124928,"num_cores":16,"reserved_memory_mb":4800},"memory_mb":124928,"category":"Memory Optimized","num_cores":16.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":190048,"instance_type_id":"r3.8xlarge","node_type_id":"r3.8xlarge","description":"r3.8xlarge (beta)","support_cluster_tags":true,"container_memory_mb":237560,"node_instance_type":{"instance_type_id":"r3.8xlarge","provider":"AWS","compute_units":104.0,"number_of_ips":30,"local_disks":2,"reserved_compute_units":3.64,"memory_mb":249856,"num_cores":32,"reserved_memory_mb":4800},"memory_mb":249856,"category":"Memory Optimized","num_cores":32.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":8278,"instance_type_id":"r4.large","node_type_id":"r4.large","description":"r4.large (beta)","support_cluster_tags":true,"container_memory_mb":10347,"node_instance_type":{"instance_type_id":"r4.large","provider":"AWS","compute_units":7.0,"number_of_ips":10,"reserved_compute_units":3.64,"memory_mb":15616,"num_cores":2,"reserved_memory_mb":4800},"memory_mb":15616,"category":"Memory Optimized","num_cores":2.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":20396,"instance_type_id":"r4.xlarge","node_type_id":"r4.xlarge","description":"r4.xlarge (beta)","support_cluster_tags":true,"container_memory_mb":25495,"node_instance_type":{"instance_type_id":"r4.xlarge","provider":"AWS","compute_units":13.5,"number_of_ips":15,"reserved_compute_units":3.64,"memory_mb":31232,"num_cores":4,"reserved_memory_mb":4800},"memory_mb":31232,"category":"Memory Optimized","num_cores":4.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":44632,"instance_type_id":"r4.2xlarge","node_type_id":"r4.2xlarge","description":"r4.2xlarge (beta)","support_cluster_tags":true,"container_memory_mb":55790,"node_instance_type":{"instance_type_id":"r4.2xlarge","provider":"AWS","compute_units":27.0,"number_of_ips":15,"reserved_compute_units":3.64,"memory_mb":62464,"num_cores":8,"reserved_memory_mb":4800},"memory_mb":62464,"category":"Memory Optimized","num_cores":8.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":93104,"instance_type_id":"r4.4xlarge","node_type_id":"r4.4xlarge","description":"r4.4xlarge (beta)","support_cluster_tags":true,"container_memory_mb":116380,"node_instance_type":{"instance_type_id":"r4.4xlarge","provider":"AWS","compute_units":53.0,"number_of_ips":30,"reserved_compute_units":3.64,"memory_mb":124928,"num_cores":16,"reserved_memory_mb":4800},"memory_mb":124928,"category":"Memory Optimized","num_cores":16.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":190048,"instance_type_id":"r4.8xlarge","node_type_id":"r4.8xlarge","description":"r4.8xlarge (beta)","support_cluster_tags":true,"container_memory_mb":237560,"node_instance_type":{"instance_type_id":"r4.8xlarge","provider":"AWS","compute_units":99.0,"number_of_ips":30,"reserved_compute_units":3.64,"memory_mb":249856,"num_cores":32,"reserved_memory_mb":4800},"memory_mb":249856,"category":"Memory Optimized","num_cores":32.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":383936,"instance_type_id":"r4.16xlarge","node_type_id":"r4.16xlarge","description":"r4.16xlarge (beta)","support_cluster_tags":true,"container_memory_mb":479920,"node_instance_type":{"instance_type_id":"r4.16xlarge","provider":"AWS","compute_units":195.0,"number_of_ips":50,"reserved_compute_units":3.64,"memory_mb":499712,"num_cores":64,"reserved_memory_mb":4800},"memory_mb":499712,"category":"Memory Optimized","num_cores":64.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":8079,"instance_type_id":"c3.2xlarge","node_type_id":"c3.2xlarge","description":"c3.2xlarge (beta)","support_cluster_tags":true,"container_memory_mb":10099,"node_instance_type":{"instance_type_id":"c3.2xlarge","provider":"AWS","compute_units":28.0,"number_of_ips":15,"local_disks":2,"reserved_compute_units":3.64,"memory_mb":15360,"num_cores":8,"reserved_memory_mb":4800},"memory_mb":15360,"category":"Compute Optimized","num_cores":8.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":19998,"instance_type_id":"c3.4xlarge","node_type_id":"c3.4xlarge","description":"c3.4xlarge","support_cluster_tags":true,"container_memory_mb":24998,"node_instance_type":{"instance_type_id":"c3.4xlarge","provider":"AWS","compute_units":55.0,"number_of_ips":30,"local_disks":2,"reserved_compute_units":3.64,"memory_mb":30720,"num_cores":16,"reserved_memory_mb":4800},"memory_mb":30720,"category":"Compute Optimized","num_cores":16.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":43837,"instance_type_id":"c3.8xlarge","node_type_id":"c3.8xlarge","description":"c3.8xlarge","support_cluster_tags":true,"container_memory_mb":54796,"node_instance_type":{"instance_type_id":"c3.8xlarge","provider":"AWS","compute_units":108.0,"number_of_ips":30,"local_disks":2,"reserved_compute_units":3.64,"memory_mb":61440,"num_cores":32,"reserved_memory_mb":4800},"memory_mb":61440,"category":"Compute Optimized","num_cores":32.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":8079,"instance_type_id":"c4.2xlarge","node_type_id":"c4.2xlarge","description":"c4.2xlarge (beta)","support_cluster_tags":true,"container_memory_mb":10099,"node_instance_type":{"instance_type_id":"c4.2xlarge","provider":"AWS","compute_units":31.0,"number_of_ips":15,"reserved_compute_units":3.64,"memory_mb":15360,"num_cores":8,"reserved_memory_mb":4800},"memory_mb":15360,"category":"Compute Optimized","num_cores":8.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":19998,"instance_type_id":"c4.4xlarge","node_type_id":"c4.4xlarge","description":"c4.4xlarge (beta)","support_cluster_tags":true,"container_memory_mb":24998,"node_instance_type":{"instance_type_id":"c4.4xlarge","provider":"AWS","compute_units":62.0,"number_of_ips":30,"reserved_compute_units":3.64,"memory_mb":30720,"num_cores":16,"reserved_memory_mb":4800},"memory_mb":30720,"category":"Compute Optimized","num_cores":16.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":43837,"instance_type_id":"c4.8xlarge","node_type_id":"c4.8xlarge","description":"c4.8xlarge (beta)","support_cluster_tags":true,"container_memory_mb":54796,"node_instance_type":{"instance_type_id":"c4.8xlarge","provider":"AWS","compute_units":132.0,"number_of_ips":30,"reserved_compute_units":3.64,"memory_mb":61440,"num_cores":36,"reserved_memory_mb":4800},"memory_mb":61440,"category":"Compute Optimized","num_cores":36.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":20396,"instance_type_id":"i2.xlarge","node_type_id":"i2.xlarge","description":"i2.xlarge","support_cluster_tags":true,"container_memory_mb":25495,"node_instance_type":{"instance_type_id":"i2.xlarge","provider":"AWS","compute_units":14.0,"number_of_ips":15,"local_disks":1,"reserved_compute_units":3.64,"memory_mb":31232,"num_cores":4,"reserved_memory_mb":4800},"memory_mb":31232,"category":"Storage Optimized","num_cores":4.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":44632,"instance_type_id":"i2.2xlarge","node_type_id":"i2.2xlarge","description":"i2.2xlarge","support_cluster_tags":true,"container_memory_mb":55790,"node_instance_type":{"instance_type_id":"i2.2xlarge","provider":"AWS","compute_units":27.0,"number_of_ips":15,"local_disks":2,"reserved_compute_units":3.64,"memory_mb":62464,"num_cores":8,"reserved_memory_mb":4800},"memory_mb":62464,"category":"Storage Optimized","num_cores":8.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":93104,"instance_type_id":"i2.4xlarge","node_type_id":"i2.4xlarge","description":"i2.4xlarge","support_cluster_tags":true,"container_memory_mb":116380,"node_instance_type":{"instance_type_id":"i2.4xlarge","provider":"AWS","compute_units":53.0,"number_of_ips":30,"local_disks":4,"reserved_compute_units":3.64,"memory_mb":124928,"num_cores":16,"reserved_memory_mb":4800},"memory_mb":124928,"category":"Storage Optimized","num_cores":16.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":190048,"instance_type_id":"i2.8xlarge","node_type_id":"i2.8xlarge","description":"i2.8xlarge","support_cluster_tags":true,"container_memory_mb":237560,"node_instance_type":{"instance_type_id":"i2.8xlarge","provider":"AWS","compute_units":104.0,"number_of_ips":30,"local_disks":8,"reserved_compute_units":3.64,"memory_mb":249856,"num_cores":32,"reserved_memory_mb":4800},"memory_mb":249856,"category":"Storage Optimized","num_cores":32.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":1,"spark_heap_memory":44632,"instance_type_id":"p2.xlarge","node_type_id":"p2.xlarge","description":"p2.xlarge (beta)","support_cluster_tags":true,"container_memory_mb":55790,"node_instance_type":{"instance_type_id":"p2.xlarge","provider":"AWS","compute_units":12.0,"number_of_ips":15,"local_disks":0,"reserved_compute_units":3.64,"gpus":1,"memory_mb":62464,"num_cores":4,"reserved_memory_mb":4800},"memory_mb":62464,"category":"GPU Accelerated","num_cores":4.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":8,"spark_heap_memory":383936,"instance_type_id":"p2.8xlarge","node_type_id":"p2.8xlarge","description":"p2.8xlarge (beta)","support_cluster_tags":true,"container_memory_mb":479920,"node_instance_type":{"instance_type_id":"p2.8xlarge","provider":"AWS","compute_units":94.0,"number_of_ips":30,"local_disks":0,"reserved_compute_units":3.64,"gpus":8,"memory_mb":499712,"num_cores":32,"reserved_memory_mb":4800},"memory_mb":499712,"category":"GPU Accelerated","num_cores":32.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":16,"spark_heap_memory":577824,"instance_type_id":"p2.16xlarge","node_type_id":"p2.16xlarge","description":"p2.16xlarge (beta)","support_cluster_tags":true,"container_memory_mb":722280,"node_instance_type":{"instance_type_id":"p2.16xlarge","provider":"AWS","compute_units":188.0,"number_of_ips":30,"local_disks":0,"reserved_compute_units":3.64,"gpus":16,"memory_mb":749568,"num_cores":64,"reserved_memory_mb":4800},"memory_mb":749568,"category":"GPU Accelerated","num_cores":64.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":2516,"instance_type_id":"m4.large","node_type_id":"m4.large","description":"m4.large (beta)","support_cluster_tags":true,"container_memory_mb":3146,"node_instance_type":{"instance_type_id":"m4.large","provider":"AWS","compute_units":6.5,"number_of_ips":10,"reserved_compute_units":3.64,"memory_mb":8192,"num_cores":2,"reserved_memory_mb":4800},"memory_mb":8192,"category":"General Purpose","num_cores":2.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":8873,"instance_type_id":"m4.xlarge","node_type_id":"m4.xlarge","description":"m4.xlarge (beta)","support_cluster_tags":true,"container_memory_mb":11092,"node_instance_type":{"instance_type_id":"m4.xlarge","provider":"AWS","compute_units":13.0,"number_of_ips":15,"reserved_compute_units":3.64,"memory_mb":16384,"num_cores":4,"reserved_memory_mb":4800},"memory_mb":16384,"category":"General Purpose","num_cores":4.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":21587,"instance_type_id":"m4.2xlarge","node_type_id":"m4.2xlarge","description":"m4.2xlarge (beta)","support_cluster_tags":true,"container_memory_mb":26984,"node_instance_type":{"instance_type_id":"m4.2xlarge","provider":"AWS","compute_units":26.0,"number_of_ips":15,"reserved_compute_units":3.64,"memory_mb":32768,"num_cores":8,"reserved_memory_mb":4800},"memory_mb":32768,"category":"General Purpose","num_cores":8.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":47015,"instance_type_id":"m4.4xlarge","node_type_id":"m4.4xlarge","description":"m4.4xlarge (beta)","support_cluster_tags":true,"container_memory_mb":58769,"node_instance_type":{"instance_type_id":"m4.4xlarge","provider":"AWS","compute_units":53.5,"number_of_ips":30,"reserved_compute_units":3.64,"memory_mb":65536,"num_cores":16,"reserved_memory_mb":4800},"memory_mb":65536,"category":"General Purpose","num_cores":16.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":123299,"instance_type_id":"m4.10xlarge","node_type_id":"m4.10xlarge","description":"m4.10xlarge (beta)","support_cluster_tags":true,"container_memory_mb":154124,"node_instance_type":{"instance_type_id":"m4.10xlarge","provider":"AWS","compute_units":124.5,"number_of_ips":30,"reserved_compute_units":3.64,"memory_mb":163840,"num_cores":40,"reserved_memory_mb":4800},"memory_mb":163840,"category":"General Purpose","num_cores":40.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":199583,"instance_type_id":"m4.16xlarge","node_type_id":"m4.16xlarge","description":"m4.16xlarge (beta)","support_cluster_tags":true,"container_memory_mb":249479,"node_instance_type":{"instance_type_id":"m4.16xlarge","provider":"AWS","compute_units":188.0,"number_of_ips":30,"reserved_compute_units":3.64,"memory_mb":262144,"num_cores":64,"reserved_memory_mb":4800},"memory_mb":262144,"category":"General Purpose","num_cores":64.0,"support_ebs_volumes":true},{"support_ssh":false,"spark_heap_memory":23800,"instance_type_id":"r3.2xlarge","node_type_id":"memory-optimized","description":"Memory Optimized (legacy)","support_cluster_tags":false,"container_memory_mb":28000,"node_instance_type":{"instance_type_id":"r3.2xlarge","provider":"AWS","compute_units":26.0,"number_of_ips":15,"local_disks":1,"reserved_compute_units":3.64,"memory_mb":62464,"num_cores":8,"reserved_memory_mb":4800},"memory_mb":30720,"category":"Memory Optimized","num_cores":4.0,"support_ebs_volumes":false},{"support_ssh":false,"spark_heap_memory":9702,"instance_type_id":"c3.4xlarge","node_type_id":"compute-optimized","description":"Compute Optimized (legacy)","support_cluster_tags":false,"container_memory_mb":12128,"node_instance_type":{"instance_type_id":"c3.4xlarge","provider":"AWS","compute_units":55.0,"number_of_ips":30,"local_disks":2,"reserved_compute_units":3.64,"memory_mb":30720,"num_cores":16,"reserved_memory_mb":4800},"memory_mb":15360,"category":"Compute Optimized","num_cores":8.0,"support_ebs_volumes":false}],"default_node_type_id":"r3.xlarge"},"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"maxEbsVolumesPerInstance":10,"isAdmin":true,"deltaProcessingBatchSize":1000,"enableLargeResultDownload":true,"zoneInfos":[{"id":"us-west-2a","isDefault":true}],"enableCustomSpotPricingUIByTier":true,"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":false,"enableMaxConcurrentRuns":true,"enableJobAclsConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"enableNewClustersCreate":false,"clusters":true,"allowRunOnPendingClusters":true,"fileStoreBase":"FileStore","enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":false,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."},{"keyPattern":"\\.*","valuePattern":"\\.*","keyPatternDisplay":"","valuePatternDisplay":"","description":""},{"keyPattern":"spark\\.driver\\.memory","valuePattern":"([1-9][0-9]*)(m|mb|g|gb|M|MB|G|GB|Mb|Gb)","keyPatternDisplay":"spark.driver.memory","valuePatternDisplay":"Positive memory quantity in MB or GB, e.g. 500mb or 10gb","description":"Size of the heap memory allocated to the driver."},{"keyPattern":"spark\\.executor\\.memory","valuePattern":"([1-9][0-9]*)(m|mb|g|gb|M|MB|G|GB|Mb|Gb)","keyPatternDisplay":"spark.executor.memory","valuePatternDisplay":"Positive memory quantity in MB or GB, e.g. 500mb or 10gb","description":"Size of the heap memory allocated to the executors."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":true,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"sparkVersions":[{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.2.x-scala2.11","displayName":"Spark 2.2 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-53c31dd6d34be7f86ab55e233cba199edc4b5c63b22f471bafed7d8b17249be4","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db2-scala2.11","displayName":"Spark 2.1.0-db2 (Scala 2.11)","packageLabel":"spark-image-ab147bdd6662fef83fa48e41d909aa045422585552758dc667f1971876c4486f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-891f16699705b9e2450809a9ef9d564d8d3cb4da8e2940da4b6ebcc98408682d","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.x-hadoop2-scala2.10","displayName":"Spark 1.6.x (Auto-updating, Ubuntu 16.04, Hadoop 2, Scala 2.10 experimental)","packageLabel":"spark-image-24aeb8576408866c458959f6a2735eac51fe3d7e8a10a0d3a197ca40b695c709","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db3-scala2.10","displayName":"Spark 2.0.2-db3 (Scala 2.10)","packageLabel":"spark-image-25dac86138b91b354c5882419df5c45cd3695fb36b3a14ad63ff459cd7ae28b8","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.11","displayName":"Spark 2.1.0-db1 (Scala 2.11)","packageLabel":"spark-image-410e3e28f14e82cb88a9c2adee617c9058601d26fa1a05d1c8a9a45607936ad3","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db2-scala2.10","displayName":"Spark 2.1.0-db2 (Scala 2.10)","packageLabel":"spark-image-0adabd34980ecfcbc6a48ac5c9e3a954cf5ec3a53a3300ae0e392c03299d89ca","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-gpu-scala2.11","displayName":"Spark 2.0 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-71b2df478bebe1c211aa4e476bbb3f0bcc609632b3fc642af359f5086210d518","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop2-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-fe8300ae2b3bf12c79ce76d7e9d2d03995e58d1497cfb6c0d3172efb630cb2df","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db2-scala2.10","displayName":"Spark 2.0.2-db2 (Scala 2.10)","packageLabel":"spark-image-da31030269736668dc2769c6bad245367c9bfb44cc7b89181b42442f2978b7d1","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ml-ubuntu16.04-scala2.11","displayName":"Spark 2.0 Streaming ML (Ubuntu 16.04, Scala 2.11 experimental)","packageLabel":"spark-image-a4c6408b67f19b05378d5ad3d052270f9d514fcfb3218e6bdc148cdfdfc8a26e","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.11, deprecated)","packageLabel":"spark-image-266e94319f2e9a59162c6dac38cd33bc319f22f3fd7066d8110cb3c283ece1ed","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-ff4e0b44f7f3391f3718f9faaefefbe1af5a1a5634e65ce34a34f7c4e8afe299","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.11","displayName":"Spark 2.0.2-db1 (Scala 2.11)","packageLabel":"spark-image-262413f1b6d90cca1ff46ab1b86c07bd9dcdadab85ab6a7a0ac1df64f7f95c5e","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-hadoop1-scala2.10","displayName":"Spark 1.6.x (Auto-updating, Ubuntu 16.04, Hadoop 1, Scala 2.10 experimental)","packageLabel":"spark-image-e9f709ae2e8f042aa8f0e377acecd580b4f60195a1a4b9f03be461d22297eec1","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-4470beac99abf970aa45ded9ca81b158a6664e76acc3df8e142f22a8d548ad53","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.x-scala2.10","displayName":"Spark 2.1 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-6580eb17ca66fbf3cb854312f48f53b8f41dc351d7fff96d0e9348b24c16a8e5","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db2-scala2.11","displayName":"Spark 2.0.2-db2 (Scala 2.11)","packageLabel":"spark-image-1ad51066ad0569734095ff93e9d5779b8f0e523f0fb8171ad36e5c10d6310bbe","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-d17103a23a24e1c5139c2866438db79755c7f4f52c8696838a87f4f5a0760d00","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.0.2-db3-scala2.11","displayName":"Spark 2.0.2-db3 (Scala 2.11)","packageLabel":"spark-image-c8547fab0cc66b918f9a64c7e35532de209d0d0e06da6078ad787ec718b37e7b","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop1-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-b512ba976616af10269438701339a9bd5a97cc386737c6ecffdda5fb31c3cf20","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.10","displayName":"Spark 2.0.2-db1 (Scala 2.10)","packageLabel":"spark-image-1f0edcda3e66839ca2c3e2286978fe2a045be16768c1490274ae88a697bfd8e1","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.10, deprecated)","packageLabel":"spark-image-3d3b230ef7ae74f6de6afa49571e237380161492cb7cd61320de8e4c93fc04be","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-815e9f90c588ae6a82373d78018b6819a4e9e717aee28a186ffe94d38e5c4463","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.1.x-scala2.11","displayName":"Spark 2.1 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-0e4f6d489fabf6da43897417a0fb1bdadbb33b847946dcfb07376c4acbe9679c","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.10","displayName":"Spark 2.1.0-db1 (Scala 2.10)","packageLabel":"spark-image-f6f8dbf354b66625ec8d93132729a03ec28af3c0284a75c0355ee451396e1fc8","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-9ac64523faceb21710de9fdc022f363ee3c9afabe9231d194c6d728a957f5901","upgradable":true,"deprecated":true,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":true,"enableClusterAutoScaling":true,"enableUserVisibleDefaultTags":true,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":true,"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"class-node":1,"m4.2xlarge":0.5,"r4.xlarge":1,"m4.4xlarge":0.5,"r4.16xlarge":8,"p2.8xlarge":16,"m4.10xlarge":0.5,"r3.8xlarge":8,"r4.4xlarge":4,"dev-tier-node":1,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"m4.xlarge":0.5,"r4.8xlarge":8,"r4.large":0.5,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":6,"memory-optimized":1,"m4.large":0.5,"p2.16xlarge":24,"c3.2xlarge":1,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"c3.4xlarge":2,"g2.2xlarge":1.5,"p2.xlarge":2,"m4.16xlarge":0.5,"c4.8xlarge":4,"r3.xlarge":1,"r4.2xlarge":2,"i2.8xlarge":12},"enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":true,"enableClusterTagsUI":true,"enableNotebookHistoryDiffing":true,"branch":"2.40.322","accountsLimit":-1,"enableSparkEnvironmentVariables":true,"enableX509Authentication":false,"enableNotebookGitBranching":true,"local":false,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"displayDefaultContainerMemoryGB":30,"enableNotebookCommandMode":true,"disableS3TableImport":false,"deploymentMode":"staging","useSpotForWorkers":true,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"enableCssTransitions":true,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":true,"useTempS3UrlForTableUpload":false,"notifyLastLogin":true,"enableSshKeyUIByTier":true,"enableNotebookGitVersioning":true,"files":"files/","feedbackEmail":"help@databricks.com","enableDriverLogsUI":true,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":2047,"enableNewClustersList":false,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"enableSparkEnvironmentVariablesUI":false,"defaultSparkVersion":{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-ff4e0b44f7f3391f3718f9faaefefbe1af5a1a5634e65ce34a34f7c4e8afe299","upgradable":true,"deprecated":false,"customerVisible":true},"enableCustomSpotPricing":true,"enableMountAclsConfig":false,"useDevTierHomePage":false,"enableClusterClone":false,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":true,"enableClusterAclsByTier":true,"databricksDocsBaseUrl":"https://docs.databricks.com/","disallowAddingAdmins":false,"enableSparkConfUI":true,"featureTier":"UNKNOWN_TIER","enableOrgSwitcherUI":false,"clustersLimit":-1,"enableJdbcImport":true,"logfiles":"logfiles/","enableWebappSharding":false,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":true,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableMountAclService":true,"enableWorkspaceAcls":true,"maxClusterTagKeyLength":127,"gitHash":"d440bd553ebdc1bd5992aa96fc6e74206ea922ac","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableMiniClusters":false,"enableDebugUI":true,"enableStreamingMetricsDashboard":true,"allowNonAdminUsers":true,"enableSingleSignOnByTier":true,"enableJobsRetryOnTimeout":true,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/72e6554be4a818a61d5bc121b20e45d9f00a9eeaaa426472a9acc7750b86817a/","enableSpotClusterType":true,"enableSparkPackages":true,"dynamicSparkVersions":true,"enableClusterTagsUIByTier":true,"enableNotebookHistoryUI":true,"enableClusterLoggingUI":false,"showDebugCounters":true,"enableInstanceProfilesUI":true,"enableFolderHtmlExport":true,"homepageFeaturedLinks":[{"linkURI":"https://databricks-michael.s3.amazonaws.com/databricks.logs.html","displayName":"Databricks Streaming Logs","icon":"img/home/add_data_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/databricks-for-data-scientists.html","displayName":"Databricks for Data Scientists","icon":null},{"linkURI":"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html","displayName":"Introduction to Structured Streaming","icon":null}],"enableEBSVolumesUIByTier":true,"upgradeURL":"","notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":true,"defaultMemoryPerContainerMB":28000,"enablePresenceUI":true,"accounts":true,"useOnDemandClustersByDefault":false,"useFramedStaticNotebooks":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4,"enableNewClustersGet":false,"showSqlProxyUI":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":3898667,"name":"Transforming Complex Data Types - Scala","language":"scala","commands":[{"version":"CommandV1","origId":3898670,"guid":"d89e41ec-c4df-494d-8549-5de8ac4cb7e0","subtype":"command","commandType":"auto","position":0.5,"command":"%md # Transforming Complex Data Types in Spark SQL\n\nIn this notebook we're going to go through some data transformation examples using Spark SQL. Spark SQL supports many\nbuilt-in transformation functions in the module `org.apache.spark.sql.functions._` therefore we will start off by importing that.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ecc9381b-a819-4ba8-8dac-426a75d36407"},{"version":"CommandV1","origId":3898669,"guid":"f18dc11a-53eb-4312-ac43-104ef7e245f7","subtype":"command","commandType":"auto","position":1.0,"command":"import org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\n\n// Convenience function for turning JSON strings into DataFrames.\ndef jsonToDataFrame(json: String, schema: StructType = null): DataFrame = {\n  // SparkSessions are available with Spark 2.0+\n  val reader = spark.read\n  Option(schema).foreach(reader.schema)\n  reader.json(sc.parallelize(Array(json)))\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\njsonToDataFrame: (json: String, schema: org.apache.spark.sql.types.StructType)org.apache.spark.sql.DataFrame\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:40: error: not found: type DataFrame\n       def jsonToDataFrame(json: String, schema: StructType = null): DataFrame = {\n                                                                     ^\n</div>","error":null,"workflows":[],"startTime":1.487355835416E12,"submitTime":1.487355834887E12,"finishTime":1.487355835719E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"13df8946-76dc-4756-b5db-ea54224c74e7"},{"version":"CommandV1","origId":3898673,"guid":"45f03b97-ac3e-4a45-a635-09e4c200eb03","subtype":"command","commandType":"auto","position":2.0,"command":"%md <b>Selecting from nested columns</b> - Dots (`\".\"`) can be used to access nested columns for structs and maps.\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c6236f1f-8bb0-4f63-887f-4f83e880ac67"},{"version":"CommandV1","origId":3898702,"guid":"a31d069c-9b0a-4a78-88f7-af03e0e150c1","subtype":"command","commandType":"auto","position":2.5,"command":"// Using a struct\nval schema = new StructType().add(\"a\", new StructType().add(\"b\", IntegerType))\n                          \nval events = jsonToDataFrame(\"\"\"\n{\n  \"a\": {\n     \"b\": 1\n  }\n}\n\"\"\", schema)\n\ndisplay(events.select(\"a.b\"))","commandVersion":0,"state":"finished","results":{"type":"table","data":[[1.0]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"b","type":"\"integer\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:42: error: overloaded method value apply with alternatives:\n  (fields: Array[org.apache.spark.sql.types.StructField])org.apache.spark.sql.types.StructType &lt;and&gt;\n  (fields: java.util.List[org.apache.spark.sql.types.StructField])org.apache.spark.sql.types.StructType &lt;and&gt;\n  (fields: Seq[org.apache.spark.sql.types.StructField])org.apache.spark.sql.types.StructType\n cannot be applied to ()\nval schema = new StructType().add(&quot;a&quot;, StructType().add(&quot;b&quot;, IntegerType()))\n                                       ^\n&lt;console&gt;:42: error: org.apache.spark.sql.types.IntegerType.type does not take parameters\nval schema = new StructType().add(&quot;a&quot;, StructType().add(&quot;b&quot;, IntegerType()))\n                                                                        ^\n</div>","error":null,"workflows":[],"startTime":1.487355870279E12,"submitTime":1.487355869743E12,"finishTime":1.487355870657E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9b458026-3d9b-41ab-8e0e-6bc848951ddd"},{"version":"CommandV1","origId":3898703,"guid":"a65d9699-23ea-408e-a437-68bf8a7d038e","subtype":"command","commandType":"auto","position":2.75,"command":"// Using a map\nval schema = new StructType().add(\"a\", MapType(StringType, IntegerType))\n                          \nval events = jsonToDataFrame(\"\"\"\n{\n  \"a\": {\n     \"b\": 1\n  }\n}\n\"\"\", schema)\n\ndisplay(events.select(\"a.b\"))","commandVersion":0,"state":"finished","results":{"type":"table","data":[[1.0]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"b","type":"\"integer\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:45: error: type add is not a member of object org.apache.spark.sql.types.StructType\nval schema = new StructType.add(&quot;a&quot;, MapType(StringType, IntegerType))\n                            ^\n</div>","error":null,"workflows":[],"startTime":1.487355914519E12,"submitTime":1.487355913993E12,"finishTime":1.487355914891E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"56194aa0-87c5-4481-8c84-b5f820ca6f77"},{"version":"CommandV1","origId":3898675,"guid":"f304591d-9d51-4588-b1a0-56a260b11fe0","subtype":"command","commandType":"auto","position":3.0,"command":"%md <b>Flattening structs</b> - A star (`\"*\"`) can be used to select all of the subfields in a struct.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"fe020f1b-d160-4eaa-96ff-c7b48addb566"},{"version":"CommandV1","origId":3898705,"guid":"5e3a56bd-2d44-4db0-880f-1ddf9b7602fe","subtype":"command","commandType":"auto","position":3.5,"command":"val events = jsonToDataFrame(\"\"\"\n{\n  \"a\": {\n     \"b\": 1,\n     \"c\": 2\n  }\n}\n\"\"\")\n\ndisplay(events.select(\"a.*\"))","commandVersion":0,"state":"finished","results":{"type":"table","data":[[1.0,2.0]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"b","type":"\"long\"","metadata":"{}"},{"name":"c","type":"\"long\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":null,"error":null,"workflows":[],"startTime":1.487355921025E12,"submitTime":1.48735592053E12,"finishTime":1.487355922233E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"acf0cc53-f40b-4d3a-8230-3d95de4408d2"},{"version":"CommandV1","origId":3898676,"guid":"a58ade27-b4ab-4c99-8956-6ad589212ff5","subtype":"command","commandType":"auto","position":4.0,"command":"%md <b>Nesting columns</b> - The `struct()` function or just parentheses in SQL can be used to create a new struct.\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6dcc0985-8d69-47db-b81f-ac50f9af67ed"},{"version":"CommandV1","origId":3898706,"guid":"48850ada-dd22-4499-a62f-4170a20fb42c","subtype":"command","commandType":"auto","position":4.5,"command":"val events = jsonToDataFrame(\"\"\"\n{\n  \"a\": 1,\n  \"b\": 2,\n  \"c\": 3\n}\n\"\"\")\n\ndisplay(events.select(struct('a as 'y) as 'x))","commandVersion":0,"state":"finished","results":{"type":"table","data":[[[1.0]]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"x","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"y\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":"<div class=\"ansiout\">Unclosed block</div>","error":null,"workflows":[],"startTime":1.487355950062E12,"submitTime":1.487355949569E12,"finishTime":1.48735595048E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"58b8ef0e-8dcb-454c-947b-7c1fff4d58b6"},{"version":"CommandV1","origId":3898677,"guid":"a4cb3eb4-46e8-4206-930a-9dfca291a42d","subtype":"command","commandType":"auto","position":5.0,"command":"%md <b>Nesting all columns</b> - The star (`\"*\"`) can also be used to include all columns in a nested struct.\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"602030bf-df1a-48a5-a116-e99543b6c10f"},{"version":"CommandV1","origId":3898708,"guid":"cdd3182d-a7a5-4172-b910-bf5b9bee13f4","subtype":"command","commandType":"auto","position":5.5,"command":"val events = jsonToDataFrame(\"\"\"\n{\n  \"a\": 1,\n  \"b\": 2\n}\n\"\"\")\n\ndisplay(events.select(struct(\"*\") as 'x))","commandVersion":0,"state":"finished","results":{"type":"table","data":[[[1.0,2.0]]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"x","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"a\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"b\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":null,"error":null,"workflows":[],"startTime":1.487355989016E12,"submitTime":1.487355988525E12,"finishTime":1.487355989459E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3af0e811-b46a-4b50-92bb-46c1eff31224"},{"version":"CommandV1","origId":3898678,"guid":"c1620664-f6d5-4fa4-9d95-27cb95816e49","subtype":"command","commandType":"auto","position":6.0,"command":"%md <b>Selecting a single array or map element</b> - `getItem()` or square brackets (i.e. `[ ]`) can be used to select a single element out of an array or a map.\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a5e89313-7b37-41fe-8d14-0ca3644ea1bf"},{"version":"CommandV1","origId":3898709,"guid":"987a7115-aaa5-4f71-bb13-ac498b502789","subtype":"command","commandType":"auto","position":6.5,"command":"val events = jsonToDataFrame(\"\"\"\n{\n  \"a\": [1, 2]\n}\n\"\"\")\n\ndisplay(events.select('a.getItem(0) as 'x))","commandVersion":0,"state":"finished","results":{"type":"table","data":[[1.0]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"x","type":"\"long\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":null,"error":null,"workflows":[],"startTime":1.487356025513E12,"submitTime":1.487356021981E12,"finishTime":1.487356025876E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"528ef819-9a31-453d-8d34-4671afd41526"},{"version":"CommandV1","origId":3898710,"guid":"7476f808-b7bc-40e7-8e46-340c750e4096","subtype":"command","commandType":"auto","position":6.75,"command":"// Using a map\nval schema = new StructType().add(\"a\", MapType(StringType, IntegerType))\n\nval events = jsonToDataFrame(\"\"\"\n{\n  \"a\": {\n    \"b\": 1\n  }\n}\n\"\"\", schema)\n\ndisplay(events.select('a.getItem(\"b\") as 'x))","commandVersion":0,"state":"finished","results":{"type":"table","data":[[1.0]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"x","type":"\"integer\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":null,"error":null,"workflows":[],"startTime":1.487356057171E12,"submitTime":1.487356056663E12,"finishTime":1.487356057656E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ea9702f7-e9b9-4168-87ca-6e54b05fd2ff"},{"version":"CommandV1","origId":3898679,"guid":"abc6ee17-f4d7-482d-adef-0667594e4fd3","subtype":"command","commandType":"auto","position":7.0,"command":"%md <b>Creating a row for each array or map element</b> - `explode()` can be used to create a new row for each element in an array or each key-value pair.  This is similar to `LATERAL VIEW EXPLODE` in HiveQL.\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"96c8f195-7e4d-45d2-aa5c-42e6098c3d08"},{"version":"CommandV1","origId":3898681,"guid":"ecc87511-d5ca-4eb0-8b52-977718aae4b8","subtype":"command","commandType":"auto","position":8.0,"command":"val events = jsonToDataFrame(\"\"\"\n{\n  \"a\": [1, 2]\n}\n\"\"\")\n\ndisplay(events.select(explode('a) as 'x))","commandVersion":0,"state":"finished","results":{"type":"table","data":[[1.0],[2.0]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"x","type":"\"long\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:51: error: type mismatch;\n found   : String(&quot;a&quot;)\n required: org.apache.spark.sql.Column\ndisplay(events.select(explode(&quot;a&quot;) as 'x))\n                              ^\n</div>","error":null,"workflows":[],"startTime":1.487356121135E12,"submitTime":1.487356120646E12,"finishTime":1.487356121478E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b86376ca-6e00-4f85-962b-7f64e5393675"},{"version":"CommandV1","origId":3898713,"guid":"073ce782-e923-4091-a11d-e81b3e90dd6b","subtype":"command","commandType":"auto","position":8.0625,"command":"// Using a map\nval schema = new StructType().add(\"a\", MapType(StringType, IntegerType))\n\nval events = jsonToDataFrame(\"\"\"\n{\n  \"a\": {\n    \"b\": 1,\n    \"c\": 2\n  }\n}\n\"\"\", schema)\n\ndisplay(events.select(explode('a) as (Seq(\"x\", \"y\"))))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["b",1.0],["c",2.0]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"x","type":"\"string\"","metadata":"{}"},{"name":"y","type":"\"integer\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:58: error: overloaded method value as with alternatives:\n  (alias: Symbol)org.apache.spark.sql.Column &lt;and&gt;\n  (aliases: Array[String])org.apache.spark.sql.Column &lt;and&gt;\n  (aliases: Seq[String])org.apache.spark.sql.Column &lt;and&gt;\n  (alias: String)org.apache.spark.sql.Column &lt;and&gt;\n  [U](implicit evidence$1: org.apache.spark.sql.Encoder[U])org.apache.spark.sql.TypedColumn[Any,U]\n cannot be applied to (Seq[Symbol])\ndisplay(events.select(explode('a) as (Seq('x, 'y))))\n                                  ^\n</div>","error":null,"workflows":[],"startTime":1.487356319197E12,"submitTime":1.487356318651E12,"finishTime":1.48735631952E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"29737cb4-9a81-4ad9-981c-f588958be0b2"},{"version":"CommandV1","origId":3898685,"guid":"750bda08-f5b8-426d-95e4-0184a83a4bf8","subtype":"command","commandType":"auto","position":8.125,"command":"%md <b>Collecting multiple rows into an array</b> - `collect_list()` and `collect_set()` can be used to aggregate items into an array.\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"01bbc112-2e9b-465f-acd2-de93c8f70b0f"},{"version":"CommandV1","origId":3898684,"guid":"20462949-0829-48d8-bb10-566f94be7430","subtype":"command","commandType":"auto","position":8.25,"command":"val events = jsonToDataFrame(\"\"\"\n[{ \"x\": 1 }, { \"x\": 2 }]\n\"\"\")\n\ndisplay(events.select(collect_list('x) as 'x))","commandVersion":0,"state":"finished","results":{"type":"table","data":[[[1.0,2.0]]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"x","type":"{\"type\":\"array\",\"elementType\":\"long\",\"containsNull\":true}","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":null,"error":null,"workflows":[],"startTime":1.48735639957E12,"submitTime":1.487356399031E12,"finishTime":1.487356399907E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"13a8015e-ed85-4d89-a1d3-fd6cf990ccd8"},{"version":"CommandV1","origId":3898683,"guid":"bf2e7e49-ff77-4f1c-897a-1c5cbd421d53","subtype":"command","commandType":"auto","position":8.5,"command":"// using an aggregation\nval events = jsonToDataFrame(\"\"\"\n[{ \"x\": 1, \"y\": \"a\" }, { \"x\": 2, \"y\": \"b\" }]\n\"\"\")\n\ndisplay(events.groupBy(\"y\").agg(collect_list('x) as 'x))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["b",[2.0]],["a",[1.0]]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"y","type":"\"string\"","metadata":"{}"},{"name":"x","type":"{\"type\":\"array\",\"elementType\":\"long\",\"containsNull\":true}","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":null,"error":null,"workflows":[],"startTime":1.487356412409E12,"submitTime":1.4873564118E12,"finishTime":1.487356412919E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b7fccad0-dfe4-467a-94ba-52aeafa97673"},{"version":"CommandV1","origId":3898686,"guid":"d187793e-19c8-4b00-8380-ddacf4c1f9f1","subtype":"command","commandType":"auto","position":8.75,"command":"%md <b>Selecting one field from each item in an array</b> - when you use dot notation on an array we return a new array where that field has been selected from each array element.\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"50f5046a-29c9-4ad2-949b-f013fb023cb8"},{"version":"CommandV1","origId":3898718,"guid":"6fb1e7b4-73fa-4238-8faf-0f4b79ef0930","subtype":"command","commandType":"auto","position":8.8125,"command":"val events = jsonToDataFrame(\"\"\"\n{\n  \"a\": [\n    {\"b\": 1},\n    {\"b\": 2}\n  ]\n}\n\"\"\")\n\ndisplay(events.select(\"a.b\"))","commandVersion":0,"state":"finished","results":{"type":"table","data":[[[1.0,2.0]]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"b","type":"{\"type\":\"array\",\"elementType\":\"long\",\"containsNull\":true}","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":null,"error":null,"workflows":[],"startTime":1.487356370276E12,"submitTime":1.487356368973E12,"finishTime":1.48735637062E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"dc1014d3-b618-440d-bd3d-9277f94ec5d4"},{"version":"CommandV1","origId":3898687,"guid":"2f3db374-bd4a-45da-a9f9-2d2903372f2c","subtype":"command","commandType":"auto","position":8.875,"command":"%md <b>Convert a group of columns to json</b> - `to_json()` can be used to turn structs into json strings. This method is particularly useful when you would like to re-encode multiple columns into a single one when writing data out to Kafka. This method is not presently available in SQL.\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6c4469a8-08b3-4402-abc7-13a16d5e4bda"},{"version":"CommandV1","origId":3898719,"guid":"1e73fcab-9b48-4582-b59a-4d4841e1d06d","subtype":"command","commandType":"auto","position":8.90625,"command":"val events = jsonToDataFrame(\"\"\"\n{\n  \"a\": {\n    \"b\": 1\n  }\n}\n\"\"\")\n\ndisplay(events.select(to_json('a) as 'c))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["{\"b\":1}"]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"c","type":"\"string\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:53: error: type mismatch;\n found   : String(&quot;a&quot;)\n required: org.apache.spark.sql.Column\ndisplay(events.select(to_json(&quot;a&quot;).alias(&quot;c&quot;)))\n                              ^\n</div>","error":null,"workflows":[],"startTime":1.487356436179E12,"submitTime":1.487356435623E12,"finishTime":1.487356436542E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"0709e92e-8892-49f9-b019-6f490b0eeffc"},{"version":"CommandV1","origId":3898688,"guid":"2096232d-ae53-4e42-a965-6026699e616f","subtype":"command","commandType":"auto","position":8.9375,"command":"%md <b>Parse a column containing json</b> - `from_json()` can be used to turn a string column with json data into a struct. Then you may flatten the struct as described above to have individual columns. This method is not presently available in SQL. \n**This method is available since Spark 2.1**","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"83765ddc-be25-4cf5-8870-4c879e442a84"},{"version":"CommandV1","origId":3898682,"guid":"61ae1ef1-80dd-42cd-a53a-aa471e9cb057","subtype":"command","commandType":"auto","position":9.0,"command":"val events = jsonToDataFrame(\"\"\"\n{\n  \"a\": \"{\\\"b\\\":1}\"\n}\n\"\"\")\n\nval schema = new StructType().add(\"b\", IntegerType)\ndisplay(events.select(from_json('a, schema) as 'c))","commandVersion":0,"state":"finished","results":{"type":"table","data":[[[1.0]]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"c","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"b\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":"org.apache.spark.sql.AnalysisException: cannot resolve '`a`' given input columns: [_corrupt_record];;","error":"<div class=\"ansiout\">'Project [jsontostruct(StructField(b,IntegerType,true), 'a) AS c#93078]\n+- LogicalRDD [_corrupt_record#93075]\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:77)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:74)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:309)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:282)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:292)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2$1.apply(QueryPlan.scala:296)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$7.apply(QueryPlan.scala:301)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:301)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:74)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:67)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:128)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:67)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:63)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2803)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1129)\n\tat line0bdb67413dc9487fb5fe2b53821aba05119.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:54)\n\tat line0bdb67413dc9487fb5fe2b53821aba05119.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:63)\n\tat line0bdb67413dc9487fb5fe2b53821aba05119.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:65)\n\tat line0bdb67413dc9487fb5fe2b53821aba05119.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:67)\n\tat line0bdb67413dc9487fb5fe2b53821aba05119.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:69)\n\tat line0bdb67413dc9487fb5fe2b53821aba05119.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:71)\n\tat line0bdb67413dc9487fb5fe2b53821aba05119.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:73)\n\tat line0bdb67413dc9487fb5fe2b53821aba05119.$read$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:75)\n\tat line0bdb67413dc9487fb5fe2b53821aba05119.$read$$iw$$iw.&lt;init&gt;(&lt;console&gt;:77)\n\tat line0bdb67413dc9487fb5fe2b53821aba05119.$read$$iw.&lt;init&gt;(&lt;console&gt;:79)\n\tat line0bdb67413dc9487fb5fe2b53821aba05119.$eval$.$print$lzycompute(&lt;console&gt;:7)\n\tat line0bdb67413dc9487fb5fe2b53821aba05119.$eval$.$print(&lt;console&gt;:6)</div>","workflows":[],"startTime":1.487356480705E12,"submitTime":1.487356480131E12,"finishTime":1.487356481071E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"47ae6805-db1e-4b2e-adff-275ecc0d274e"},{"version":"CommandV1","origId":3898973,"guid":"1486b743-fc68-4f29-a190-ee2679838d50","subtype":"command","commandType":"auto","position":9.5,"command":"%md Sometimes you may want to leave a part of the JSON string still as JSON to avoid too much complexity in your schema.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ddfee2ed-551c-41fe-a507-67684ad9d1ee"},{"version":"CommandV1","origId":3898974,"guid":"5492cc7e-2d90-4df5-8991-f289089f8085","subtype":"command","commandType":"auto","position":9.75,"command":"val events = jsonToDataFrame(\"\"\"\n{\n  \"a\": \"{\\\"b\\\":{\\\"x\\\":1,\\\"y\\\":{\\\"z\\\":2}}}\"\n}\n\"\"\")\n\nval schema = new StructType().add(\"b\", new StructType().add(\"x\", IntegerType)\n  .add(\"y\", StringType))\ndisplay(events.select(from_json('a, schema) as 'c))","commandVersion":0,"state":"finished","results":{"type":"table","data":[[[[1.0,"{\"z\":2}"]]]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"c","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"b\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"x\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"y\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:55: error: overloaded method value select with alternatives:\n  [U1](c1: org.apache.spark.sql.TypedColumn[org.apache.spark.sql.Row,U1])org.apache.spark.sql.Dataset[U1] &lt;and&gt;\n  (col: String,cols: String*)org.apache.spark.sql.DataFrame &lt;and&gt;\n  (cols: org.apache.spark.sql.Column*)org.apache.spark.sql.DataFrame\n cannot be applied to (org.apache.spark.sql.DataFrame)\ndisplay(events.select(events.select(from_json('a, schema) as 'c)))\n               ^\n</div>","error":null,"workflows":[],"startTime":1.487363945145E12,"submitTime":1.487363943777E12,"finishTime":1.487363945603E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"bb60a816-6ad2-4618-badf-e2ef4b05cd90"},{"version":"CommandV1","origId":3898689,"guid":"bca95366-e935-40f3-ba84-88cdff385b6b","subtype":"command","commandType":"auto","position":10.0,"command":"%md <b>Parse a set of fields from a column containing json</b> - `json_tuple()` can be used to extract a fields available in a string column with json data.\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"5c9019ff-7e67-469d-80c7-e379c29bb40f"},{"version":"CommandV1","origId":3898721,"guid":"c2598e83-da6e-43f1-bffa-075ec3add3c4","subtype":"command","commandType":"auto","position":11.0,"command":"val events = jsonToDataFrame(\"\"\"\n{\n  \"a\": \"{\\\"b\\\":1}\"\n}\n\"\"\")\n\ndisplay(events.select(json_tuple('a, \"b\") as 'c))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["1"]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"c","type":"\"string\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":"org.apache.spark.sql.AnalysisException: cannot resolve '`a`' given input columns: [_corrupt_record];;","error":"<div class=\"ansiout\">'Project [json_tuple('a, b) AS c#93093]\n+- LogicalRDD [_corrupt_record#93090]\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:77)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:74)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:309)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5$$anonfun$apply$11.apply(TreeNode.scala:360)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:282)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:292)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2$1.apply(QueryPlan.scala:296)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$7.apply(QueryPlan.scala:301)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:301)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:74)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:67)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:128)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:67)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:63)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2803)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1129)\n\tat line0bdb67413dc9487fb5fe2b53821aba05123.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:51)\n\tat line0bdb67413dc9487fb5fe2b53821aba05123.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:59)\n\tat line0bdb67413dc9487fb5fe2b53821aba05123.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:61)\n\tat line0bdb67413dc9487fb5fe2b53821aba05123.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:63)\n\tat line0bdb67413dc9487fb5fe2b53821aba05123.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:65)\n\tat line0bdb67413dc9487fb5fe2b53821aba05123.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:67)\n\tat line0bdb67413dc9487fb5fe2b53821aba05123.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:69)\n\tat line0bdb67413dc9487fb5fe2b53821aba05123.$read$$iw$$iw$$iw.&lt;init&gt;(&lt;console&gt;:71)\n\tat line0bdb67413dc9487fb5fe2b53821aba05123.$read$$iw$$iw.&lt;init&gt;(&lt;console&gt;:73)\n\tat line0bdb67413dc9487fb5fe2b53821aba05123.$read$$iw.&lt;init&gt;(&lt;console&gt;:75)\n\tat line0bdb67413dc9487fb5fe2b53821aba05123.$eval$.$print$lzycompute(&lt;console&gt;:7)\n\tat line0bdb67413dc9487fb5fe2b53821aba05123.$eval$.$print(&lt;console&gt;:6)</div>","workflows":[],"startTime":1.487356514008E12,"submitTime":1.487356513473E12,"finishTime":1.487356514329E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"44e50244-3652-416d-bb1e-628b2204e064"},{"version":"CommandV1","origId":3898968,"guid":"55f0519c-b0ae-45be-bf9c-0b2d321a735b","subtype":"command","commandType":"auto","position":12.0,"command":"%md <b>Parse a well formed string column</b> - `regexp_extract()` can be used to parse strings using regular expressions.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"54519244-fd5b-4bac-aef5-00d4257fce11"},{"version":"CommandV1","origId":3898969,"guid":"e4fad5c5-afa6-4dbc-b918-5cef9e181b25","subtype":"command","commandType":"auto","position":13.0,"command":"val events = jsonToDataFrame(\"\"\"\n[{ \"a\": \"x: 1\" }, { \"a\": \"y: 2\" }]\n\"\"\")\n\ndisplay(events.select(regexp_extract('a, \"([a-z]):\", 1) as 'c))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["x"],["y"]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"c","type":"\"string\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":null,"error":null,"workflows":[],"startTime":1.487361743627E12,"submitTime":1.487361743134E12,"finishTime":1.487361744025E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b88603ad-75fc-4c8b-bc92-b0f91667f655"}],"dashboards":[],"guid":"82970f32-6bc8-427f-b941-4f2ea0880845","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/72e6554be4a818a61d5bc121b20e45d9f00a9eeaaa426472a9acc7750b86817a/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/72e6554be4a818a61d5bc121b20e45d9f00a9eeaaa426472a9acc7750b86817a/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>
