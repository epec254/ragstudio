<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>CloudTrail ETL - Scala - Databricks</title>
<link rel="canonical" href="https://docs.databricks.com/en/structured-streaming/examples.html" />

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-T85FQ33');</script>
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T85FQ33"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/72e6554be4a818a61d5bc121b20e45d9f00a9eeaaa426472a9acc7750b86817a/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/72e6554be4a818a61d5bc121b20e45d9f00a9eeaaa426472a9acc7750b86817a/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/72e6554be4a818a61d5bc121b20e45d9f00a9eeaaa426472a9acc7750b86817a/css/notebook-main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/72e6554be4a818a61d5bc121b20e45d9f00a9eeaaa426472a9acc7750b86817a/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/72e6554be4a818a61d5bc121b20e45d9f00a9eeaaa426472a9acc7750b86817a/login/favicon.ico"/>
<script>window.settings = {"enableSshKeyUI":true,"enableOnDemandClusterType":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","workspaceFeaturedLinks":[{"linkURI":"https://docs.databricks.com/index.html","displayName":"Documentation","icon":"question"},{"linkURI":"https://docs.databricks.com/release-notes/product/index.html","displayName":"Release Notes","icon":"code"},{"linkURI":"https://docs.databricks.com/spark/latest/training/index.html","displayName":"Training & Tutorials","icon":"graduation-cap"}],"enableClearStateFeature":false,"dbcForumURL":"http://forums.databricks.com/","enableProtoClusterInfoDeltaPublisher":false,"maxCustomTags":45,"enableInstanceProfilesUIInJobs":true,"nodeInfo":{"node_types":[{"support_ssh":true,"num_gpus":0,"spark_heap_memory":20396,"instance_type_id":"r3.xlarge","node_type_id":"r3.xlarge","description":"r3.xlarge","support_cluster_tags":true,"container_memory_mb":25495,"node_instance_type":{"instance_type_id":"r3.xlarge","provider":"AWS","compute_units":13.0,"number_of_ips":15,"local_disks":1,"reserved_compute_units":3.64,"memory_mb":31232,"num_cores":4,"reserved_memory_mb":4800},"memory_mb":31232,"category":"Memory Optimized","num_cores":4.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":44632,"instance_type_id":"r3.2xlarge","node_type_id":"r3.2xlarge","description":"r3.2xlarge","support_cluster_tags":true,"container_memory_mb":55790,"node_instance_type":{"instance_type_id":"r3.2xlarge","provider":"AWS","compute_units":26.0,"number_of_ips":15,"local_disks":1,"reserved_compute_units":3.64,"memory_mb":62464,"num_cores":8,"reserved_memory_mb":4800},"memory_mb":62464,"category":"Memory Optimized","num_cores":8.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":93104,"instance_type_id":"r3.4xlarge","node_type_id":"r3.4xlarge","description":"r3.4xlarge","support_cluster_tags":true,"container_memory_mb":116380,"node_instance_type":{"instance_type_id":"r3.4xlarge","provider":"AWS","compute_units":52.0,"number_of_ips":30,"local_disks":1,"reserved_compute_units":3.64,"memory_mb":124928,"num_cores":16,"reserved_memory_mb":4800},"memory_mb":124928,"category":"Memory Optimized","num_cores":16.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":190048,"instance_type_id":"r3.8xlarge","node_type_id":"r3.8xlarge","description":"r3.8xlarge (beta)","support_cluster_tags":true,"container_memory_mb":237560,"node_instance_type":{"instance_type_id":"r3.8xlarge","provider":"AWS","compute_units":104.0,"number_of_ips":30,"local_disks":2,"reserved_compute_units":3.64,"memory_mb":249856,"num_cores":32,"reserved_memory_mb":4800},"memory_mb":249856,"category":"Memory Optimized","num_cores":32.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":8278,"instance_type_id":"r4.large","node_type_id":"r4.large","description":"r4.large (beta)","support_cluster_tags":true,"container_memory_mb":10347,"node_instance_type":{"instance_type_id":"r4.large","provider":"AWS","compute_units":7.0,"number_of_ips":10,"reserved_compute_units":3.64,"memory_mb":15616,"num_cores":2,"reserved_memory_mb":4800},"memory_mb":15616,"category":"Memory Optimized","num_cores":2.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":20396,"instance_type_id":"r4.xlarge","node_type_id":"r4.xlarge","description":"r4.xlarge (beta)","support_cluster_tags":true,"container_memory_mb":25495,"node_instance_type":{"instance_type_id":"r4.xlarge","provider":"AWS","compute_units":13.5,"number_of_ips":15,"reserved_compute_units":3.64,"memory_mb":31232,"num_cores":4,"reserved_memory_mb":4800},"memory_mb":31232,"category":"Memory Optimized","num_cores":4.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":44632,"instance_type_id":"r4.2xlarge","node_type_id":"r4.2xlarge","description":"r4.2xlarge (beta)","support_cluster_tags":true,"container_memory_mb":55790,"node_instance_type":{"instance_type_id":"r4.2xlarge","provider":"AWS","compute_units":27.0,"number_of_ips":15,"reserved_compute_units":3.64,"memory_mb":62464,"num_cores":8,"reserved_memory_mb":4800},"memory_mb":62464,"category":"Memory Optimized","num_cores":8.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":93104,"instance_type_id":"r4.4xlarge","node_type_id":"r4.4xlarge","description":"r4.4xlarge (beta)","support_cluster_tags":true,"container_memory_mb":116380,"node_instance_type":{"instance_type_id":"r4.4xlarge","provider":"AWS","compute_units":53.0,"number_of_ips":30,"reserved_compute_units":3.64,"memory_mb":124928,"num_cores":16,"reserved_memory_mb":4800},"memory_mb":124928,"category":"Memory Optimized","num_cores":16.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":190048,"instance_type_id":"r4.8xlarge","node_type_id":"r4.8xlarge","description":"r4.8xlarge (beta)","support_cluster_tags":true,"container_memory_mb":237560,"node_instance_type":{"instance_type_id":"r4.8xlarge","provider":"AWS","compute_units":99.0,"number_of_ips":30,"reserved_compute_units":3.64,"memory_mb":249856,"num_cores":32,"reserved_memory_mb":4800},"memory_mb":249856,"category":"Memory Optimized","num_cores":32.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":383936,"instance_type_id":"r4.16xlarge","node_type_id":"r4.16xlarge","description":"r4.16xlarge (beta)","support_cluster_tags":true,"container_memory_mb":479920,"node_instance_type":{"instance_type_id":"r4.16xlarge","provider":"AWS","compute_units":195.0,"number_of_ips":50,"reserved_compute_units":3.64,"memory_mb":499712,"num_cores":64,"reserved_memory_mb":4800},"memory_mb":499712,"category":"Memory Optimized","num_cores":64.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":8079,"instance_type_id":"c3.2xlarge","node_type_id":"c3.2xlarge","description":"c3.2xlarge (beta)","support_cluster_tags":true,"container_memory_mb":10099,"node_instance_type":{"instance_type_id":"c3.2xlarge","provider":"AWS","compute_units":28.0,"number_of_ips":15,"local_disks":2,"reserved_compute_units":3.64,"memory_mb":15360,"num_cores":8,"reserved_memory_mb":4800},"memory_mb":15360,"category":"Compute Optimized","num_cores":8.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":19998,"instance_type_id":"c3.4xlarge","node_type_id":"c3.4xlarge","description":"c3.4xlarge","support_cluster_tags":true,"container_memory_mb":24998,"node_instance_type":{"instance_type_id":"c3.4xlarge","provider":"AWS","compute_units":55.0,"number_of_ips":30,"local_disks":2,"reserved_compute_units":3.64,"memory_mb":30720,"num_cores":16,"reserved_memory_mb":4800},"memory_mb":30720,"category":"Compute Optimized","num_cores":16.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":43837,"instance_type_id":"c3.8xlarge","node_type_id":"c3.8xlarge","description":"c3.8xlarge","support_cluster_tags":true,"container_memory_mb":54796,"node_instance_type":{"instance_type_id":"c3.8xlarge","provider":"AWS","compute_units":108.0,"number_of_ips":30,"local_disks":2,"reserved_compute_units":3.64,"memory_mb":61440,"num_cores":32,"reserved_memory_mb":4800},"memory_mb":61440,"category":"Compute Optimized","num_cores":32.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":8079,"instance_type_id":"c4.2xlarge","node_type_id":"c4.2xlarge","description":"c4.2xlarge (beta)","support_cluster_tags":true,"container_memory_mb":10099,"node_instance_type":{"instance_type_id":"c4.2xlarge","provider":"AWS","compute_units":31.0,"number_of_ips":15,"reserved_compute_units":3.64,"memory_mb":15360,"num_cores":8,"reserved_memory_mb":4800},"memory_mb":15360,"category":"Compute Optimized","num_cores":8.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":19998,"instance_type_id":"c4.4xlarge","node_type_id":"c4.4xlarge","description":"c4.4xlarge (beta)","support_cluster_tags":true,"container_memory_mb":24998,"node_instance_type":{"instance_type_id":"c4.4xlarge","provider":"AWS","compute_units":62.0,"number_of_ips":30,"reserved_compute_units":3.64,"memory_mb":30720,"num_cores":16,"reserved_memory_mb":4800},"memory_mb":30720,"category":"Compute Optimized","num_cores":16.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":43837,"instance_type_id":"c4.8xlarge","node_type_id":"c4.8xlarge","description":"c4.8xlarge (beta)","support_cluster_tags":true,"container_memory_mb":54796,"node_instance_type":{"instance_type_id":"c4.8xlarge","provider":"AWS","compute_units":132.0,"number_of_ips":30,"reserved_compute_units":3.64,"memory_mb":61440,"num_cores":36,"reserved_memory_mb":4800},"memory_mb":61440,"category":"Compute Optimized","num_cores":36.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":20396,"instance_type_id":"i2.xlarge","node_type_id":"i2.xlarge","description":"i2.xlarge","support_cluster_tags":true,"container_memory_mb":25495,"node_instance_type":{"instance_type_id":"i2.xlarge","provider":"AWS","compute_units":14.0,"number_of_ips":15,"local_disks":1,"reserved_compute_units":3.64,"memory_mb":31232,"num_cores":4,"reserved_memory_mb":4800},"memory_mb":31232,"category":"Storage Optimized","num_cores":4.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":44632,"instance_type_id":"i2.2xlarge","node_type_id":"i2.2xlarge","description":"i2.2xlarge","support_cluster_tags":true,"container_memory_mb":55790,"node_instance_type":{"instance_type_id":"i2.2xlarge","provider":"AWS","compute_units":27.0,"number_of_ips":15,"local_disks":2,"reserved_compute_units":3.64,"memory_mb":62464,"num_cores":8,"reserved_memory_mb":4800},"memory_mb":62464,"category":"Storage Optimized","num_cores":8.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":93104,"instance_type_id":"i2.4xlarge","node_type_id":"i2.4xlarge","description":"i2.4xlarge","support_cluster_tags":true,"container_memory_mb":116380,"node_instance_type":{"instance_type_id":"i2.4xlarge","provider":"AWS","compute_units":53.0,"number_of_ips":30,"local_disks":4,"reserved_compute_units":3.64,"memory_mb":124928,"num_cores":16,"reserved_memory_mb":4800},"memory_mb":124928,"category":"Storage Optimized","num_cores":16.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":190048,"instance_type_id":"i2.8xlarge","node_type_id":"i2.8xlarge","description":"i2.8xlarge","support_cluster_tags":true,"container_memory_mb":237560,"node_instance_type":{"instance_type_id":"i2.8xlarge","provider":"AWS","compute_units":104.0,"number_of_ips":30,"local_disks":8,"reserved_compute_units":3.64,"memory_mb":249856,"num_cores":32,"reserved_memory_mb":4800},"memory_mb":249856,"category":"Storage Optimized","num_cores":32.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":1,"spark_heap_memory":44632,"instance_type_id":"p2.xlarge","node_type_id":"p2.xlarge","description":"p2.xlarge (beta)","support_cluster_tags":true,"container_memory_mb":55790,"node_instance_type":{"instance_type_id":"p2.xlarge","provider":"AWS","compute_units":12.0,"number_of_ips":15,"local_disks":0,"reserved_compute_units":3.64,"gpus":1,"memory_mb":62464,"num_cores":4,"reserved_memory_mb":4800},"memory_mb":62464,"category":"GPU Accelerated","num_cores":4.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":8,"spark_heap_memory":383936,"instance_type_id":"p2.8xlarge","node_type_id":"p2.8xlarge","description":"p2.8xlarge (beta)","support_cluster_tags":true,"container_memory_mb":479920,"node_instance_type":{"instance_type_id":"p2.8xlarge","provider":"AWS","compute_units":94.0,"number_of_ips":30,"local_disks":0,"reserved_compute_units":3.64,"gpus":8,"memory_mb":499712,"num_cores":32,"reserved_memory_mb":4800},"memory_mb":499712,"category":"GPU Accelerated","num_cores":32.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":16,"spark_heap_memory":577824,"instance_type_id":"p2.16xlarge","node_type_id":"p2.16xlarge","description":"p2.16xlarge (beta)","support_cluster_tags":true,"container_memory_mb":722280,"node_instance_type":{"instance_type_id":"p2.16xlarge","provider":"AWS","compute_units":188.0,"number_of_ips":30,"local_disks":0,"reserved_compute_units":3.64,"gpus":16,"memory_mb":749568,"num_cores":64,"reserved_memory_mb":4800},"memory_mb":749568,"category":"GPU Accelerated","num_cores":64.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":2516,"instance_type_id":"m4.large","node_type_id":"m4.large","description":"m4.large (beta)","support_cluster_tags":true,"container_memory_mb":3146,"node_instance_type":{"instance_type_id":"m4.large","provider":"AWS","compute_units":6.5,"number_of_ips":10,"reserved_compute_units":3.64,"memory_mb":8192,"num_cores":2,"reserved_memory_mb":4800},"memory_mb":8192,"category":"General Purpose","num_cores":2.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":8873,"instance_type_id":"m4.xlarge","node_type_id":"m4.xlarge","description":"m4.xlarge (beta)","support_cluster_tags":true,"container_memory_mb":11092,"node_instance_type":{"instance_type_id":"m4.xlarge","provider":"AWS","compute_units":13.0,"number_of_ips":15,"reserved_compute_units":3.64,"memory_mb":16384,"num_cores":4,"reserved_memory_mb":4800},"memory_mb":16384,"category":"General Purpose","num_cores":4.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":21587,"instance_type_id":"m4.2xlarge","node_type_id":"m4.2xlarge","description":"m4.2xlarge (beta)","support_cluster_tags":true,"container_memory_mb":26984,"node_instance_type":{"instance_type_id":"m4.2xlarge","provider":"AWS","compute_units":26.0,"number_of_ips":15,"reserved_compute_units":3.64,"memory_mb":32768,"num_cores":8,"reserved_memory_mb":4800},"memory_mb":32768,"category":"General Purpose","num_cores":8.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":47015,"instance_type_id":"m4.4xlarge","node_type_id":"m4.4xlarge","description":"m4.4xlarge (beta)","support_cluster_tags":true,"container_memory_mb":58769,"node_instance_type":{"instance_type_id":"m4.4xlarge","provider":"AWS","compute_units":53.5,"number_of_ips":30,"reserved_compute_units":3.64,"memory_mb":65536,"num_cores":16,"reserved_memory_mb":4800},"memory_mb":65536,"category":"General Purpose","num_cores":16.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":123299,"instance_type_id":"m4.10xlarge","node_type_id":"m4.10xlarge","description":"m4.10xlarge (beta)","support_cluster_tags":true,"container_memory_mb":154124,"node_instance_type":{"instance_type_id":"m4.10xlarge","provider":"AWS","compute_units":124.5,"number_of_ips":30,"reserved_compute_units":3.64,"memory_mb":163840,"num_cores":40,"reserved_memory_mb":4800},"memory_mb":163840,"category":"General Purpose","num_cores":40.0,"support_ebs_volumes":true},{"support_ssh":true,"num_gpus":0,"spark_heap_memory":199583,"instance_type_id":"m4.16xlarge","node_type_id":"m4.16xlarge","description":"m4.16xlarge (beta)","support_cluster_tags":true,"container_memory_mb":249479,"node_instance_type":{"instance_type_id":"m4.16xlarge","provider":"AWS","compute_units":188.0,"number_of_ips":30,"reserved_compute_units":3.64,"memory_mb":262144,"num_cores":64,"reserved_memory_mb":4800},"memory_mb":262144,"category":"General Purpose","num_cores":64.0,"support_ebs_volumes":true},{"support_ssh":false,"spark_heap_memory":23800,"instance_type_id":"r3.2xlarge","node_type_id":"memory-optimized","description":"Memory Optimized (legacy)","support_cluster_tags":false,"container_memory_mb":28000,"node_instance_type":{"instance_type_id":"r3.2xlarge","provider":"AWS","compute_units":26.0,"number_of_ips":15,"local_disks":1,"reserved_compute_units":3.64,"memory_mb":62464,"num_cores":8,"reserved_memory_mb":4800},"memory_mb":30720,"category":"Memory Optimized","num_cores":4.0,"support_ebs_volumes":false},{"support_ssh":false,"spark_heap_memory":9702,"instance_type_id":"c3.4xlarge","node_type_id":"compute-optimized","description":"Compute Optimized (legacy)","support_cluster_tags":false,"container_memory_mb":12128,"node_instance_type":{"instance_type_id":"c3.4xlarge","provider":"AWS","compute_units":55.0,"number_of_ips":30,"local_disks":2,"reserved_compute_units":3.64,"memory_mb":30720,"num_cores":16,"reserved_memory_mb":4800},"memory_mb":15360,"category":"Compute Optimized","num_cores":8.0,"support_ebs_volumes":false}],"default_node_type_id":"r3.xlarge"},"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"maxEbsVolumesPerInstance":10,"isAdmin":true,"deltaProcessingBatchSize":1000,"enableLargeResultDownload":true,"zoneInfos":[{"id":"us-west-2a","isDefault":true}],"enableCustomSpotPricingUIByTier":true,"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":false,"enableMaxConcurrentRuns":true,"enableJobAclsConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"enableNewClustersCreate":false,"clusters":true,"allowRunOnPendingClusters":true,"fileStoreBase":"FileStore","enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":false,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."},{"keyPattern":"\\.*","valuePattern":"\\.*","keyPatternDisplay":"","valuePatternDisplay":"","description":""},{"keyPattern":"spark\\.driver\\.memory","valuePattern":"([1-9][0-9]*)(m|mb|g|gb|M|MB|G|GB|Mb|Gb)","keyPatternDisplay":"spark.driver.memory","valuePatternDisplay":"Positive memory quantity in MB or GB, e.g. 500mb or 10gb","description":"Size of the heap memory allocated to the driver."},{"keyPattern":"spark\\.executor\\.memory","valuePattern":"([1-9][0-9]*)(m|mb|g|gb|M|MB|G|GB|Mb|Gb)","keyPatternDisplay":"spark.executor.memory","valuePatternDisplay":"Positive memory quantity in MB or GB, e.g. 500mb or 10gb","description":"Size of the heap memory allocated to the executors."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":true,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"sparkVersions":[{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.2.x-scala2.11","displayName":"Spark 2.2 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-53c31dd6d34be7f86ab55e233cba199edc4b5c63b22f471bafed7d8b17249be4","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db2-scala2.11","displayName":"Spark 2.1.0-db2 (Scala 2.11)","packageLabel":"spark-image-ab147bdd6662fef83fa48e41d909aa045422585552758dc667f1971876c4486f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-891f16699705b9e2450809a9ef9d564d8d3cb4da8e2940da4b6ebcc98408682d","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.x-hadoop2-scala2.10","displayName":"Spark 1.6.x (Auto-updating, Ubuntu 16.04, Hadoop 2, Scala 2.10 experimental)","packageLabel":"spark-image-24aeb8576408866c458959f6a2735eac51fe3d7e8a10a0d3a197ca40b695c709","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db3-scala2.10","displayName":"Spark 2.0.2-db3 (Scala 2.10)","packageLabel":"spark-image-25dac86138b91b354c5882419df5c45cd3695fb36b3a14ad63ff459cd7ae28b8","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.11","displayName":"Spark 2.1.0-db1 (Scala 2.11)","packageLabel":"spark-image-410e3e28f14e82cb88a9c2adee617c9058601d26fa1a05d1c8a9a45607936ad3","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db2-scala2.10","displayName":"Spark 2.1.0-db2 (Scala 2.10)","packageLabel":"spark-image-0adabd34980ecfcbc6a48ac5c9e3a954cf5ec3a53a3300ae0e392c03299d89ca","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-gpu-scala2.11","displayName":"Spark 2.0 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-71b2df478bebe1c211aa4e476bbb3f0bcc609632b3fc642af359f5086210d518","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop2-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-fe8300ae2b3bf12c79ce76d7e9d2d03995e58d1497cfb6c0d3172efb630cb2df","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db2-scala2.10","displayName":"Spark 2.0.2-db2 (Scala 2.10)","packageLabel":"spark-image-da31030269736668dc2769c6bad245367c9bfb44cc7b89181b42442f2978b7d1","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ml-ubuntu16.04-scala2.11","displayName":"Spark 2.0 Streaming ML (Ubuntu 16.04, Scala 2.11 experimental)","packageLabel":"spark-image-a4c6408b67f19b05378d5ad3d052270f9d514fcfb3218e6bdc148cdfdfc8a26e","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.11, deprecated)","packageLabel":"spark-image-266e94319f2e9a59162c6dac38cd33bc319f22f3fd7066d8110cb3c283ece1ed","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-ff4e0b44f7f3391f3718f9faaefefbe1af5a1a5634e65ce34a34f7c4e8afe299","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.11","displayName":"Spark 2.0.2-db1 (Scala 2.11)","packageLabel":"spark-image-262413f1b6d90cca1ff46ab1b86c07bd9dcdadab85ab6a7a0ac1df64f7f95c5e","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-hadoop1-scala2.10","displayName":"Spark 1.6.x (Auto-updating, Ubuntu 16.04, Hadoop 1, Scala 2.10 experimental)","packageLabel":"spark-image-e9f709ae2e8f042aa8f0e377acecd580b4f60195a1a4b9f03be461d22297eec1","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-4470beac99abf970aa45ded9ca81b158a6664e76acc3df8e142f22a8d548ad53","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.x-scala2.10","displayName":"Spark 2.1 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-6580eb17ca66fbf3cb854312f48f53b8f41dc351d7fff96d0e9348b24c16a8e5","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db2-scala2.11","displayName":"Spark 2.0.2-db2 (Scala 2.11)","packageLabel":"spark-image-1ad51066ad0569734095ff93e9d5779b8f0e523f0fb8171ad36e5c10d6310bbe","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-d17103a23a24e1c5139c2866438db79755c7f4f52c8696838a87f4f5a0760d00","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.0.2-db3-scala2.11","displayName":"Spark 2.0.2-db3 (Scala 2.11)","packageLabel":"spark-image-c8547fab0cc66b918f9a64c7e35532de209d0d0e06da6078ad787ec718b37e7b","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop1-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-b512ba976616af10269438701339a9bd5a97cc386737c6ecffdda5fb31c3cf20","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.10","displayName":"Spark 2.0.2-db1 (Scala 2.10)","packageLabel":"spark-image-1f0edcda3e66839ca2c3e2286978fe2a045be16768c1490274ae88a697bfd8e1","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.10, deprecated)","packageLabel":"spark-image-3d3b230ef7ae74f6de6afa49571e237380161492cb7cd61320de8e4c93fc04be","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-815e9f90c588ae6a82373d78018b6819a4e9e717aee28a186ffe94d38e5c4463","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.1.x-scala2.11","displayName":"Spark 2.1 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-0e4f6d489fabf6da43897417a0fb1bdadbb33b847946dcfb07376c4acbe9679c","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.10","displayName":"Spark 2.1.0-db1 (Scala 2.10)","packageLabel":"spark-image-f6f8dbf354b66625ec8d93132729a03ec28af3c0284a75c0355ee451396e1fc8","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-9ac64523faceb21710de9fdc022f363ee3c9afabe9231d194c6d728a957f5901","upgradable":true,"deprecated":true,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":true,"enableClusterAutoScaling":true,"enableUserVisibleDefaultTags":true,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":true,"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"class-node":1,"m4.2xlarge":0.5,"r4.xlarge":1,"m4.4xlarge":0.5,"r4.16xlarge":8,"p2.8xlarge":16,"m4.10xlarge":0.5,"r3.8xlarge":8,"r4.4xlarge":4,"dev-tier-node":1,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"m4.xlarge":0.5,"r4.8xlarge":8,"r4.large":0.5,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":6,"memory-optimized":1,"m4.large":0.5,"p2.16xlarge":24,"c3.2xlarge":1,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"c3.4xlarge":2,"g2.2xlarge":1.5,"p2.xlarge":2,"m4.16xlarge":0.5,"c4.8xlarge":4,"r3.xlarge":1,"r4.2xlarge":2,"i2.8xlarge":12},"enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":true,"enableClusterTagsUI":true,"enableNotebookHistoryDiffing":true,"branch":"2.40.322","accountsLimit":-1,"enableSparkEnvironmentVariables":true,"enableX509Authentication":false,"enableNotebookGitBranching":true,"local":false,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"displayDefaultContainerMemoryGB":30,"enableNotebookCommandMode":true,"disableS3TableImport":false,"deploymentMode":"staging","useSpotForWorkers":true,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"enableCssTransitions":true,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":true,"useTempS3UrlForTableUpload":false,"notifyLastLogin":true,"enableSshKeyUIByTier":true,"enableNotebookGitVersioning":true,"files":"files/","feedbackEmail":"help@databricks.com","enableDriverLogsUI":true,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":2047,"enableNewClustersList":false,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"enableSparkEnvironmentVariablesUI":false,"defaultSparkVersion":{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-ff4e0b44f7f3391f3718f9faaefefbe1af5a1a5634e65ce34a34f7c4e8afe299","upgradable":true,"deprecated":false,"customerVisible":true},"enableCustomSpotPricing":true,"enableMountAclsConfig":false,"useDevTierHomePage":false,"enableClusterClone":false,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":true,"enableClusterAclsByTier":true,"databricksDocsBaseUrl":"https://docs.databricks.com/","disallowAddingAdmins":false,"enableSparkConfUI":true,"featureTier":"UNKNOWN_TIER","enableOrgSwitcherUI":false,"clustersLimit":-1,"enableJdbcImport":true,"logfiles":"logfiles/","enableWebappSharding":false,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":true,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableMountAclService":true,"enableWorkspaceAcls":true,"maxClusterTagKeyLength":127,"gitHash":"d440bd553ebdc1bd5992aa96fc6e74206ea922ac","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableMiniClusters":false,"enableDebugUI":true,"enableStreamingMetricsDashboard":true,"allowNonAdminUsers":true,"enableSingleSignOnByTier":true,"enableJobsRetryOnTimeout":true,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/72e6554be4a818a61d5bc121b20e45d9f00a9eeaaa426472a9acc7750b86817a/","enableSpotClusterType":true,"enableSparkPackages":true,"dynamicSparkVersions":true,"enableClusterTagsUIByTier":true,"enableNotebookHistoryUI":true,"enableClusterLoggingUI":false,"showDebugCounters":true,"enableInstanceProfilesUI":true,"enableFolderHtmlExport":true,"homepageFeaturedLinks":[{"linkURI":"https://databricks-michael.s3.amazonaws.com/databricks.logs.html","displayName":"Databricks Streaming Logs","icon":"img/home/add_data_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/databricks-for-data-scientists.html","displayName":"Databricks for Data Scientists","icon":null},{"linkURI":"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html","displayName":"Introduction to Structured Streaming","icon":null}],"enableEBSVolumesUIByTier":true,"upgradeURL":"","notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":true,"defaultMemoryPerContainerMB":28000,"enablePresenceUI":true,"accounts":true,"useOnDemandClustersByDefault":false,"useFramedStaticNotebooks":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4,"enableNewClustersGet":false,"showSqlProxyUI":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":3935198,"name":"CloudTrail ETL - Scala","language":"scala","commands":[{"version":"CommandV1","origId":3935200,"guid":"2425b816-b19c-476d-85e5-7b74c25d5d1e","subtype":"command","commandType":"auto","position":1.0,"command":"%md # Streaming ETL on CloudTrail Logs using Structured Streaming\nIn this Scala notebook, we are going to explore how we can use Structured Streaming to perform streaming ETL on CloudTrail logs. For more context, read the Databricks blog. \n\n[AWS CloudTrail](https://aws.amazon.com/cloudtrail/) is a web service that records AWS API calls for your account and delivers audit logs to you as JSON files in a S3 bucket. If you do not have it configured, see their documentations on how to do so. \n\n","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d8e77f50-f747-46d3-ba04-0f547f1c46f5"},{"version":"CommandV1","origId":3935201,"guid":"60343dbc-057b-41a6-9928-7558e26d38f1","subtype":"command","commandType":"auto","position":2.0,"command":"%md ### Step 1: Where is your input data and where do you want your final Parquet table?\nTo run this notebook, first of all, you need to specify the location of the CloudTrail logs files. You can open your CloudTrail configuration, find the bucket and set the value below.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"31b1d825-b18d-482d-9f4a-0a6dc8b7ef83"},{"version":"CommandV1","origId":3935202,"guid":"13972d14-bbcb-48ea-81a5-aa9a3c83c53f","subtype":"command","commandType":"auto","position":2.015625,"command":"val cloudTrailLogsPath = \"s3n://MY_CLOUDTRAIL_BUCKET/AWSLogs/*/CloudTrail/*/2017/01/03/\"\nval parquetOutputPath = \"/cloudtrail/\"  // DBFS or S3 path ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c2828be6-58b1-4e75-b409-7c54123840b2"},{"version":"CommandV1","origId":3935203,"guid":"bf649be2-fd30-48b1-bc5d-37a07c56b3c0","subtype":"command","commandType":"auto","position":2.03125,"command":"%md Note that this uses globs to read logs across all the accounts and all the AWS regions that are being reported to the bucket. If you want to limit your processing to a smaller subset of the logs, change the above path accordingly.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"1faa8529-52fa-4bc1-afda-845cfe9bd1ef"},{"version":"CommandV1","origId":3935204,"guid":"00209d5e-5031-4b5c-99f9-d683e561a164","subtype":"command","commandType":"auto","position":2.0625,"command":"%md ### Step 2: What is the schema of your data?\nTo parse the JSON files, we need to know schema of the JSON data in the log files. Below is the schema defined based on the format defined in [CloudTrail documentation](http://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference.html). It is essentially an array (named Records) of fields related to events, some of which are nested structures.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"394e243a-4a60-49b3-b541-3b304c64387d"},{"version":"CommandV1","origId":3935205,"guid":"e7e7fe79-b332-4f6c-812d-cf39ef5f012a","subtype":"command","commandType":"auto","position":2.125,"command":"import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.streaming._\nimport org.apache.spark.sql.types._\n\nval cloudTrailSchema = new StructType()\n  .add(\"Records\", ArrayType(new StructType()\n    .add(\"additionalEventData\", StringType)\n    .add(\"apiVersion\", StringType)\n    .add(\"awsRegion\", StringType)\n    .add(\"errorCode\", StringType)\n    .add(\"errorMessage\", StringType)\n    .add(\"eventID\", StringType)\n    .add(\"eventName\", StringType)\n    .add(\"eventSource\", StringType)\n    .add(\"eventTime\", StringType)\n    .add(\"eventType\", StringType)\n    .add(\"eventVersion\", StringType)\n    .add(\"readOnly\", BooleanType)\n    .add(\"recipientAccountId\", StringType)\n    .add(\"requestID\", StringType)\n    .add(\"requestParameters\", MapType(StringType, StringType))\n    .add(\"resources\", ArrayType(new StructType()\n      .add(\"ARN\", StringType)\n      .add(\"accountId\", StringType)\n      .add(\"type\", StringType)\n    ))\n    .add(\"responseElements\", MapType(StringType, StringType))\n    .add(\"sharedEventID\", StringType)\n    .add(\"sourceIPAddress\", StringType)\n    .add(\"serviceEventDetails\", MapType(StringType, StringType))\n    .add(\"userAgent\", StringType)\n    .add(\"userIdentity\", new StructType()\n      .add(\"accessKeyId\", StringType)\n      .add(\"accountId\", StringType)\n      .add(\"arn\", StringType)\n      .add(\"invokedBy\", StringType)\n      .add(\"principalId\", StringType)\n      .add(\"sessionContext\", new StructType()\n        .add(\"attributes\", new StructType()\n          .add(\"creationDate\", StringType)\n          .add(\"mfaAuthenticated\", StringType)\n        )\n        .add(\"sessionIssuer\", new StructType()\n          .add(\"accountId\", StringType)\n          .add(\"arn\", StringType)\n          .add(\"principalId\", StringType)\n          .add(\"type\", StringType)\n          .add(\"userName\", StringType)\n        )\n      )\n      .add(\"type\", StringType)\n      .add(\"userName\", StringType)\n      .add(\"webIdFederationData\", new StructType()\n        .add(\"federatedProvider\", StringType)\n        .add(\"attributes\", MapType(StringType, StringType))\n      )\n    )\n    .add(\"vpcEndpointId\", StringType)))","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.484352211777E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"affaa2f3-a46b-4e8f-a245-575a47eb5499"},{"version":"CommandV1","origId":3935206,"guid":"c69346e1-f77a-461e-9291-b67501da0b4f","subtype":"command","commandType":"auto","position":2.25,"command":"%md ### Step 3: Let's do streaming ETL on it!\nNow, we can start reading the data and writing to Parquet table. First, we are going to create the streaming DataFrame that represents the raw records in the files, using the schema we have defined.\nWe are also option `maxFilesPerTrigger` to get earlier access the final Parquet data, as this limit the number of log files processed and written out every trigger.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"63e44de2-dd2a-4fc2-819e-1c319cf02b99"},{"version":"CommandV1","origId":3935207,"guid":"2f3dd62a-1617-4431-ad22-6e5db1cf7c7e","subtype":"command","commandType":"auto","position":2.5,"command":"val rawRecords = spark.readStream\n  .option(\"maxFilesPerTrigger\", \"100\")\n  .schema(cloudTrailSchema)\n  .json(cloudTrailLogsPath)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"74b0db16-505b-45ae-bdb7-c41f0a8a21b0"},{"version":"CommandV1","origId":3935208,"guid":"5898f623-d760-4379-9446-0affc66960a9","subtype":"command","commandType":"auto","position":3.0,"command":"%md Then, we are going to transform the data in the following way.\n\n1. `Explode` (split) the array of records loaded from each file into separate records.\n2. Parse the string event time string in each record to Spark’s timestamp type.\n3. Flatten out the nested columns for easier querying.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6bbe69f0-c430-4e12-a202-2a8f03da922d"},{"version":"CommandV1","origId":3935209,"guid":"e663c743-692a-4264-9362-769ed1460494","subtype":"command","commandType":"auto","position":4.0,"command":"val cloudTrailEvents = rawRecords\n  .select(explode($\"Records\") as \"record\")\n  .select(\n    unix_timestamp($\"record.eventTime\", \"yyyy-MM-dd'T'hh:mm:ss\").cast(\"timestamp\") as \"timestamp\",\n    $\"record.*\")","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"83fae046-566c-48b3-8fb6-213f106e5f7c"},{"version":"CommandV1","origId":3935210,"guid":"ead0aaab-6a2c-46b9-8b98-ade71bc9b5c7","subtype":"command","commandType":"auto","position":5.0,"command":"%md Finally, we can define how to write out the transformed data and start the `StreamingQuery`. We are going to do the following\n\n- Write the data out in the Parquet format, \n- Define the `date` column from that `timestamp` and partition the Parquet data by date for efficient time-slice queries.\n- Define the trigger to be every 10 seconds.\n- Define the checkpoint location\n- Finally, start the query.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"51784669-1fca-4bfc-8041-647f3c861fe0"},{"version":"CommandV1","origId":3935211,"guid":"06684b23-638e-4bf1-9455-d78224181941","subtype":"command","commandType":"auto","position":6.0,"command":"val checkpointPath = \"/cloudtrail.checkpoint/\"\n\nval streamingETLQuery = cloudTrailEvents\n  .withColumn(\"date\", $\"timestamp\".cast(\"date\")) \n  .writeStream\n  .format(\"parquet\")\n  .option(\"path\", parquetOutputPath)\n  .partitionBy(\"date\")\n  .trigger(ProcessingTime(\"10 seconds\"))\n  .option(\"checkpointLocation\", checkpointPath)\n  .start()","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"88a3d6d0-74fe-4d2a-b3c3-fd06161d9d3a"},{"version":"CommandV1","origId":3935212,"guid":"2900edf6-1cca-4416-8e0e-6c5ac21f7876","subtype":"command","commandType":"auto","position":7.0,"command":"%md The `streamingETLQuery` should be running in the background. See the `Details` to understand the progress.  ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"53d9a024-241d-4360-8065-ca68a6a186a6"},{"version":"CommandV1","origId":3935213,"guid":"b44e4f7f-8973-49fb-8994-a93284eb74d7","subtype":"command","commandType":"auto","position":7.5,"command":"%md ### Step 4: Query up-to-the-minute data from Parquet Table\n\nWhile the `streamingETLQuery` is continuously converting the data to Parquet, you can already start running ad-hoc queries on the Parquet table. \nLet's define a table/view in Spark on the Parquet files.\nIf you see an error, then probably the table has not been created yet; wait for a few minutes for the streaming query to write some data and create the table.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"101aa66f-bf1d-4446-855b-62ab6d129a98"},{"version":"CommandV1","origId":3935214,"guid":"6c982d07-f61d-406d-880e-e9eb3ae9c426","subtype":"command","commandType":"auto","position":8.0,"command":"val parquetData = sql(s\"select * from parquet.`$parquetOutputPath`\")\ndisplay(parquetData)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"1ae2ec25-36f3-4c44-b3ed-bebe49de9a9f"},{"version":"CommandV1","origId":3935215,"guid":"74787ac2-9572-4c09-8dc1-3c9478a7d898","subtype":"command","commandType":"auto","position":9.0,"command":"%md If you count the number of rows in the table, you should find the value increasing over time. Run the following every few minutes.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"dc86579b-e9e4-4a59-a34a-d04042557773"},{"version":"CommandV1","origId":3935216,"guid":"5e58ec5c-124a-4ab8-9e92-efeb680845fd","subtype":"command","commandType":"auto","position":10.0,"command":"sql(s\"select * from parquet.`$parquetOutputPath`\").count()","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d66ea4d1-ab2c-4db3-848e-0b146497f79f"}],"dashboards":[],"guid":"36cd8a9b-d181-4a81-8f5f-499f61730996","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/72e6554be4a818a61d5bc121b20e45d9f00a9eeaaa426472a9acc7750b86817a/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/72e6554be4a818a61d5bc121b20e45d9f00a9eeaaa426472a9acc7750b86817a/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>
