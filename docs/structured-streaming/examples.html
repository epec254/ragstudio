

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en-US" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en-US"> <!--<![endif]-->

<head>
  <!-- cookie consent -->
  
    <!-- Combined Onetrust and Rudderstack Implementation Scripts -->
    <!-- Onetrust Initialization -->
    <script type="text/javascript" src="https://cdn.cookielaw.org/consent/92466579-1717-44d3-809d-a05fb02843ed-test/OtAutoBlock.js"></script>
    <script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="92466579-1717-44d3-809d-a05fb02843ed-test"></script>
    <link rel="stylesheet" id="db-onetrust-style" href="https://www.databricks.com/wp-content/uploads/db_onetrust.css" media="all" />
    <!-- Setting Rudderstack Write Key -->
    <script>window.rudderstackKey = "2SOR9fvSr5Fi6tN2ihPbVHnX1SZ" </script>
    <!-- Rudderstack Initialization + Onetrust Integration + Rudderstack Custom Events -->
    <script type="text/javascript" src="https://www.databricks.com/sites/default/files/rudderstack/v1/db-rudderstack-events.js"></script>

  <!-- cookie consent -->

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="X-UA-Compatible" content="IE=9" />
  <meta content="See examples of using Spark Structured Streaming with Cassandra, Azure Synapse Analytics, Python notebooks, and Scala notebooks in Databricks." name="description" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0">
  <meta property="og:image" content="https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:title" content="Structured Streaming patterns on Databricks">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://docs.databricks.com">
  <meta property="og:description" content="" id="og-description">
  <meta name="twitter:image" content="https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png">
  <meta name="twitter:site" content="@databricks">
  <meta name="twitter:creator" content="@databricks">
  <meta property="twitter:description" content="">
  
  <title>Structured Streaming patterns on Databricks &#124; Databricks on AWS</title>
  
  
  <link rel="canonical" href="https://docs.databricks.com/en/structured-streaming/examples.html">
  <!-- Start hreflang tag -->
  <link rel="alternate" hreflang="en" href="https://docs.databricks.com/en/structured-streaming/examples.html" />
<link rel="alternate" hreflang="x-default" href="https://docs.databricks.com/en/structured-streaming/examples.html" />
  <!-- End hreflang tag -->
  
  
  <link rel="shortcut icon" href="../_static/favicon.ico" />
  

  

  

  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;
j.setAttributeNode(d.createAttribute('data-ot-ignore'));
f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-T85FQ33');</script>
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;
j.setAttributeNode(d.createAttribute('data-ot-ignore'));
f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-TWTKQQ');</script>
    
  <!-- End Google Tag Manager -->


  <!-- MaxMind / GEO IP -->
  <script src="//js.maxmind.com/js/apis/geoip2/v2.1/geoip2.js" type="text/javascript"></script>
  <!-- End MaxMind / GEO IP -->

  
  
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600&display=swap" rel="stylesheet">
  <link rel="preload" href="../_static/fonts/DMSans-Bold.ttf" as="font">
  <link rel="preload" href="../_static/fonts/DMSans-Regular.ttf" as="font">
  <link rel="preload" href="../_static/fonts/DMMono-Regular.ttf" as="font">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/cloud-provider-selector.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/translation-selector.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/searchunify/main.css" type="text/css" />

  
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="top" title="Databricks on AWS" href="../index.html" /> 
</head>

<body class="wy-body-for-nav" role="document">

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T85FQ33"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TWTKQQ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  
  <nav class="wy-nav-top header su_header" role="navigation" aria-label="top navigation">
    
<nav class="wy-nav-top header su_header" role="navigation" aria-label="top navigation">
  <div class="container-logo">
    <ul class="mobile-menu-toggle">
        <li class="menu-toggle">
            <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
            
            <a href="https://www.databricks.com/" class="wy-nav-top-logo"><img src="../_static/small-scale-lockup-full-color-rgb.svg" width="137" height="21"
              alt="Databricks" /></a>   
               
              </li>
    </ul>
    <ul class="su_nav-menu">
      <li class="menu-toggle">
        <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
        
          
        
        <a href="https://www.databricks.com/" class="wy-nav-top-logo"><img src="../_static/small-scale-lockup-full-color-rgb.svg" width="137" height="21"
            alt="Databricks" /></a></li>
        <!-- 
<li><a href="https://help.databricks.com/s/">Help Center</a></li>
<li class="active"><a href="https://docs.databricks.com/en/">Documentation</a></li>
<li><a href="https://kb.databricks.com/">Knowledge Base</a></li>
 -->
    </ul>
  </div>
  <div class="su_nav-right">
    <ul class="su_link-mobile">
  <!-- Mobile header code can go here -->
</ul>
<ul class="right-try-list">
   
</ul>
  </div>
</nav>
  </nav>

  <div class="su_sub-header">
    <div class="container">
      <div class="su_sub-header-inner">
        <!-- <div class="su_subnav-menu-right">
  <div id="auto" style="width: 100%;">
    <div ng-controller="SearchautoController">
      <div bind-html-compile="autocompleteHtml">
        <form class="su__search-box-1" disabled="disabled">
          <input class="su__search-input" type="search" name="Search box" id="su__search-b" placeholder="Search Documentation" disabled="disabled"/>
          <button class="su__search-button" type="submit" class="button button-success" disabled="disabled">
            <svg width="24" height="24" viewBox="0 0 24 24">
              <path
                d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"
                fill="#333"></path>
            </svg>
          </button>
        </form>
      </div>
    </div>
  </div>
</div> -->
        <div class="search-lng-gap"></div>
        <div style="margin-left: 16px; margin-right: 16px;">
          <!-- <select name="lng selector" id="lng-selector">
    <option value="../../en/structured-streaming/examples.html" class="notranslate">English</option>
    <option value="../../ja/structured-streaming/examples.html" class="notranslate">日本語</option>
    <option value="../../pt/structured-streaming/examples.html" class="notranslate">Português (Brasil)</option>
</select> -->
        </div>
        <div class="cloud-selector-container">
          <!-- <select name="cloud provider selector" id="cloud-provider-selector">
    <option value="aws" selected class="notranslate">
        Amazon Web Services
    </option>
    <option value="azure"  class="notranslate">
        Microsoft Azure
    </option>
    <option value="gcp"  class="notranslate">
        Google Cloud Platform
    </option>
</select> -->
        </div>
      </div>
    </div>
  </div>
  <page class="js-page-container">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side su_nav-side">
<div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    

    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../index.html" class="main-navigation-home">Databricks on AWS</a>
    

    
      

      
        <p class="caption"><span class="caption-text">Load &amp; manage data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../rag-temp/index.html">RAG Studio</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    
  <p class="build_info notranslate"data-last-edit="December 23, 2023">
    Updated Jan 11, 2024
  </p>
<script>
  window.addEventListener('DOMContentLoaded',function(){
    var h1=document.querySelector('h1');
    var bi=document.querySelector('[data-last-edit]');
    if(h1 && bi){
      var ver = document.createElement('p');
      ver.className = 'version_info';
      ver.textContent = bi.getAttribute('data-last-edit');
      h1.parentElement.insertBefore(ver, h1.nextElementSibling);
    }
  });
</script>

    <p>
      
        <a id='feedbacklink' href="mailto:doc-feedback@databricks.com?subject=Documentation Feedback">Send us feedback</a>
      
    </p>
  </div>
</div>
</nav>
    
    
<main class="wy-grid-for-nav su_nav-grid">
  <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
    <div class="wy-nav-content su__nav_content">
      <div class="rst-content">
        





<div role="navigation" aria-label="breadcrumbs navigation" class="wy-breadcrumbs-wrapper">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>Structured Streaming patterns on Databricks</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>
</div>
        
        <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
          <div itemprop="articleBody">
            
    
  <div class="section" id="structured-streaming-patterns-on-databricks">
<span id="ss-patterns-on-databricks"></span><h1>Structured Streaming patterns on Databricks<a class="headerlink" href="#structured-streaming-patterns-on-databricks" title="Permalink to this headline"> </a></h1>
<p>This contains notebooks and code samples for common patterns for working with Structured Streaming on Databricks.</p>
<p></p>
<div class="section" id="getting-started-with-structured-streaming">
<span id="getting-started-with-ss"></span><span id="getting-started"></span><h2>Getting started with Structured Streaming<a class="headerlink" href="#getting-started-with-structured-streaming" title="Permalink to this headline"> </a></h2>
<p>If you are brand new to Structured Streaming, see <a class="reference internal" href="tutorial.html"><span class="doc">Run your first Structured Streaming workload</span></a>.</p>
</div>
<div class="section" id="write-to-cassandra-as-a-sink-for-structured-streaming-in-python">
<span id="write-to-cassandra-as-a-sink-for-ss-in-python"></span><span id="foreachbatch-cassandra-example"></span><h2>Write to Cassandra as a sink for Structured Streaming in Python<a class="headerlink" href="#write-to-cassandra-as-a-sink-for-structured-streaming-in-python" title="Permalink to this headline"> </a></h2>
<p><a class="reference external" href="https://cassandra.apache.org/">Apache Cassandra</a> is a distributed, low-latency, scalable, highly-available OLTP database.</p>
<p>Structured Streaming works with Cassandra through the <a class="reference external" href="https://github.com/datastax/spark-cassandra-connector">Spark Cassandra Connector</a>. This connector supports both RDD and DataFrame APIs, and it has native support for writing streaming data.
<strong>*Important</strong>* You must use the corresponding version of the <a class="reference external" href="https://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector-assembly">spark-cassandra-connector-assembly</a>.</p>
<p>The following example connects to one or more hosts in a Cassandra database cluster. It also specifies connection configurations such as the checkpoint location and the specific keyspace and table names:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.cassandra.connection.host&quot;</span><span class="p">,</span> <span class="s2">&quot;host1,host2&quot;</span><span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">writeStream</span> \
  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;org.apache.spark.sql.cassandra&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">outputMode</span><span class="p">(</span><span class="s2">&quot;append&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;checkpointLocation&quot;</span><span class="p">,</span> <span class="s2">&quot;/path/to/checkpoint&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;keyspace&quot;</span><span class="p">,</span> <span class="s2">&quot;keyspace_name&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;table&quot;</span><span class="p">,</span> <span class="s2">&quot;table_name&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">start</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="write-to-azure-synapse-analytics-using-foreachbatch-in-python">
<span id="foreachbatch-sqldw-example"></span><h2>Write to Azure Synapse Analytics using <code class="docutils literal notranslate"><span class="pre">foreachBatch()</span></code> in Python<a class="headerlink" href="#write-to-azure-synapse-analytics-using-foreachbatch-in-python" title="Permalink to this headline"> </a></h2>
<p><code class="docutils literal notranslate"><span class="pre">streamingDF.writeStream.foreachBatch()</span></code> allows you to reuse existing batch data writers to write the
output of a streaming query to Azure Synapse Analytics. See the <a class="reference internal" href="foreach.html"><span class="doc">foreachBatch documentation</span></a> for details.</p>
<p>To run this example, you need the Azure Synapse Analytics connector. For details on the Azure Synapse Analytics connector, see <a class="reference internal" href="../connect/external-systems/synapse-analytics.html"><span class="doc">Query data in Azure Synapse Analytics</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="o">*</span>

<span class="k">def</span> <span class="nf">writeToSQLWarehouse</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">epochId</span><span class="p">):</span>
  <span class="n">df</span><span class="o">.</span><span class="n">write</span> \
    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s1">&#39;overwrite&#39;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;url&quot;</span><span class="p">,</span> <span class="s2">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;forward_spark_azure_storage_credentials&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;dbtable&quot;</span><span class="p">,</span> <span class="s2">&quot;my_table_in_dw_copy&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;tempdir&quot;</span><span class="p">,</span> <span class="s2">&quot;wasbs://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.blob.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.sql.shuffle.partitions&quot;</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">)</span>

<span class="n">query</span> <span class="o">=</span> <span class="p">(</span>
  <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;rate&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
    <span class="o">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s2">&quot;value % 10 as key&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">count</span><span class="p">()</span>
    <span class="o">.</span><span class="n">toDF</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;count&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">writeStream</span>
    <span class="o">.</span><span class="n">foreachBatch</span><span class="p">(</span><span class="n">writeToSQLWarehouse</span><span class="p">)</span>
    <span class="o">.</span><span class="n">outputMode</span><span class="p">(</span><span class="s2">&quot;update&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">start</span><span class="p">()</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="write-to-amazon-dynamodb-using-foreach-in-scala-and-python">
<span id="foreach-dynamodb-example"></span><h2>Write to Amazon DynamoDB using <code class="docutils literal notranslate"><span class="pre">foreach()</span></code> in Scala and Python<a class="headerlink" href="#write-to-amazon-dynamodb-using-foreach-in-scala-and-python" title="Permalink to this headline"> </a></h2>
<p><code class="docutils literal notranslate"><span class="pre">streamingDF.writeStream.foreach()</span></code> allows you to write the output of a streaming query to arbitrary locations.</p>
<div class="section" id="use-python">
<h3>Use Python<a class="headerlink" href="#use-python" title="Permalink to this headline"> </a></h3>
<p>This example shows how to use <code class="docutils literal notranslate"><span class="pre">streamingDataFrame.writeStream.foreach()</span></code> in Python to write to DynamoDB.  The first step gets the DynamoDB boto resource. This example is written to use <code class="docutils literal notranslate"><span class="pre">access_key</span></code> and <code class="docutils literal notranslate"><span class="pre">secret_key</span></code>, but Databricks recommends that you use instance profiles. See <a class="reference internal" href="../connect/storage/tutorial-s3-instance-profile.html"><span class="doc">Tutorial: Configure S3 access with an instance profile</span></a>.</p>
<ol class="arabic">
<li><p>Define a few helper methods to create DynamoDB table for running the example.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">table_name</span> <span class="o">=</span> <span class="s2">&quot;PythonForeachTest&quot;</span>

<span class="k">def</span> <span class="nf">get_dynamodb</span><span class="p">():</span>
  <span class="kn">import</span> <span class="nn">boto3</span>

  <span class="n">access_key</span> <span class="o">=</span> <span class="s2">&quot;&lt;access key&gt;&quot;</span>
  <span class="n">secret_key</span> <span class="o">=</span> <span class="s2">&quot;&lt;secret key&gt;&quot;</span>
  <span class="n">region</span> <span class="o">=</span> <span class="s2">&quot;&lt;region name&gt;&quot;</span>
  <span class="k">return</span> <span class="n">boto3</span><span class="o">.</span><span class="n">resource</span><span class="p">(</span><span class="s1">&#39;dynamodb&#39;</span><span class="p">,</span>
                 <span class="n">aws_access_key_id</span><span class="o">=</span><span class="n">access_key</span><span class="p">,</span>
                 <span class="n">aws_secret_access_key</span><span class="o">=</span><span class="n">secret_key</span><span class="p">,</span>
                 <span class="n">region_name</span><span class="o">=</span><span class="n">region</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">createTableIfNotExists</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Create a DynamoDB table if it does not exist.</span>
<span class="sd">    This must be run on the Spark driver, and not inside foreach.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">dynamodb</span> <span class="o">=</span> <span class="n">get_dynamodb</span><span class="p">()</span>

    <span class="n">existing_tables</span> <span class="o">=</span> <span class="n">dynamodb</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">list_tables</span><span class="p">()[</span><span class="s1">&#39;TableNames&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">table_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">existing_tables</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Creating table </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">table_name</span><span class="p">)</span>
      <span class="n">table</span> <span class="o">=</span> <span class="n">dynamodb</span><span class="o">.</span><span class="n">create_table</span><span class="p">(</span>
          <span class="n">TableName</span><span class="o">=</span><span class="n">table_name</span><span class="p">,</span>
          <span class="n">KeySchema</span><span class="o">=</span><span class="p">[</span> <span class="p">{</span> <span class="s1">&#39;AttributeName&#39;</span><span class="p">:</span> <span class="s1">&#39;key&#39;</span><span class="p">,</span> <span class="s1">&#39;KeyType&#39;</span><span class="p">:</span> <span class="s1">&#39;HASH&#39;</span> <span class="p">}</span> <span class="p">],</span>
          <span class="n">AttributeDefinitions</span><span class="o">=</span><span class="p">[</span> <span class="p">{</span> <span class="s1">&#39;AttributeName&#39;</span><span class="p">:</span> <span class="s1">&#39;key&#39;</span><span class="p">,</span> <span class="s1">&#39;AttributeType&#39;</span><span class="p">:</span> <span class="s1">&#39;S&#39;</span> <span class="p">}</span> <span class="p">],</span>
          <span class="n">ProvisionedThroughput</span> <span class="o">=</span> <span class="p">{</span> <span class="s1">&#39;ReadCapacityUnits&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;WriteCapacityUnits&#39;</span><span class="p">:</span> <span class="mi">5</span> <span class="p">}</span>
      <span class="p">)</span>

      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Waiting for table to be ready&quot;</span><span class="p">)</span>

<span class="n">table</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">get_waiter</span><span class="p">(</span><span class="s1">&#39;table_exists&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">wait</span><span class="p">(</span><span class="n">TableName</span><span class="o">=</span><span class="n">table_name</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Define the classes and methods that writes to DynamoDB and then call them from <code class="docutils literal notranslate"><span class="pre">foreach</span></code>. There are two ways to specify your custom logic in <code class="docutils literal notranslate"><span class="pre">foreach</span></code>.</p>
<ul>
<li><p>Use a function: This is the simple approach that can be used to write 1 row at a time. However, client/connection initialization to write a row will be done in every call.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sendToDynamoDB_simple</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">  Function to send a row to DynamoDB.</span>
<span class="sd">  When used with `foreach`, this method is going to be called in the executor</span>
<span class="sd">  with the generated output rows.</span>
<span class="sd">  &#39;&#39;&#39;</span>
  <span class="c1"># Create client object in the executor,</span>
  <span class="c1"># do not use client objects created in the driver</span>
  <span class="n">dynamodb</span> <span class="o">=</span> <span class="n">get_dynamodb</span><span class="p">()</span>

  <span class="n">dynamodb</span><span class="o">.</span><span class="n">Table</span><span class="p">(</span><span class="n">table_name</span><span class="p">)</span><span class="o">.</span><span class="n">put_item</span><span class="p">(</span>
      <span class="n">Item</span> <span class="o">=</span> <span class="p">{</span> <span class="s1">&#39;key&#39;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;key&#39;</span><span class="p">]),</span> <span class="s1">&#39;count&#39;</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;count&#39;</span><span class="p">]</span> <span class="p">})</span>
</pre></div>
</div>
</li>
<li><p>Use a class with <code class="docutils literal notranslate"><span class="pre">open</span></code>, <code class="docutils literal notranslate"><span class="pre">process</span></code>, and <code class="docutils literal notranslate"><span class="pre">close</span></code> methods: This allows for a more efficient implementation where a client/connection is initialized and multiple rows can be written out.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SendToDynamoDB_ForeachWriter</span><span class="p">:</span>
<span class="w">  </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">  Class to send a set of rows to DynamoDB.</span>
<span class="sd">  When used with `foreach`, copies of this class is going to be used to write</span>
<span class="sd">  multiple rows in the executor. See the python docs for `DataStreamWriter.foreach`</span>
<span class="sd">  for more details.</span>
<span class="sd">  &#39;&#39;&#39;</span>

  <span class="k">def</span> <span class="nf">open</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partition_id</span><span class="p">,</span> <span class="n">epoch_id</span><span class="p">):</span>
    <span class="c1"># This is called first when preparing to send multiple rows.</span>
    <span class="c1"># Put all the initialization code inside open() so that a fresh</span>
    <span class="c1"># copy of this class is initialized in the executor where open()</span>
    <span class="c1"># will be called.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dynamodb</span> <span class="o">=</span> <span class="n">get_dynamodb</span><span class="p">()</span>
    <span class="k">return</span> <span class="kc">True</span>

  <span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">row</span><span class="p">):</span>
    <span class="c1"># This is called for each row after open() has been called.</span>
    <span class="c1"># This implementation sends one row at a time.</span>
    <span class="c1"># For further enhancements, contact the Spark+DynamoDB connector</span>
    <span class="c1"># team: https://github.com/audienceproject/spark-dynamodb</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dynamodb</span><span class="o">.</span><span class="n">Table</span><span class="p">(</span><span class="n">table_name</span><span class="p">)</span><span class="o">.</span><span class="n">put_item</span><span class="p">(</span>
        <span class="n">Item</span> <span class="o">=</span> <span class="p">{</span> <span class="s1">&#39;key&#39;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;key&#39;</span><span class="p">]),</span> <span class="s1">&#39;count&#39;</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;count&#39;</span><span class="p">]</span> <span class="p">})</span>

  <span class="k">def</span> <span class="nf">close</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">err</span><span class="p">):</span>
    <span class="c1"># This is called after all the rows have been processed.</span>
    <span class="k">if</span> <span class="n">err</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">err</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Invoke <code class="docutils literal notranslate"><span class="pre">foreach</span></code> in your streaming query with the above function or object.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.sql.shuffle.partitions&quot;</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">)</span>

<span class="n">query</span> <span class="o">=</span> <span class="p">(</span>
  <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;rate&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
    <span class="o">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s2">&quot;value % 10 as key&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">count</span><span class="p">()</span>
    <span class="o">.</span><span class="n">toDF</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;count&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">writeStream</span>
    <span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="n">SendToDynamoDB_ForeachWriter</span><span class="p">())</span>
    <span class="c1">#.foreach(sendToDynamoDB_simple)  // alternative, use one or the other</span>
    <span class="o">.</span><span class="n">outputMode</span><span class="p">(</span><span class="s2">&quot;update&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">start</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="use-scala">
<h3>Use Scala<a class="headerlink" href="#use-scala" title="Permalink to this headline"> </a></h3>
<p>This example shows how to use <code class="docutils literal notranslate"><span class="pre">streamingDataFrame.writeStream.foreach()</span></code> in Scala to write to DynamoDB.</p>
<p>To run this you will have to create a DynamoDB table that has a single string key named “value”.</p>
<ol class="arabic">
<li><p>Define an implementation of the <code class="docutils literal notranslate"><span class="pre">ForeachWriter</span></code> interface that performs the write.</p>
<div class="highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="k">import</span><span class="w"> </span><span class="nn">org</span><span class="p">.</span><span class="nn">apache</span><span class="p">.</span><span class="nn">spark</span><span class="p">.</span><span class="nn">sql</span><span class="p">.{</span><span class="nc">ForeachWriter</span><span class="p">,</span><span class="w"> </span><span class="nc">Row</span><span class="p">}</span>
<span class="k">import</span><span class="w"> </span><span class="nn">com</span><span class="p">.</span><span class="nn">amazonaws</span><span class="p">.</span><span class="nc">AmazonServiceException</span>
<span class="k">import</span><span class="w"> </span><span class="nn">com</span><span class="p">.</span><span class="nn">amazonaws</span><span class="p">.</span><span class="nn">auth</span><span class="p">.</span><span class="n">_</span>
<span class="k">import</span><span class="w"> </span><span class="nn">com</span><span class="p">.</span><span class="nn">amazonaws</span><span class="p">.</span><span class="nn">services</span><span class="p">.</span><span class="nn">dynamodbv2</span><span class="p">.</span><span class="nc">AmazonDynamoDB</span>
<span class="k">import</span><span class="w"> </span><span class="nn">com</span><span class="p">.</span><span class="nn">amazonaws</span><span class="p">.</span><span class="nn">services</span><span class="p">.</span><span class="nn">dynamodbv2</span><span class="p">.</span><span class="nc">AmazonDynamoDBClientBuilder</span>
<span class="k">import</span><span class="w"> </span><span class="nn">com</span><span class="p">.</span><span class="nn">amazonaws</span><span class="p">.</span><span class="nn">services</span><span class="p">.</span><span class="nn">dynamodbv2</span><span class="p">.</span><span class="nn">model</span><span class="p">.</span><span class="nc">AttributeValue</span>
<span class="k">import</span><span class="w"> </span><span class="nn">com</span><span class="p">.</span><span class="nn">amazonaws</span><span class="p">.</span><span class="nn">services</span><span class="p">.</span><span class="nn">dynamodbv2</span><span class="p">.</span><span class="nn">model</span><span class="p">.</span><span class="nc">ResourceNotFoundException</span>
<span class="k">import</span><span class="w"> </span><span class="nn">java</span><span class="p">.</span><span class="nn">util</span><span class="p">.</span><span class="nc">ArrayList</span>

<span class="k">import</span><span class="w"> </span><span class="nn">scala</span><span class="p">.</span><span class="nn">collection</span><span class="p">.</span><span class="nc">JavaConverters</span><span class="p">.</span><span class="n">_</span>

<span class="k">class</span><span class="w"> </span><span class="nc">DynamoDbWriter</span><span class="w"> </span><span class="k">extends</span><span class="w"> </span><span class="nc">ForeachWriter</span><span class="p">[</span><span class="nc">Row</span><span class="p">]</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">private</span><span class="w"> </span><span class="kd">val</span><span class="w"> </span><span class="n">tableName</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&lt;table name&gt;&quot;</span>
<span class="w">  </span><span class="k">private</span><span class="w"> </span><span class="kd">val</span><span class="w"> </span><span class="n">accessKey</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&lt;aws access key&gt;&quot;</span>
<span class="w">  </span><span class="k">private</span><span class="w"> </span><span class="kd">val</span><span class="w"> </span><span class="n">secretKey</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&lt;aws secret key&gt;&quot;</span>
<span class="w">  </span><span class="k">private</span><span class="w"> </span><span class="kd">val</span><span class="w"> </span><span class="n">regionName</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&lt;region&gt;&quot;</span>

<span class="w">  </span><span class="c1">// This will lazily be initialized only when open() is called</span>
<span class="w">  </span><span class="k">lazy</span><span class="w"> </span><span class="kd">val</span><span class="w"> </span><span class="n">ddb</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">AmazonDynamoDBClientBuilder</span><span class="p">.</span><span class="n">standard</span><span class="p">()</span>
<span class="w">    </span><span class="p">.</span><span class="n">withCredentials</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="nc">AWSStaticCredentialsProvider</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="nc">BasicAWSCredentials</span><span class="p">(</span><span class="n">accessKey</span><span class="p">,</span><span class="w"> </span><span class="n">secretKey</span><span class="p">)))</span>
<span class="w">    </span><span class="p">.</span><span class="n">withRegion</span><span class="p">(</span><span class="n">regionName</span><span class="p">)</span>
<span class="w">    </span><span class="p">.</span><span class="n">build</span><span class="p">()</span>

<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">// This is called first when preparing to send multiple rows.</span>
<span class="w">  </span><span class="c1">// Put all the initialization code inside open() so that a fresh</span>
<span class="w">  </span><span class="c1">// copy of this class is initialized in the executor where open()</span>
<span class="w">  </span><span class="c1">// will be called.</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="k">def</span><span class="w"> </span><span class="nf">open</span><span class="p">(</span><span class="n">partitionId</span><span class="p">:</span><span class="w"> </span><span class="nc">Long</span><span class="p">,</span><span class="w"> </span><span class="n">epochId</span><span class="p">:</span><span class="w"> </span><span class="nc">Long</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">ddb</span><span class="w">  </span><span class="c1">// force the initialization of the client</span>
<span class="w">    </span><span class="kc">true</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">// This is called for each row after open() has been called.</span>
<span class="w">  </span><span class="c1">// This implementation sends one row at a time.</span>
<span class="w">  </span><span class="c1">// A more efficient implementation can be to send batches of rows at a time.</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="k">def</span><span class="w"> </span><span class="nf">process</span><span class="p">(</span><span class="n">row</span><span class="p">:</span><span class="w"> </span><span class="nc">Row</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kd">val</span><span class="w"> </span><span class="n">rowAsMap</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">row</span><span class="p">.</span><span class="n">getValuesMap</span><span class="p">(</span><span class="n">row</span><span class="p">.</span><span class="n">schema</span><span class="p">.</span><span class="n">fieldNames</span><span class="p">)</span>
<span class="w">    </span><span class="kd">val</span><span class="w"> </span><span class="n">dynamoItem</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rowAsMap</span><span class="p">.</span><span class="n">mapValues</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">v</span><span class="p">:</span><span class="w"> </span><span class="nc">Any</span><span class="w"> </span><span class="o">=&gt;</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="nc">AttributeValue</span><span class="p">(</span><span class="n">v</span><span class="p">.</span><span class="n">toString</span><span class="p">)</span>
<span class="w">    </span><span class="p">}.</span><span class="n">asJava</span>

<span class="w">    </span><span class="n">ddb</span><span class="p">.</span><span class="n">putItem</span><span class="p">(</span><span class="n">tableName</span><span class="p">,</span><span class="w"> </span><span class="n">dynamoItem</span><span class="p">)</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">// This is called after all the rows have been processed.</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="k">def</span><span class="w"> </span><span class="nf">close</span><span class="p">(</span><span class="n">errorOrNull</span><span class="p">:</span><span class="w"> </span><span class="nc">Throwable</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">ddb</span><span class="p">.</span><span class="n">shutdown</span><span class="p">()</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">DynamoDbWriter</span></code> to write a rate stream into DynamoDB.</p>
<div class="highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="p">.</span><span class="n">readStream</span>
<span class="w">  </span><span class="p">.</span><span class="n">format</span><span class="p">(</span><span class="s">&quot;rate&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">load</span><span class="p">()</span>
<span class="w">  </span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">&quot;value&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">writeStream</span>
<span class="w">  </span><span class="p">.</span><span class="n">foreach</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="nc">DynamoDbWriter</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">start</span><span class="p">()</span>
</pre></div>
</div>
</li>
</ol>
</div>
</div>
<div class="section" id="amazon-cloudtrail-etl">
<span id="write-to-azure-synapse-analytics-using-foreachbatch-in-python"></span><span id="cloud-trail-etl"></span><h2>Amazon CloudTrail ETL<a class="headerlink" href="#amazon-cloudtrail-etl" title="Permalink to this headline"> </a></h2>
<p>The following notebooks show how you can easily transform your Amazon CloudTrail logs from JSON into Parquet for efficient ad-hoc querying.
See <a class="reference external" href="https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html">Real-time Streaming ETL with Structured Streaming</a> for details.</p>
<div class="embedded-notebook-section section" id="etl-of-amazon-cloudtrail-logs-using-structured-streaming-python-notebook">
<span id="structured-streaming-cloudtrail-python"></span><h3>ETL of Amazon CloudTrail logs using Structured Streaming Python notebook<a class="headerlink" href="#etl-of-amazon-cloudtrail-logs-using-structured-streaming-python-notebook" title="Permalink to this headline"> </a></h3>
<div class="embedded-notebook">
    <p class="notebook-import-help">
        <a href="/_extras/notebooks/source/structured-streaming-cloudtrail-python.html"            target="_blank">Open notebook in new tab</a>
        <button class="embedded-notebook-copy-to-clipboard"                copy-path-to-clipboard-on-click="/_extras/notebooks/source/structured-streaming-cloudtrail-python.html">            <img src="/_static/clippy.svg"                alt="Copy to clipboard">            Copy link for import        </button>    <div class="embedded-notebook-container">        <div class="loading-spinner"></div>        <iframe data-src="/_extras/notebooks/source/structured-streaming-cloudtrail-python.html"            id="062cb2bd2b29851aa8c6287ff7d47d644a566cbf714560c02e1686d21df1e4b3" height="1000px" width="100%"            style="overflow-y:hidden;" scrolling="no"></iframe>    </div></div></div>
<div class="embedded-notebook-section section" id="etl-of-amazon-cloudtrail-logs-using-structured-streaming-scala-notebook">
<span id="structured-streaming-cloudtrail-scala"></span><h3>ETL of Amazon CloudTrail logs using Structured Streaming Scala notebook<a class="headerlink" href="#etl-of-amazon-cloudtrail-logs-using-structured-streaming-scala-notebook" title="Permalink to this headline"> </a></h3>
<div class="embedded-notebook">
    <p class="notebook-import-help">
        <a href="/_extras/notebooks/source/structured-streaming-cloudtrail-scala.html"            target="_blank">Open notebook in new tab</a>
        <button class="embedded-notebook-copy-to-clipboard"                copy-path-to-clipboard-on-click="/_extras/notebooks/source/structured-streaming-cloudtrail-scala.html">            <img src="/_static/clippy.svg"                alt="Copy to clipboard">            Copy link for import        </button>    <div class="embedded-notebook-container">        <div class="loading-spinner"></div>        <iframe data-src="/_extras/notebooks/source/structured-streaming-cloudtrail-scala.html"            id="3efd964f915ce83aa912a32eefc428cb22f8d6cb3e4d6c0a9845884ef0dc309b" height="1000px" width="100%"            style="overflow-y:hidden;" scrolling="no"></iframe>    </div></div></div>
</div>
<div class="section" id="stream-stream-joins">
<h2>Stream-Stream joins<a class="headerlink" href="#stream-stream-joins" title="Permalink to this headline"> </a></h2>
<p>These two notebooks show how to use stream-stream joins in Python and Scala.</p>
<div class="embedded-notebook-section section" id="stream-stream-joins-python-notebook">
<span id="stream-stream-joins-python"></span><h3>Stream-Stream joins Python notebook<a class="headerlink" href="#stream-stream-joins-python-notebook" title="Permalink to this headline"> </a></h3>
<div class="embedded-notebook">
    <p class="notebook-import-help">
        <a href="/_extras/notebooks/source/stream-stream-joins-python.html"            target="_blank">Open notebook in new tab</a>
        <button class="embedded-notebook-copy-to-clipboard"                copy-path-to-clipboard-on-click="/_extras/notebooks/source/stream-stream-joins-python.html">            <img src="/_static/clippy.svg"                alt="Copy to clipboard">            Copy link for import        </button>    <div class="embedded-notebook-container">        <div class="loading-spinner"></div>        <iframe data-src="/_extras/notebooks/source/stream-stream-joins-python.html"            id="b802dc4e593fb80177970014cef1b0bb9a209f5a23a901e1c080163774f5fa64" height="1000px" width="100%"            style="overflow-y:hidden;" scrolling="no"></iframe>    </div></div></div>
<div class="embedded-notebook-section section" id="stream-stream-joins-scala-notebook">
<span id="stream-stream-joins-scala"></span><h3>Stream-Stream joins Scala notebook<a class="headerlink" href="#stream-stream-joins-scala-notebook" title="Permalink to this headline"> </a></h3>
<div class="embedded-notebook">
    <p class="notebook-import-help">
        <a href="/_extras/notebooks/source/stream-stream-joins-scala.html"            target="_blank">Open notebook in new tab</a>
        <button class="embedded-notebook-copy-to-clipboard"                copy-path-to-clipboard-on-click="/_extras/notebooks/source/stream-stream-joins-scala.html">            <img src="/_static/clippy.svg"                alt="Copy to clipboard">            Copy link for import        </button>    <div class="embedded-notebook-container">        <div class="loading-spinner"></div>        <iframe data-src="/_extras/notebooks/source/stream-stream-joins-scala.html"            id="52caed0d3bfbf4606e8b48b0b6e80978fc0875969497c9979ac1175f77e4a800" height="1000px" width="100%"            style="overflow-y:hidden;" scrolling="no"></iframe>    </div></div></div>
</div>
</div>


    
          </div>
        </div>
        <div  class="suapp-rating">
  <div id="suPageRateApp">
     <su-app></su-app>
   </div> 
 </div>
<hr> 
<footer>
  <div role="contentinfo">
      <p class="copyright">
          &copy; Databricks 2023. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the <a href="http://www.apache.org/">Apache Software Foundation</a>.
      </p>
      <p> 
        
          <a id='feedbacklink' href="mailto:doc-feedback@databricks.com?subject=Documentation Feedback">Send us feedback</a>
        
     | <a href="https://databricks.com/privacy-policy">Privacy Policy</a> | <a href="https://databricks.com/terms-of-use">Terms of Use</a></p>

  </div> 

</footer>
      </div>
    </div>
  </section>
</main>

  </page>
  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT: '../',
      VERSION: '1.0',
      COLLAPSE_INDEX: false,
      FILE_SUFFIX: '.html',
      HAS_SOURCE: 'false'
    };
  </script>
  <script type="text/javascript" src="../_static/jquery.js"></script>
  <script type="text/javascript" src="../_static/underscore.js"></script>
  <script type="text/javascript" src="../_static/doctools.js"></script>
  <script type="text/javascript" src="../_static/language_data.js"></script>
  

  <script type="text/javascript" src="../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../_static/js/jquery.waypoints.min.js"></script>

  <!-- Select2 (https://select2.org/) -->
  <link href="https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/css/select2.min.css" rel="stylesheet" />
  <script src="https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/js/select2.min.js"></script>
  <!-- End Select2 -->

  
  
  <script type="text/javascript" src="../_static/js/localized.js"></script>
  <script type="text/javascript" src="../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  
 



  <script>
  window.__searchunifyLoaderConfig = JSON.parse('{"clients": {"en": "02c2e804-27e9-11ee-aefb-0242ac120011", "ja": "6a42c3f2-2820-11ee-aefb-0242ac120011", "pt": "6a86badd-2821-11ee-aefb-0242ac120011"}}')
</script>
<script type="text/javascript" src="../_static/js/search-loader.js"></script>
</body>
<script type='text/javascript'>
  window.onload = function () {
    var description = document.querySelector('meta[name="description"]').getAttribute("content");
    let titleText = document.querySelector('h1').textContent;
    document.querySelector('meta[property="og:title"]').setAttribute("content", titleText);
    document.querySelector('meta[property="og:description"]').setAttribute("content", description);
    document.querySelector('meta[property="twitter:description"]').setAttribute("content", description);
  };
</script>

</html>