

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en-US" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en-US"> <!--<![endif]-->

<head>
  <!-- cookie consent -->
  
    <!-- Combined Onetrust and Rudderstack Implementation Scripts -->
    <!-- Onetrust Initialization -->
    <script type="text/javascript" src="https://cdn.cookielaw.org/consent/92466579-1717-44d3-809d-a05fb02843ed-test/OtAutoBlock.js"></script>
    <script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="92466579-1717-44d3-809d-a05fb02843ed-test"></script>
    <link rel="stylesheet" id="db-onetrust-style" href="https://www.databricks.com/wp-content/uploads/db_onetrust.css" media="all" />
    <!-- Setting Rudderstack Write Key -->
    <script>window.rudderstackKey = "2SOR9fvSr5Fi6tN2ihPbVHnX1SZ" </script>
    <!-- Rudderstack Initialization + Onetrust Integration + Rudderstack Custom Events -->
    <script type="text/javascript" src="https://www.databricks.com/sites/default/files/rudderstack/v1/db-rudderstack-events.js"></script>

  <!-- cookie consent -->

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="X-UA-Compatible" content="IE=9" />
  <meta content="Learn the basics of near real-time and incremental processing with Structured Streaming on Databricks." name="description" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0">
  <meta property="og:image" content="https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:title" content="Run your first Structured Streaming workload">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://docs.databricks.com">
  <meta property="og:description" content="" id="og-description">
  <meta name="twitter:image" content="https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png">
  <meta name="twitter:site" content="@databricks">
  <meta name="twitter:creator" content="@databricks">
  <meta property="twitter:description" content="">
  
  <title>Run your first Structured Streaming workload &#124; Databricks on AWS</title>
  
  
  <link rel="canonical" href="https://docs.databricks.com/en/structured-streaming/tutorial.html">
  <!-- Start hreflang tag -->
  <link rel="alternate" hreflang="en" href="https://docs.databricks.com/en/structured-streaming/tutorial.html" />
<link rel="alternate" hreflang="x-default" href="https://docs.databricks.com/en/structured-streaming/tutorial.html" />
  <!-- End hreflang tag -->
  
  
  <link rel="shortcut icon" href="../_static/favicon.ico" />
  

  

  

  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;
j.setAttributeNode(d.createAttribute('data-ot-ignore'));
f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-T85FQ33');</script>
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;
j.setAttributeNode(d.createAttribute('data-ot-ignore'));
f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-TWTKQQ');</script>
    
  <!-- End Google Tag Manager -->


  <!-- MaxMind / GEO IP -->
  <script src="//js.maxmind.com/js/apis/geoip2/v2.1/geoip2.js" type="text/javascript"></script>
  <!-- End MaxMind / GEO IP -->

  
  
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600&display=swap" rel="stylesheet">
  <link rel="preload" href="../_static/fonts/DMSans-Bold.ttf" as="font">
  <link rel="preload" href="../_static/fonts/DMSans-Regular.ttf" as="font">
  <link rel="preload" href="../_static/fonts/DMMono-Regular.ttf" as="font">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/cloud-provider-selector.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/translation-selector.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/searchunify/main.css" type="text/css" />

  
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="top" title="Databricks on AWS" href="../index.html" /> 
</head>

<body class="wy-body-for-nav" role="document">

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T85FQ33"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TWTKQQ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  
  <nav class="wy-nav-top header su_header" role="navigation" aria-label="top navigation">
    
<nav class="wy-nav-top header su_header" role="navigation" aria-label="top navigation">
  <div class="container-logo">
    <ul class="mobile-menu-toggle">
        <li class="menu-toggle">
            <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
            
            <a href="https://www.databricks.com/" class="wy-nav-top-logo"><img src="../_static/small-scale-lockup-full-color-rgb.svg" width="137" height="21"
              alt="Databricks" /></a>   
               
              </li>
    </ul>
    <ul class="su_nav-menu">
      <li class="menu-toggle">
        <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
        
          
        
        <a href="https://www.databricks.com/" class="wy-nav-top-logo"><img src="../_static/small-scale-lockup-full-color-rgb.svg" width="137" height="21"
            alt="Databricks" /></a></li>
        <!-- 
<li><a href="https://help.databricks.com/s/">Help Center</a></li>
<li class="active"><a href="https://docs.databricks.com/en/">Documentation</a></li>
<li><a href="https://kb.databricks.com/">Knowledge Base</a></li>
 -->
    </ul>
  </div>
  <div class="su_nav-right">
    <ul class="su_link-mobile">
  <!-- Mobile header code can go here -->
</ul>
<ul class="right-try-list">
   
</ul>
  </div>
</nav>
  </nav>

  <div class="su_sub-header">
    <div class="container">
      <div class="su_sub-header-inner">
        <!-- <div class="su_subnav-menu-right">
  <div id="auto" style="width: 100%;">
    <div ng-controller="SearchautoController">
      <div bind-html-compile="autocompleteHtml">
        <form class="su__search-box-1" disabled="disabled">
          <input class="su__search-input" type="search" name="Search box" id="su__search-b" placeholder="Search Documentation" disabled="disabled"/>
          <button class="su__search-button" type="submit" class="button button-success" disabled="disabled">
            <svg width="24" height="24" viewBox="0 0 24 24">
              <path
                d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"
                fill="#333"></path>
            </svg>
          </button>
        </form>
      </div>
    </div>
  </div>
</div> -->
        <div class="search-lng-gap"></div>
        <div style="margin-left: 16px; margin-right: 16px;">
          <!-- <select name="lng selector" id="lng-selector">
    <option value="../../en/structured-streaming/tutorial.html" class="notranslate">English</option>
    <option value="../../ja/structured-streaming/tutorial.html" class="notranslate">日本語</option>
    <option value="../../pt/structured-streaming/tutorial.html" class="notranslate">Português (Brasil)</option>
</select> -->
        </div>
        <div class="cloud-selector-container">
          <!-- <select name="cloud provider selector" id="cloud-provider-selector">
    <option value="aws" selected class="notranslate">
        Amazon Web Services
    </option>
    <option value="azure"  class="notranslate">
        Microsoft Azure
    </option>
    <option value="gcp"  class="notranslate">
        Google Cloud Platform
    </option>
</select> -->
        </div>
      </div>
    </div>
  </div>
  <page class="js-page-container">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side su_nav-side">
<div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    

    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../index.html" class="main-navigation-home">Databricks on AWS</a>
    

    
      

      
        <p class="caption"><span class="caption-text">Load &amp; manage data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../rag-temp/index.html">RAG Studio</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    
  <p class="build_info notranslate"data-last-edit="December 23, 2023">
    Updated Jan 11, 2024
  </p>
<script>
  window.addEventListener('DOMContentLoaded',function(){
    var h1=document.querySelector('h1');
    var bi=document.querySelector('[data-last-edit]');
    if(h1 && bi){
      var ver = document.createElement('p');
      ver.className = 'version_info';
      ver.textContent = bi.getAttribute('data-last-edit');
      h1.parentElement.insertBefore(ver, h1.nextElementSibling);
    }
  });
</script>

    <p>
      
        <a id='feedbacklink' href="mailto:doc-feedback@databricks.com?subject=Documentation Feedback">Send us feedback</a>
      
    </p>
  </div>
</div>
</nav>
    
    
<main class="wy-grid-for-nav su_nav-grid">
  <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
    <div class="wy-nav-content su__nav_content">
      <div class="rst-content">
        





<div role="navigation" aria-label="breadcrumbs navigation" class="wy-breadcrumbs-wrapper">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>Run your first Structured Streaming workload</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>
</div>
        
        <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
          <div itemprop="articleBody">
            
    
  <div class="section" id="run-your-first-structured-streaming-workload">
<span id="run-your-first-ss-workload"></span><h1>Run your first Structured Streaming workload<a class="headerlink" href="#run-your-first-structured-streaming-workload" title="Permalink to this headline"> </a></h1>
<p>This article provides code examples and explanation of basic concepts necessary to run your first Structured Streaming queries on Databricks. You can use Structured Streaming for near real-time and incremental processing workloads.</p>
<p>Structured Streaming is one of several technologies that power streaming tables in Delta Live Tables. Databricks recommends using Delta Live Tables for all new ETL, ingestion, and Structured Streaming workloads. See <a class="reference internal" href="../delta-live-tables/index.html"><span class="doc">What is Delta Live Tables?</span></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>While Delta Live Tables provides a slightly modified syntax for declaring streaming tables, the general syntax for configuring streaming reads and transformations applies to all streaming use cases on Databricks. Delta Live Tables also simplifies streaming by managing state information, metadata, and numerous configurations.</p>
</div>
<div class="section" id="read-from-a-data-stream">
<h2>Read from a data stream<a class="headerlink" href="#read-from-a-data-stream" title="Permalink to this headline"> </a></h2>
<p>You can use Structured Streaming to incrementally ingest data from supported data sources. Some of the most common data sources used in Databricks Structured Streaming workloads include the following:</p>
<ul class="simple">
<li><p>Data files in cloud object storage</p></li>
<li><p>Message buses and queues</p></li>
<li><p>Delta Lake</p></li>
</ul>
<p>Databricks recommends using Auto Loader for streaming ingestion from cloud object storage. Auto Loader supports most file formats supported by Structured Streaming. See <a class="reference internal" href="../ingestion/auto-loader/index.html"><span class="doc">What is Auto Loader?</span></a>.</p>
<p>Each data source provides a number of options to specify how to load batches of data. During reader configuration, the main options you might need to set fall into the following categories:</p>
<ul class="simple">
<li><p>Options that specify the data source or format (for example, file type, delimiters, and schema).</p></li>
<li><p>Options that configure access to source systems (for example, port settings and credentials).</p></li>
<li><p>Options that specify where to start in a stream (for example, Kafka offsets or reading all existing files).</p></li>
<li><p>Options that control how much data is processed in each batch (for example, max offsets, files, or bytes per batch).</p></li>
</ul>
</div>
<div class="section" id="use-auto-loader-to-read-streaming-data-from-object-storage">
<span id="use-al-to-read-streaming-data-from-object-storage"></span><h2>Use Auto Loader to read streaming data from object storage<a class="headerlink" href="#use-auto-loader-to-read-streaming-data-from-object-storage" title="Permalink to this headline"> </a></h2>
<p>The following example demonstrates loading JSON data with Auto Loader, which uses <code class="docutils literal notranslate"><span class="pre">cloudFiles</span></code> to denote format and options. The <code class="docutils literal notranslate"><span class="pre">schemaLocation</span></code> option enables schema inference and evolution. Paste the following code in a Databricks notebook cell and run the cell to create a streaming DataFrame named <code class="docutils literal notranslate"><span class="pre">raw_df</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">file_path</span> <span class="o">=</span> <span class="s2">&quot;/databricks-datasets/structured-streaming/events&quot;</span>
<span class="n">checkpoint_path</span> <span class="o">=</span> <span class="s2">&quot;/tmp/ss-tutorial/_checkpoint&quot;</span>

<span class="n">raw_df</span> <span class="o">=</span> <span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">readStream</span>
    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;cloudFiles&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;cloudFiles.format&quot;</span><span class="p">,</span> <span class="s2">&quot;json&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;cloudFiles.schemaLocation&quot;</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">)</span>
    <span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Like other read operations on Databricks, configuring a streaming read does not actually load data. You must trigger an action on the data before the stream begins.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Calling <code class="docutils literal notranslate"><span class="pre">display()</span></code> on a streaming DataFrame starts a streaming job. For most Structured Streaming use cases, the action that triggers a stream should be writing data to a sink. See <a class="reference internal" href="#production"><span class="std std-ref">Preparing your Structured Streaming code for production</span></a>.</p>
</div>
</div>
<div class="section" id="perform-a-streaming-transformation">
<h2>Perform a streaming transformation<a class="headerlink" href="#perform-a-streaming-transformation" title="Permalink to this headline"> </a></h2>
<p>Structured Streaming supports most transformations that are available in Databricks and Spark SQL. You can even load MLflow models as UDFs and make streaming predictions as a transformation.</p>
<p>The following code example completes a simple transformation to enrich the ingested JSON data with additional information using Spark SQL functions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">col</span><span class="p">,</span> <span class="n">current_timestamp</span>

<span class="n">transformed_df</span> <span class="o">=</span> <span class="p">(</span><span class="n">raw_df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
    <span class="s2">&quot;*&quot;</span><span class="p">,</span>
    <span class="n">col</span><span class="p">(</span><span class="s2">&quot;_metadata.file_path&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;source_file&quot;</span><span class="p">),</span>
    <span class="n">current_timestamp</span><span class="p">()</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;processing_time&quot;</span><span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The resulting <code class="docutils literal notranslate"><span class="pre">transformed_df</span></code> contains query instructions to load and transform each record as it arrives in the data source.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Structured Streaming treats data sources as unbounded or infinite datasets. As such, some transformations are not supported in Structured Streaming workloads because they would require sorting an infinite number of items.</p>
<p>Most aggregations and many joins require managing state information with watermarks, windows, and output mode. See <a class="reference internal" href="watermarks.html"><span class="doc">Apply watermarks to control data processing thresholds</span></a>.</p>
</div>
</div>
<div class="section" id="write-to-a-data-sink">
<h2>Write to a data sink<a class="headerlink" href="#write-to-a-data-sink" title="Permalink to this headline"> </a></h2>
<p>A data sink is the target of a streaming write operation. Common sinks used in Databricks streaming workloads include the following:</p>
<ul class="simple">
<li><p>Delta Lake</p></li>
<li><p>Message buses and queues</p></li>
<li><p>Key-value databases</p></li>
</ul>
<p>As with data sources, most data sinks provide a number of options to control how data is written to the target system. During writer configuration, the main options you might need to set fall into the following categories:</p>
<ul class="simple">
<li><p>Output mode (append by default).</p></li>
<li><p>A checkpoint location (required for each <strong>writer</strong>).</p></li>
<li><p>Trigger intervals; see <a class="reference internal" href="triggers.html"><span class="doc">Configure Structured Streaming trigger intervals</span></a>.</p></li>
<li><p>Options that specify the data sink or format (for example, file type, delimiters, and schema).</p></li>
<li><p>Options that configure access to target systems (for example, port settings and credentials).</p></li>
</ul>
</div>
<div class="section" id="perform-an-incremental-batch-write-to-delta-lake">
<span id="perform-an-incremental-batch-write-to-delta"></span><h2>Perform an incremental batch write to Delta Lake<a class="headerlink" href="#perform-an-incremental-batch-write-to-delta-lake" title="Permalink to this headline"> </a></h2>
<p>The following example writes to Delta Lake using a specified file path and checkpoint.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Always make sure you specify a unique checkpoint location for each streaming writer you configure. The checkpoint provides the unique identity for your stream, tracking all records processed and state information associated with your streaming query.</p>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">availableNow</span></code> setting for the trigger instructs Structured Streaming to process all previously unprocessed records from the source dataset and then shut down, so you can safely execute the following code without worrying about leaving a stream running:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">target_path</span> <span class="o">=</span> <span class="s2">&quot;/tmp/ss-tutorial/&quot;</span>
<span class="n">checkpoint_path</span> <span class="o">=</span> <span class="s2">&quot;/tmp/ss-tutorial/_checkpoint&quot;</span>

<span class="n">transformed_df</span><span class="o">.</span><span class="n">writeStream</span>
    <span class="o">.</span><span class="n">trigger</span><span class="p">(</span><span class="n">availableNow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;checkpointLocation&quot;</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">)</span>
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;path&quot;</span><span class="p">,</span> <span class="n">target_path</span><span class="p">)</span>
    <span class="o">.</span><span class="n">start</span><span class="p">()</span>
</pre></div>
</div>
<p>In this example, no new records arrive in our data source, so repeat execution of this code does not ingest new records.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Structured Streaming execution can prevent auto termination from shutting down compute resources. To avoid unexpected costs, be sure to terminate streaming queries.</p>
</div>
</div>
<div class="section" id="preparing-your-structured-streaming-code-for-production">
<span id="preparing-your-ss-code-for-production"></span><span id="production"></span><h2>Preparing your Structured Streaming code for production<a class="headerlink" href="#preparing-your-structured-streaming-code-for-production" title="Permalink to this headline"> </a></h2>
<p>Databricks recommends using Delta Live Tables for most Structured Streaming workloads. The following recommendations provide a starting point for preparing Structured Streaming workloads for production:</p>
<ul class="simple">
<li><p>Remove unnecessary code from notebooks that would return results, such as <code class="docutils literal notranslate"><span class="pre">display</span></code> and <code class="docutils literal notranslate"><span class="pre">count</span></code>.</p></li>
<li><p>Do not run Structured Streaming workloads on interactive clusters; always schedule streams as jobs.</p></li>
<li><p>To help streaming jobs recover automatically, configure jobs with infinite retries.</p></li>
<li><p>Do not use auto-scaling for workloads with Structured Streaming.</p></li>
</ul>
<p>For more recommendations, see <a class="reference internal" href="production.html"><span class="doc">Production considerations for Structured Streaming</span></a>.</p>
</div>
<div class="section" id="read-data-from-delta-lake-transform-and-write-to-delta-lake">
<span id="read-data-from-delta-transform-and-write-to-delta"></span><span id="delta"></span><h2>Read data from Delta Lake, transform, and write to Delta Lake<a class="headerlink" href="#read-data-from-delta-lake-transform-and-write-to-delta-lake" title="Permalink to this headline"> </a></h2>
<p>Delta Lake has extensive support for working with Structured Streaming as both a source and a sink. See <a class="reference internal" href="delta-lake.html"><span class="doc">Delta table streaming reads and writes</span></a>.</p>
<p>The following example shows example syntax to incrementally load all new records from a Delta table, join them with a snapshot of another Delta table, and write them to a Delta table:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">readStream</span>
    <span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">&quot;&lt;table-name1&gt;&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">&quot;&lt;table-name2&gt;&quot;</span><span class="p">),</span> <span class="n">on</span><span class="o">=</span><span class="s2">&quot;&lt;id&gt;&quot;</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">writeStream</span>
    <span class="o">.</span><span class="n">trigger</span><span class="p">(</span><span class="n">availableNow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;checkpointLocation&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;checkpoint-path&gt;&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">toTable</span><span class="p">(</span><span class="s2">&quot;&lt;table-name3&gt;&quot;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>You must have proper permissions configured to read source tables and write to target tables and the specified checkpoint location. Fill in all parameters denoted with angle brackets (<code class="docutils literal notranslate"><span class="pre">&lt;&gt;</span></code>) using the relevant values for your data sources and sinks.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Delta Live Tables provides a fully declarative syntax for creating Delta Lake pipelines and manages properties like triggers and checkpoints automatically. See <a class="reference internal" href="../delta-live-tables/index.html"><span class="doc">What is Delta Live Tables?</span></a>.</p>
</div>
</div>
<div class="section" id="read-data-from-kafka-transform-and-write-to-kafka">
<span id="kafka"></span><h2>Read data from Kafka, transform, and write to Kafka<a class="headerlink" href="#read-data-from-kafka-transform-and-write-to-kafka" title="Permalink to this headline"> </a></h2>
<p>Apache Kafka and other messaging buses provide some of the lowest latency available for large datasets. You can use Databricks to apply transformations to data ingested from Kafka and then write data back to Kafka.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Writing data to cloud object storage adds additional latency overhead. If you wish to store data from a messaging bus in Delta Lake but require the lowest latency possible for streaming workloads, Databricks recommends configuring separate streaming jobs to ingest data to the lakehouse and apply near real-time transformations for downstream messaging bus sinks.</p>
</div>
<p>The following code example demonstrates a simple pattern to enrich data from Kafka by joining it with data in a Delta table and then writing back to Kafka:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">readStream</span>
    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;kafka&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;kafka.bootstrap.servers&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;server:ip&gt;&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;subscribe&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;topic&gt;&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;startingOffsets&quot;</span><span class="p">,</span> <span class="s2">&quot;latest&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">load</span><span class="p">()</span>
    <span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">&quot;&lt;table-name&gt;&quot;</span><span class="p">),</span> <span class="n">on</span><span class="o">=</span><span class="s2">&quot;&lt;id&gt;&quot;</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">writeStream</span>
    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;kafka&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;kafka.bootstrap.servers&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;server:ip&gt;&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;topic&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;topic&gt;&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;checkpointLocation&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;checkpoint-path&gt;&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">start</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
</div>
<p>You must have proper permissions configured for access to your Kafka service. Fill in all parameters denoted with angle brackets (<code class="docutils literal notranslate"><span class="pre">&lt;&gt;</span></code>) using the relevant values for your data sources and sinks. See <a class="reference internal" href="../connect/streaming/kafka.html"><span class="doc">Stream processing with Apache Kafka and Databricks</span></a>.</p>
</div>
</div>


    
          </div>
        </div>
        <div  class="suapp-rating">
  <div id="suPageRateApp">
     <su-app></su-app>
   </div> 
 </div>
<hr> 
<footer>
  <div role="contentinfo">
      <p class="copyright">
          &copy; Databricks 2023. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the <a href="http://www.apache.org/">Apache Software Foundation</a>.
      </p>
      <p> 
        
          <a id='feedbacklink' href="mailto:doc-feedback@databricks.com?subject=Documentation Feedback">Send us feedback</a>
        
     | <a href="https://databricks.com/privacy-policy">Privacy Policy</a> | <a href="https://databricks.com/terms-of-use">Terms of Use</a></p>

  </div> 

</footer>
      </div>
    </div>
  </section>
</main>

  </page>
  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT: '../',
      VERSION: '1.0',
      COLLAPSE_INDEX: false,
      FILE_SUFFIX: '.html',
      HAS_SOURCE: 'false'
    };
  </script>
  <script type="text/javascript" src="../_static/jquery.js"></script>
  <script type="text/javascript" src="../_static/underscore.js"></script>
  <script type="text/javascript" src="../_static/doctools.js"></script>
  <script type="text/javascript" src="../_static/language_data.js"></script>
  

  <script type="text/javascript" src="../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../_static/js/jquery.waypoints.min.js"></script>

  <!-- Select2 (https://select2.org/) -->
  <link href="https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/css/select2.min.css" rel="stylesheet" />
  <script src="https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/js/select2.min.js"></script>
  <!-- End Select2 -->

  
  
  <script type="text/javascript" src="../_static/js/localized.js"></script>
  <script type="text/javascript" src="../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  
 



  <script>
  window.__searchunifyLoaderConfig = JSON.parse('{"clients": {"en": "02c2e804-27e9-11ee-aefb-0242ac120011", "ja": "6a42c3f2-2820-11ee-aefb-0242ac120011", "pt": "6a86badd-2821-11ee-aefb-0242ac120011"}}')
</script>
<script type="text/javascript" src="../_static/js/search-loader.js"></script>
</body>
<script type='text/javascript'>
  window.onload = function () {
    var description = document.querySelector('meta[name="description"]').getAttribute("content");
    let titleText = document.querySelector('h1').textContent;
    document.querySelector('meta[property="og:title"]').setAttribute("content", titleText);
    document.querySelector('meta[property="og:description"]').setAttribute("content", description);
    document.querySelector('meta[property="twitter:description"]').setAttribute("content", description);
  };
</script>

</html>