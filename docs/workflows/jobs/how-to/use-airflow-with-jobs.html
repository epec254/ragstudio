

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en-US" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en-US"> <!--<![endif]-->

<head>
  <!-- cookie consent -->
  
    <!-- Combined Onetrust and Rudderstack Implementation Scripts -->
    <!-- Onetrust Initialization -->
    <script type="text/javascript" src="https://cdn.cookielaw.org/consent/92466579-1717-44d3-809d-a05fb02843ed-test/OtAutoBlock.js"></script>
    <script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="92466579-1717-44d3-809d-a05fb02843ed-test"></script>
    <link rel="stylesheet" id="db-onetrust-style" href="https://www.databricks.com/wp-content/uploads/db_onetrust.css" media="all" />
    <!-- Setting Rudderstack Write Key -->
    <script>window.rudderstackKey = "2SOR9fvSr5Fi6tN2ihPbVHnX1SZ" </script>
    <!-- Rudderstack Initialization + Onetrust Integration + Rudderstack Custom Events -->
    <script type="text/javascript" src="https://www.databricks.com/sites/default/files/rudderstack/v1/db-rudderstack-events.js"></script>

  <!-- cookie consent -->

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="X-UA-Compatible" content="IE=9" />
  <meta content="Learn how to orchestrate Databricks jobs in a data pipeline with Apache Airflow and how to set up the Airflow integration." name="description" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0">
  <meta property="og:image" content="https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:title" content="Orchestrate Databricks jobs with Apache Airflow">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://docs.databricks.com">
  <meta property="og:description" content="" id="og-description">
  <meta name="twitter:image" content="https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png">
  <meta name="twitter:site" content="@databricks">
  <meta name="twitter:creator" content="@databricks">
  <meta property="twitter:description" content="">
  
  <title>Orchestrate Databricks jobs with Apache Airflow &#124; Databricks on AWS</title>
  
  
  <link rel="canonical" href="https://docs.databricks.com/en/workflows/jobs/how-to/use-airflow-with-jobs.html">
  <!-- Start hreflang tag -->
  <link rel="alternate" hreflang="en" href="https://docs.databricks.com/en/workflows/jobs/how-to/use-airflow-with-jobs.html" />
<link rel="alternate" hreflang="x-default" href="https://docs.databricks.com/en/workflows/jobs/how-to/use-airflow-with-jobs.html" />
  <!-- End hreflang tag -->
  
  
  <link rel="shortcut icon" href="../../../_static/favicon.ico" />
  

  

  

  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;
j.setAttributeNode(d.createAttribute('data-ot-ignore'));
f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-T85FQ33');</script>
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;
j.setAttributeNode(d.createAttribute('data-ot-ignore'));
f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-TWTKQQ');</script>
    
  <!-- End Google Tag Manager -->


  <!-- MaxMind / GEO IP -->
  <script src="//js.maxmind.com/js/apis/geoip2/v2.1/geoip2.js" type="text/javascript"></script>
  <!-- End MaxMind / GEO IP -->

  
  
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600&display=swap" rel="stylesheet">
  <link rel="preload" href="../../../_static/fonts/DMSans-Bold.ttf" as="font">
  <link rel="preload" href="../../../_static/fonts/DMSans-Regular.ttf" as="font">
  <link rel="preload" href="../../../_static/fonts/DMMono-Regular.ttf" as="font">
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/cloud-provider-selector.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/translation-selector.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/searchunify/main.css" type="text/css" />

  
  <link rel="index" title="Index" href="../../../genindex.html" />
  <link rel="search" title="Search" href="../../../search.html" />
  <link rel="top" title="Databricks on AWS" href="../../../index.html" /> 
</head>

<body class="wy-body-for-nav" role="document">

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T85FQ33"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TWTKQQ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  
  <nav class="wy-nav-top header su_header" role="navigation" aria-label="top navigation">
    
<nav class="wy-nav-top header su_header" role="navigation" aria-label="top navigation">
  <div class="container-logo">
    <ul class="mobile-menu-toggle">
        <li class="menu-toggle">
            <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
            
            <a href="https://www.databricks.com/" class="wy-nav-top-logo"><img src="../../../_static/small-scale-lockup-full-color-rgb.svg" width="137" height="21"
              alt="Databricks" /></a>   
               
              </li>
    </ul>
    <ul class="su_nav-menu">
      <li class="menu-toggle">
        <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
        
          
        
        <a href="https://www.databricks.com/" class="wy-nav-top-logo"><img src="../../../_static/small-scale-lockup-full-color-rgb.svg" width="137" height="21"
            alt="Databricks" /></a></li>
        <!-- 
<li><a href="https://help.databricks.com/s/">Help Center</a></li>
<li class="active"><a href="https://docs.databricks.com/en/">Documentation</a></li>
<li><a href="https://kb.databricks.com/">Knowledge Base</a></li>
 -->
    </ul>
  </div>
  <div class="su_nav-right">
    <ul class="su_link-mobile">
  <!-- Mobile header code can go here -->
</ul>
<ul class="right-try-list">
   
</ul>
  </div>
</nav>
  </nav>

  <div class="su_sub-header">
    <div class="container">
      <div class="su_sub-header-inner">
        <!-- <div class="su_subnav-menu-right">
  <div id="auto" style="width: 100%;">
    <div ng-controller="SearchautoController">
      <div bind-html-compile="autocompleteHtml">
        <form class="su__search-box-1" disabled="disabled">
          <input class="su__search-input" type="search" name="Search box" id="su__search-b" placeholder="Search Documentation" disabled="disabled"/>
          <button class="su__search-button" type="submit" class="button button-success" disabled="disabled">
            <svg width="24" height="24" viewBox="0 0 24 24">
              <path
                d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"
                fill="#333"></path>
            </svg>
          </button>
        </form>
      </div>
    </div>
  </div>
</div> -->
        <div class="search-lng-gap"></div>
        <div style="margin-left: 16px; margin-right: 16px;">
          <!-- <select name="lng selector" id="lng-selector">
    <option value="../../../../en/workflows/jobs/how-to/use-airflow-with-jobs.html" class="notranslate">English</option>
    <option value="../../../../ja/workflows/jobs/how-to/use-airflow-with-jobs.html" class="notranslate">日本語</option>
    <option value="../../../../pt/workflows/jobs/how-to/use-airflow-with-jobs.html" class="notranslate">Português (Brasil)</option>
</select> -->
        </div>
        <div class="cloud-selector-container">
          <!-- <select name="cloud provider selector" id="cloud-provider-selector">
    <option value="aws" selected class="notranslate">
        Amazon Web Services
    </option>
    <option value="azure"  class="notranslate">
        Microsoft Azure
    </option>
    <option value="gcp"  class="notranslate">
        Google Cloud Platform
    </option>
</select> -->
        </div>
      </div>
    </div>
  </div>
  <page class="js-page-container">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side su_nav-side">
<div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    

    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../../index.html" class="main-navigation-home">Databricks on AWS</a>
    

    
      

      
        <p class="caption"><span class="caption-text">Load &amp; manage data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../rag-temp/index.html">RAG Studio</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    
  <p class="build_info notranslate"data-last-edit="December 23, 2023">
    Updated Jan 11, 2024
  </p>
<script>
  window.addEventListener('DOMContentLoaded',function(){
    var h1=document.querySelector('h1');
    var bi=document.querySelector('[data-last-edit]');
    if(h1 && bi){
      var ver = document.createElement('p');
      ver.className = 'version_info';
      ver.textContent = bi.getAttribute('data-last-edit');
      h1.parentElement.insertBefore(ver, h1.nextElementSibling);
    }
  });
</script>

    <p>
      
        <a id='feedbacklink' href="mailto:doc-feedback@databricks.com?subject=Documentation Feedback">Send us feedback</a>
      
    </p>
  </div>
</div>
</nav>
    
    
<main class="wy-grid-for-nav su_nav-grid">
  <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
    <div class="wy-nav-content su__nav_content">
      <div class="rst-content">
        





<div role="navigation" aria-label="breadcrumbs navigation" class="wy-breadcrumbs-wrapper">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>Orchestrate Databricks jobs with Apache Airflow</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>
</div>
        
        <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
          <div itemprop="articleBody">
            
    
  <div class="section" id="orchestrate-databricks-jobs-with-apache-airflow">
<h1>Orchestrate Databricks jobs with Apache Airflow<a class="headerlink" href="#orchestrate-databricks-jobs-with-apache-airflow" title="Permalink to this headline"> </a></h1>
<p>This article describes the Apache Airflow support for orchestrating data pipelines with Databricks, has instructions for installing and configuring Airflow locally, and provides an example of deploying and running a Databricks workflow with Airflow.</p>
<div class="section" id="job-orchestration-in-a-data-pipeline">
<h2>Job orchestration in a data pipeline<a class="headerlink" href="#job-orchestration-in-a-data-pipeline" title="Permalink to this headline"> </a></h2>
<p>Developing and deploying a data processing pipeline often requires managing complex dependencies between tasks. For example, a pipeline might read data from a source, clean the data, transform the cleaned data, and write the transformed data to a target. You also need support for testing, scheduling, and troubleshooting errors when you operationalize a pipeline.</p>
<p>Workflow systems address these challenges by allowing you to define dependencies between tasks, schedule when pipelines run, and monitor workflows. <a class="reference external" href="https://airflow.apache.org/">Apache Airflow</a> is an open source solution for managing and scheduling data pipelines. Airflow represents data pipelines as directed acyclic graphs (DAGs) of operations. You define a workflow in a Python file, and Airflow manages the scheduling and execution. The Airflow Databricks connection lets you take advantage of the optimized Spark engine offered by Databricks with the scheduling features of Airflow.</p>
</div>
<div class="section" id="requirements">
<h2>Requirements<a class="headerlink" href="#requirements" title="Permalink to this headline"> </a></h2>
<ul class="simple">
<li><p>The integration between Airflow and Databricks requires Airflow version 2.5.0 and later. The examples in this article are tested with Airflow version 2.6.1.</p></li>
<li><p>Airflow requires Python 3.8, 3.9, 3.10, or 3.11. The examples in this article are tested with Python 3.8.</p></li>
<li><p>The instructions in this article to install and run Airflow require <a class="reference external" href="https://pipenv.pypa.io/en/latest/">pipenv</a> to create a <a class="reference external" href="https://realpython.com/python-virtual-environments-a-primer/">Python virtual environment</a>.</p></li>
</ul>
</div>
<div class="section" id="airflow-operators-for-databricks">
<h2>Airflow operators for Databricks<a class="headerlink" href="#airflow-operators-for-databricks" title="Permalink to this headline"> </a></h2>
<p>An Airflow DAG is composed of tasks, where each task runs an Airflow <a class="reference external" href="https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/operators.html">Operator</a>. Airflow operators supporting the integration to Databricks are implemented in the <a class="reference external" href="https://airflow.apache.org/docs/apache-airflow-providers-databricks/stable/connections/databricks.html">Databricks provider</a>.</p>
<p>The Databricks provider includes operators to run a number of tasks against a Databricks workspace, including <a class="reference external" href="https://airflow.apache.org/docs/apache-airflow-providers-databricks/stable/_api/airflow/providers/databricks/operators/databricks_sql/index.html#airflow.providers.databricks.operators.databricks_sql.DatabricksCopyIntoOperator">importing data into a table</a>, <a class="reference external" href="https://airflow.apache.org/docs/apache-airflow-providers-databricks/stable/_api/airflow/providers/databricks/operators/databricks_sql/index.html#airflow.providers.databricks.operators.databricks_sql.DatabricksSqlOperator">running SQL queries</a>, and working with <a class="reference external" href="https://airflow.apache.org/docs/apache-airflow-providers-databricks/stable/_api/airflow/providers/databricks/operators/databricks_repos/index.html">Databricks Repos</a>.</p>
<p>The Databricks provider implements two operators for triggering jobs:</p>
<ul class="simple">
<li><p>The <a class="reference external" href="https://airflow.apache.org/docs/apache-airflow-providers-databricks/stable/_api/airflow/providers/databricks/operators/databricks/index.html#airflow.providers.databricks.operators.databricks.DatabricksRunNowOperator">DatabricksRunNowOperator</a> requires an existing Databricks job and uses the <a class="reference external" href="https://docs.databricks.com/api/workspace/jobs/runnow">POST /api/2.1/jobs/run-now</a> API request to trigger a run. Databricks recommends using the <code class="docutils literal notranslate"><span class="pre">DatabricksRunNowOperator</span></code> because it reduces duplication of job definitions, and job runs triggered with this operator can be found in the <a class="reference internal" href="../monitor-job-runs.html#view-job-run-list"><span class="std std-ref">Jobs UI</span></a>.</p></li>
<li><p>The <a class="reference external" href="https://airflow.apache.org/docs/apache-airflow-providers-databricks/stable/_api/airflow/providers/databricks/operators/databricks/index.html#airflow.providers.databricks.operators.databricks.DatabricksSubmitRunOperator">DatabricksSubmitRunOperator</a> does not require a job to exist in Databricks and uses the <a class="reference external" href="https://docs.databricks.com/api/workspace/jobs/submit">POST /api/2.1/jobs/runs/submit</a> API request to submit the job specification and trigger a run.</p></li>
</ul>
<p>To create a new Databricks job or reset an existing job, the Databricks provider implements the <a class="reference external" href="https://airflow.apache.org/docs/apache-airflow-providers-databricks/stable/_api/airflow/providers/databricks/operators/databricks/index.html#airflow.providers.databricks.operators.databricks.DatabricksCreateJobsOperator">DatabricksCreateJobsOperator</a>. The <code class="docutils literal notranslate"><span class="pre">DatabricksCreateJobsOperator</span></code> uses the <a class="reference external" href="https://docs.databricks.com/api/workspace/jobs/create">POST /api/2.1/jobs/create</a> and <a class="reference external" href="https://docs.databricks.com/api/workspace/jobs/reset">POST /api/2.1/jobs/reset</a> API requests. You can use the <code class="docutils literal notranslate"><span class="pre">DatabricksCreateJobsOperator</span></code> with the <code class="docutils literal notranslate"><span class="pre">DatabricksRunNowOperator</span></code> to create and run a job.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Using the Databricks operators to trigger a job requires providing credentials in the Databricks connection configuration. See <a class="reference internal" href="#create-token"><span class="std std-ref">Create a Databricks personal access token for Airflow</span></a>.</p>
</div>
<p>The Databricks Airflow operators write the job run page URL to the Airflow logs every <code class="docutils literal notranslate"><span class="pre">polling_period_seconds</span></code> (the default is 30 seconds). For more information, see the <a class="reference external" href="https://airflow.apache.org/docs/apache-airflow-providers-databricks/stable/index.html">apache-airflow-providers-databricks</a> package page on the Airflow website.</p>
</div>
<div class="section" id="install-the-airflow-databricks-integration-locally">
<h2>Install the Airflow Databricks integration locally<a class="headerlink" href="#install-the-airflow-databricks-integration-locally" title="Permalink to this headline"> </a></h2>
<p>To install Airflow and the Databricks provider locally for testing and development, use the following steps. For other Airflow installation options, including creating a production installation, see <a class="reference external" href="https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html">installation</a> in the Airflow documentation.</p>
<p>Open a terminal and run the following commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>airflow
<span class="nb">cd</span><span class="w"> </span>airflow
pipenv<span class="w"> </span>--python<span class="w"> </span><span class="m">3</span>.8
pipenv<span class="w"> </span>shell
<span class="nb">export</span><span class="w"> </span><span class="nv">AIRFLOW_HOME</span><span class="o">=</span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>
pipenv<span class="w"> </span>install<span class="w"> </span>apache-airflow
pipenv<span class="w"> </span>install<span class="w"> </span>apache-airflow-providers-databricks
mkdir<span class="w"> </span>dags
airflow<span class="w"> </span>db<span class="w"> </span>init
airflow<span class="w"> </span>users<span class="w"> </span>create<span class="w"> </span>--username<span class="w"> </span>admin<span class="w"> </span>--firstname<span class="w"> </span>&lt;firstname&gt;<span class="w"> </span>--lastname<span class="w"> </span>&lt;lastname&gt;<span class="w"> </span>--role<span class="w"> </span>Admin<span class="w"> </span>--email<span class="w"> </span>&lt;email&gt;
</pre></div>
</div>
<p>Replace <code class="docutils literal notranslate"><span class="pre">&lt;firstname&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">&lt;lastname&gt;</span></code>, and <code class="docutils literal notranslate"><span class="pre">&lt;email&gt;</span></code> with your username and email. You will be prompted to enter a password for the admin user. Make sure to save this password because it is required to log in to the Airflow UI.</p>
<p>This script performs the following steps:</p>
<ol class="arabic simple">
<li><p>Creates a directory named <code class="docutils literal notranslate"><span class="pre">airflow</span></code> and changes into that directory.</p></li>
<li><p>Uses <code class="docutils literal notranslate"><span class="pre">pipenv</span></code> to create and spawn a Python virtual environment. Databricks recommends using a Python virtual environment to isolate package versions and code dependencies to that environment. This isolation helps reduce unexpected package version mismatches and code dependency collisions.</p></li>
<li><p>Initializes an environment variable named <code class="docutils literal notranslate"><span class="pre">AIRFLOW_HOME</span></code> set to the path of the <code class="docutils literal notranslate"><span class="pre">airflow</span></code> directory.</p></li>
<li><p>Installs Airflow and the Airflow Databricks provider packages.</p></li>
<li><p>Creates an <code class="docutils literal notranslate"><span class="pre">airflow/dags</span></code> directory. Airflow uses the <code class="docutils literal notranslate"><span class="pre">dags</span></code> directory to store DAG definitions.</p></li>
<li><p>Initializes a SQLite database that Airflow uses to track metadata. In a production Airflow deployment, you would configure Airflow with a standard database. The SQLite database and default configuration for your Airflow deployment are initialized in the <code class="docutils literal notranslate"><span class="pre">airflow</span></code> directory.</p></li>
<li><p>Creates an admin user for Airflow.</p></li>
</ol>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To confirm the installation of the Databricks provider, run the following command in the Airflow installation directory:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>airflow<span class="w"> </span>providers<span class="w"> </span>list
</pre></div>
</div>
</div>
</div>
<div class="section" id="start-the-airflow-web-server-and-scheduler">
<h2>Start the Airflow web server and scheduler<a class="headerlink" href="#start-the-airflow-web-server-and-scheduler" title="Permalink to this headline"> </a></h2>
<p>The Airflow web server is required to view the Airflow UI. To start the web server, open a terminal in the Airflow installation directory and run the following commands:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the Airflow web server fails to start because of a port conflict, you can change the default port in the <a class="reference external" href="https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html">Airflow configuration</a>.</p>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pipenv<span class="w"> </span>shell
<span class="nb">export</span><span class="w"> </span><span class="nv">AIRFLOW_HOME</span><span class="o">=</span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>
airflow<span class="w"> </span>webserver
</pre></div>
</div>
<p>The scheduler is the Airflow component that schedules DAGs. To start the scheduler, open a new terminal in the Airflow installation directory and run the following commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pipenv<span class="w"> </span>shell
<span class="nb">export</span><span class="w"> </span><span class="nv">AIRFLOW_HOME</span><span class="o">=</span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>
airflow<span class="w"> </span>scheduler
</pre></div>
</div>
</div>
<div class="section" id="test-the-airflow-installation">
<h2>Test the Airflow installation<a class="headerlink" href="#test-the-airflow-installation" title="Permalink to this headline"> </a></h2>
<p>To verify the Airflow installation, you can run one of the example DAGs included with Airflow:</p>
<ol class="arabic simple">
<li><p>In a browser window, open <a class="reference external" href="http://localhost:8080/home">http://localhost:8080/home</a>. Log in to the Airflow UI with the username and password you created when installing Airflow. The Airflow <strong>DAGs</strong> page appears.</p></li>
<li><p>Click the <strong>Pause/Unpause DAG</strong> toggle to unpause one of the example DAGs, for example, the <code class="docutils literal notranslate"><span class="pre">example_python_operator</span></code>.</p></li>
<li><p>Trigger the example DAG by clicking the <strong>Trigger DAG</strong> button.</p></li>
<li><p>Click the DAG name to view details, including the run status of the DAG.</p></li>
</ol>
</div>
<div class="section" id="create-a-databricks-personal-access-token-for-airflow">
<span id="create-token"></span><h2>Create a Databricks personal access token for Airflow<a class="headerlink" href="#create-a-databricks-personal-access-token-for-airflow" title="Permalink to this headline"> </a></h2>
<p>Airflow connects to Databricks using a Databricks personal access token (PAT). To create a PAT:</p>
<ol class="arabic simple">
<li><p>In your Databricks workspace, click your Databricks username in the top bar, and then select <strong>User Settings</strong> from the drop down.</p></li>
<li><p>Click <strong>Developer</strong>.</p></li>
<li><p>Next to <strong>Access tokens</strong>, click <strong>Manage</strong>.</p></li>
<li><p>Click <strong>Generate new token</strong>.</p></li>
<li><p>(Optional) Enter a comment that helps you to identify this token in the future, and change the token’s default lifetime of 90 days. To create a token with no lifetime (not recommended), leave the <strong>Lifetime (days)</strong> box empty (blank).</p></li>
<li><p>Click <strong>Generate</strong>.</p></li>
<li><p>Copy the displayed token to a secure location, and then click <strong>Done</strong>.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Be sure to save the copied token in a secure location. Do not share your copied token with others. If you lose the copied token, you cannot regenerate that exact same token. Instead, you must repeat this procedure to create a new token. If you lose the copied token, or you believe that the token has been compromised, Databricks strongly recommends that you immediately delete that token from your workspace by clicking the trash can (<strong>Revoke</strong>) icon next to the token on the <strong>Access tokens</strong> page.</p>
<p>If you are not able to create or use tokens in your workspace, this might be because your workspace administrator has disabled tokens or has not given you permission to create or use tokens. See your workspace administrator or the following:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../../../administration-guide/access-control/tokens.html#enable-tokens"><span class="std std-ref">Enable or disable personal access token authentication for the workspace</span></a></p></li>
<li><p><a class="reference internal" href="../../../security/auth-authz/api-access-permissions.html#pat"><span class="std std-ref">Personal access token permissions</span></a></p></li>
</ul>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As a security best practice when you authenticate with automated tools, systems, scripts, and apps, Databricks recommends that you use OAuth tokens.</p>
<p>If you use personal access token authentication, Databricks recommends using personal access tokens belonging to <a class="reference internal" href="../../../administration-guide/users-groups/service-principals.html"><span class="doc">service principals</span></a> instead of workspace users. To create tokens for service principals, see <a class="reference internal" href="../../../administration-guide/users-groups/service-principals.html#personal-access-tokens"><span class="std std-ref">Manage tokens for a service principal</span></a>.</p>
</div>
<p>You can also authenticate to Databricks using Databricks OAuth for service principals. See <a class="reference external" href="https://airflow.apache.org/docs/apache-airflow-providers-databricks/stable/connections/databricks.html">Databricks Connection</a> in the Airflow documentation.</p>
</div>
<div class="section" id="configure-a-databricks-connection">
<span id="airflow-connection"></span><h2>Configure a Databricks connection<a class="headerlink" href="#configure-a-databricks-connection" title="Permalink to this headline"> </a></h2>
<p>Your Airflow installation contains a default connection for Databricks. To update the connection to connect to your workspace using the personal access token you created above:</p>
<ol class="arabic simple">
<li><p>In a browser window, open <a class="reference external" href="http://localhost:8080/connection/list/">http://localhost:8080/connection/list/</a>. If prompted to sign in, enter your admin username and password.</p></li>
<li><p>Under <strong>Conn ID</strong>, locate <strong>databricks_default</strong> and click the <strong>Edit record</strong> button.</p></li>
<li><p>Replace the value in the <strong>Host</strong> field with the <a class="reference internal" href="../../../workspace/workspace-details.html#workspace-url"><span class="std std-ref">workspace instance name</span></a> of your Databricks deployment, for example, <code class="docutils literal notranslate"><span class="pre">https://adb-123456789.cloud.databricks.com</span></code>.</p></li>
<li><p>In the <strong>Password</strong> field, enter your Databricks personal access token.</p></li>
<li><p>Click <strong>Save</strong>.</p></li>
</ol>
</div>
<div class="section" id="example-create-an-airflow-dag-to-run-a-databricks-job">
<h2>Example: Create an Airflow DAG to run a Databricks job<a class="headerlink" href="#example-create-an-airflow-dag-to-run-a-databricks-job" title="Permalink to this headline"> </a></h2>
<p>The following example demonstrates how to create a simple Airflow deployment that runs on your local machine and deploys an example DAG to trigger runs in Databricks. In this example, you will:</p>
<ol class="arabic simple">
<li><p>Create a new notebook and add code to print a greeting based on a configured parameter.</p></li>
<li><p>Create a Databricks job with a single task that runs the notebook.</p></li>
<li><p>Configure an Airflow connection to your Databricks workspace.</p></li>
<li><p>Create an Airflow DAG to trigger the notebook job. You define the DAG in a Python script using <code class="docutils literal notranslate"><span class="pre">DatabricksRunNowOperator</span></code>.</p></li>
<li><p>Use the Airflow UI to trigger the DAG and view the run status.</p></li>
</ol>
<div class="section" id="create-a-notebook">
<h3>Create a notebook<a class="headerlink" href="#create-a-notebook" title="Permalink to this headline"> </a></h3>
<p>This example uses a notebook containing two cells:</p>
<ul class="simple">
<li><p>The first cell contains a <a class="reference internal" href="../../../dev-tools/databricks-utils.html#dbutils-widgets-text"><span class="std std-ref">Databricks Utilities text widget</span></a> defining a variable named <code class="docutils literal notranslate"><span class="pre">greeting</span></code> set to the default value <code class="docutils literal notranslate"><span class="pre">world</span></code>.</p></li>
<li><p>The second cell prints the value of the <code class="docutils literal notranslate"><span class="pre">greeting</span></code> variable prefixed by <code class="docutils literal notranslate"><span class="pre">hello</span></code>.</p></li>
</ul>
<p>To create the notebook:</p>
<ol class="arabic">
<li><p>Go to your Databricks workspace, click <img alt="New Icon" src="../../../_images/create-icon.png" /> <strong>New</strong> in the sidebar, and select <strong>Notebook</strong>.</p></li>
<li><p>Give your notebook a name, such as <strong>Hello Airflow</strong>, and make sure the default language is set to <strong>Python</strong>.</p></li>
<li><p>Copy the following Python code and paste it into the first cell of the notebook.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dbutils</span><span class="o">.</span><span class="n">widgets</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="s2">&quot;greeting&quot;</span><span class="p">,</span> <span class="s2">&quot;world&quot;</span><span class="p">,</span> <span class="s2">&quot;Greeting&quot;</span><span class="p">)</span>
<span class="n">greeting</span> <span class="o">=</span> <span class="n">dbutils</span><span class="o">.</span><span class="n">widgets</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;greeting&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Add a new cell below the first cell and copy and paste the following Python code into the new cell:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;hello </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">greeting</span><span class="p">))</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="create-a-job">
<h3>Create a job<a class="headerlink" href="#create-a-job" title="Permalink to this headline"> </a></h3>
<ol class="arabic">
<li><p>Click <img alt="Jobs Icon" src="../../../_images/jobs-icon.png" /> <strong>Workflows</strong> in the sidebar.</p></li>
<li><p>Click <img alt="Create Job Button" src="../../../_images/create-job.png" />.</p>
<p>The <strong>Tasks</strong> tab appears with the create task dialog.</p>
<div class="figure align-default">
<img alt="Create first task dialog" src="../../../_images/create-job-dialog.png" />
</div>
</li>
<li><p>Replace <strong>Add a name for your job…</strong> with your job name.</p></li>
<li><p>In the <strong>Task name</strong> field, enter a name for the task, for example, <strong>greeting-task</strong>.</p></li>
<li><p>In the <strong>Type</strong> drop-down menu, select <strong>Notebook</strong>.</p></li>
<li><p>In the <strong>Source</strong> drop-down menu, select <strong>Workspace</strong>.</p></li>
<li><p>Click the <strong>Path</strong> text box and use the file browser to find the notebook you created, click the notebook name, and click <strong>Confirm</strong>.</p></li>
<li><p>Click <strong>Add</strong> under <strong>Parameters</strong>. In the <strong>Key</strong> field, enter <code class="docutils literal notranslate"><span class="pre">greeting</span></code>. In the <strong>Value</strong> field, enter <code class="docutils literal notranslate"><span class="pre">Airflow</span> <span class="pre">user</span></code>.</p></li>
<li><p>Click <strong>Create task</strong>.</p></li>
</ol>
<p>In the <strong>Job details</strong> panel, copy the <strong>Job ID</strong> value. This value is required to trigger the job from Airflow.</p>
</div>
<div class="section" id="run-the-job">
<h3>Run the job<a class="headerlink" href="#run-the-job" title="Permalink to this headline"> </a></h3>
<p>To test your new job in the Databricks Workflows UI, click <img alt="Run Now Button" src="../../../_images/run-now-button.png" /> in the upper right corner. When the run completes, you can verify the output by viewing the <a class="reference internal" href="../monitor-job-runs.html#job-run-details"><span class="std std-ref">job run details</span></a>.</p>
</div>
<div class="section" id="create-a-new-airflow-dag">
<h3>Create a new Airflow DAG<a class="headerlink" href="#create-a-new-airflow-dag" title="Permalink to this headline"> </a></h3>
<p>You define an Airflow DAG in a Python file. To create a DAG to trigger the example notebook job:</p>
<ol class="arabic">
<li><p>In a text editor or IDE, create a new file named <code class="docutils literal notranslate"><span class="pre">databricks_dag.py</span></code> with the following contents:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">airflow</span> <span class="kn">import</span> <span class="n">DAG</span>
<span class="kn">from</span> <span class="nn">airflow.providers.databricks.operators.databricks</span> <span class="kn">import</span> <span class="n">DatabricksRunNowOperator</span>
<span class="kn">from</span> <span class="nn">airflow.utils.dates</span> <span class="kn">import</span> <span class="n">days_ago</span>

<span class="n">default_args</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s1">&#39;owner&#39;</span><span class="p">:</span> <span class="s1">&#39;airflow&#39;</span>
<span class="p">}</span>

<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span><span class="s1">&#39;databricks_dag&#39;</span><span class="p">,</span>
  <span class="n">start_date</span> <span class="o">=</span> <span class="n">days_ago</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
  <span class="n">schedule_interval</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
  <span class="n">default_args</span> <span class="o">=</span> <span class="n">default_args</span>
  <span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>

  <span class="n">opr_run_now</span> <span class="o">=</span> <span class="n">DatabricksRunNowOperator</span><span class="p">(</span>
    <span class="n">task_id</span> <span class="o">=</span> <span class="s1">&#39;run_now&#39;</span><span class="p">,</span>
    <span class="n">databricks_conn_id</span> <span class="o">=</span> <span class="s1">&#39;databricks_default&#39;</span><span class="p">,</span>
    <span class="n">job_id</span> <span class="o">=</span> <span class="n">JOB_ID</span>
  <span class="p">)</span>
</pre></div>
</div>
<p>Replace <code class="docutils literal notranslate"><span class="pre">JOB_ID</span></code> with the value of the job ID saved earlier.</p>
</li>
<li><p>Save the file in the <code class="docutils literal notranslate"><span class="pre">airflow/dags</span></code> directory. Airflow automatically reads and installs DAG files stored in <code class="docutils literal notranslate"><span class="pre">airflow/dags/</span></code>.</p></li>
</ol>
</div>
<div class="section" id="install-and-verify-the-dag-in-airflow">
<span id="test-dag"></span><h3>Install and verify the DAG in Airflow<a class="headerlink" href="#install-and-verify-the-dag-in-airflow" title="Permalink to this headline"> </a></h3>
<p>To trigger and verify the DAG in the Airflow UI:</p>
<ol class="arabic simple">
<li><p>In a browser window, open <a class="reference external" href="http://localhost:8080/home">http://localhost:8080/home</a>. The Airflow <strong>DAGs</strong> screen appears.</p></li>
<li><p>Locate <code class="docutils literal notranslate"><span class="pre">databricks_dag</span></code> and click the <strong>Pause/Unpause DAG</strong> toggle to unpause the DAG.</p></li>
<li><p>Trigger the DAG by clicking the <strong>Trigger DAG</strong> button.</p></li>
<li><p>Click a run in the <strong>Runs</strong> column to view the status and details of the run.</p></li>
</ol>
</div>
</div>
</div>


    
          </div>
        </div>
        <div  class="suapp-rating">
  <div id="suPageRateApp">
     <su-app></su-app>
   </div> 
 </div>
<hr> 
<footer>
  <div role="contentinfo">
      <p class="copyright">
          &copy; Databricks 2023. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the <a href="http://www.apache.org/">Apache Software Foundation</a>.
      </p>
      <p> 
        
          <a id='feedbacklink' href="mailto:doc-feedback@databricks.com?subject=Documentation Feedback">Send us feedback</a>
        
     | <a href="https://databricks.com/privacy-policy">Privacy Policy</a> | <a href="https://databricks.com/terms-of-use">Terms of Use</a></p>

  </div> 

</footer>
      </div>
    </div>
  </section>
</main>

  </page>
  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT: '../../../',
      VERSION: '1.0',
      COLLAPSE_INDEX: false,
      FILE_SUFFIX: '.html',
      HAS_SOURCE: 'false'
    };
  </script>
  <script type="text/javascript" src="../../../_static/jquery.js"></script>
  <script type="text/javascript" src="../../../_static/underscore.js"></script>
  <script type="text/javascript" src="../../../_static/doctools.js"></script>
  <script type="text/javascript" src="../../../_static/language_data.js"></script>
  

  <script type="text/javascript" src="../../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/jquery.waypoints.min.js"></script>

  <!-- Select2 (https://select2.org/) -->
  <link href="https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/css/select2.min.css" rel="stylesheet" />
  <script src="https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/js/select2.min.js"></script>
  <!-- End Select2 -->

  
  
  <script type="text/javascript" src="../../../_static/js/localized.js"></script>
  <script type="text/javascript" src="../../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  
 



  <script>
  window.__searchunifyLoaderConfig = JSON.parse('{"clients": {"en": "02c2e804-27e9-11ee-aefb-0242ac120011", "ja": "6a42c3f2-2820-11ee-aefb-0242ac120011", "pt": "6a86badd-2821-11ee-aefb-0242ac120011"}}')
</script>
<script type="text/javascript" src="../../../_static/js/search-loader.js"></script>
</body>
<script type='text/javascript'>
  window.onload = function () {
    var description = document.querySelector('meta[name="description"]').getAttribute("content");
    let titleText = document.querySelector('h1').textContent;
    document.querySelector('meta[property="og:title"]').setAttribute("content", titleText);
    document.querySelector('meta[property="og:description"]').setAttribute("content", description);
    document.querySelector('meta[property="twitter:description"]').setAttribute("content", description);
  };
</script>

</html>