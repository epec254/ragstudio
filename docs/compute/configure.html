

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en-US" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en-US"> <!--<![endif]-->

<head>
  <!-- cookie consent -->
  
    <!-- Combined Onetrust and Rudderstack Implementation Scripts -->
    <!-- Onetrust Initialization -->
    <script type="text/javascript" src="https://cdn.cookielaw.org/consent/92466579-1717-44d3-809d-a05fb02843ed-test/OtAutoBlock.js"></script>
    <script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="92466579-1717-44d3-809d-a05fb02843ed-test"></script>
    <link rel="stylesheet" id="db-onetrust-style" href="https://www.databricks.com/wp-content/uploads/db_onetrust.css" media="all" />
    <!-- Setting Rudderstack Write Key -->
    <script>window.rudderstackKey = "2SOR9fvSr5Fi6tN2ihPbVHnX1SZ" </script>
    <!-- Rudderstack Initialization + Onetrust Integration + Rudderstack Custom Events -->
    <script type="text/javascript" src="https://www.databricks.com/sites/default/files/rudderstack/v1/db-rudderstack-events.js"></script>

  <!-- cookie consent -->

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="X-UA-Compatible" content="IE=9" />
  <meta content="Learn how to create a cluster using the create compute UI. Also, learn about available options for configuring and managing clusters." name="description" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0">
  <meta property="og:image" content="https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:title" content="Create a cluster">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://docs.databricks.com">
  <meta property="og:description" content="" id="og-description">
  <meta name="twitter:image" content="https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png">
  <meta name="twitter:site" content="@databricks">
  <meta name="twitter:creator" content="@databricks">
  <meta property="twitter:description" content="">
  
  <title>Create a cluster &#124; Databricks on AWS</title>
  
  
  <link rel="canonical" href="https://docs.databricks.com/en/compute/configure.html">
  <!-- Start hreflang tag -->
  <link rel="alternate" hreflang="en" href="https://docs.databricks.com/en/compute/configure.html" />
<link rel="alternate" hreflang="x-default" href="https://docs.databricks.com/en/compute/configure.html" />
  <!-- End hreflang tag -->
  
  
  <link rel="shortcut icon" href="../_static/favicon.ico" />
  

  

  

  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;
j.setAttributeNode(d.createAttribute('data-ot-ignore'));
f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-T85FQ33');</script>
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;
j.setAttributeNode(d.createAttribute('data-ot-ignore'));
f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-TWTKQQ');</script>
    
  <!-- End Google Tag Manager -->


  <!-- MaxMind / GEO IP -->
  <script src="//js.maxmind.com/js/apis/geoip2/v2.1/geoip2.js" type="text/javascript"></script>
  <!-- End MaxMind / GEO IP -->

  
  
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600&display=swap" rel="stylesheet">
  <link rel="preload" href="../_static/fonts/DMSans-Bold.ttf" as="font">
  <link rel="preload" href="../_static/fonts/DMSans-Regular.ttf" as="font">
  <link rel="preload" href="../_static/fonts/DMMono-Regular.ttf" as="font">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/cloud-provider-selector.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/translation-selector.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/searchunify/main.css" type="text/css" />

  
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="top" title="Databricks on AWS" href="../index.html" /> 
</head>

<body class="wy-body-for-nav" role="document">

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T85FQ33"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TWTKQQ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  
  <nav class="wy-nav-top header su_header" role="navigation" aria-label="top navigation">
    
<nav class="wy-nav-top header su_header" role="navigation" aria-label="top navigation">
  <div class="container-logo">
    <ul class="mobile-menu-toggle">
        <li class="menu-toggle">
            <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
            
            <a href="https://www.databricks.com/" class="wy-nav-top-logo"><img src="../_static/small-scale-lockup-full-color-rgb.svg" width="137" height="21"
              alt="Databricks" /></a>   
               
              </li>
    </ul>
    <ul class="su_nav-menu">
      <li class="menu-toggle">
        <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
        
          
        
        <a href="https://www.databricks.com/" class="wy-nav-top-logo"><img src="../_static/small-scale-lockup-full-color-rgb.svg" width="137" height="21"
            alt="Databricks" /></a></li>
        <!-- 
<li><a href="https://help.databricks.com/s/">Help Center</a></li>
<li class="active"><a href="https://docs.databricks.com/en/">Documentation</a></li>
<li><a href="https://kb.databricks.com/">Knowledge Base</a></li>
 -->
    </ul>
  </div>
  <div class="su_nav-right">
    <ul class="su_link-mobile">
  <!-- Mobile header code can go here -->
</ul>
<ul class="right-try-list">
   
</ul>
  </div>
</nav>
  </nav>

  <div class="su_sub-header">
    <div class="container">
      <div class="su_sub-header-inner">
        <!-- <div class="su_subnav-menu-right">
  <div id="auto" style="width: 100%;">
    <div ng-controller="SearchautoController">
      <div bind-html-compile="autocompleteHtml">
        <form class="su__search-box-1" disabled="disabled">
          <input class="su__search-input" type="search" name="Search box" id="su__search-b" placeholder="Search Documentation" disabled="disabled"/>
          <button class="su__search-button" type="submit" class="button button-success" disabled="disabled">
            <svg width="24" height="24" viewBox="0 0 24 24">
              <path
                d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"
                fill="#333"></path>
            </svg>
          </button>
        </form>
      </div>
    </div>
  </div>
</div> -->
        <div class="search-lng-gap"></div>
        <div style="margin-left: 16px; margin-right: 16px;">
          <!-- <select name="lng selector" id="lng-selector">
    <option value="../../en/compute/configure.html" class="notranslate">English</option>
    <option value="../../ja/compute/configure.html" class="notranslate">日本語</option>
    <option value="../../pt/compute/configure.html" class="notranslate">Português (Brasil)</option>
</select> -->
        </div>
        <div class="cloud-selector-container">
          <!-- <select name="cloud provider selector" id="cloud-provider-selector">
    <option value="aws" selected class="notranslate">
        Amazon Web Services
    </option>
    <option value="azure"  class="notranslate">
        Microsoft Azure
    </option>
    <option value="gcp"  class="notranslate">
        Google Cloud Platform
    </option>
</select> -->
        </div>
      </div>
    </div>
  </div>
  <page class="js-page-container">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side su_nav-side">
<div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    

    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../index.html" class="main-navigation-home">Databricks on AWS</a>
    

    
      

      
        <p class="caption"><span class="caption-text">Load &amp; manage data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../rag-temp/index.html">RAG Studio</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    
  <p class="build_info notranslate"data-last-edit="December 23, 2023">
    Updated Jan 11, 2024
  </p>
<script>
  window.addEventListener('DOMContentLoaded',function(){
    var h1=document.querySelector('h1');
    var bi=document.querySelector('[data-last-edit]');
    if(h1 && bi){
      var ver = document.createElement('p');
      ver.className = 'version_info';
      ver.textContent = bi.getAttribute('data-last-edit');
      h1.parentElement.insertBefore(ver, h1.nextElementSibling);
    }
  });
</script>

    <p>
      
        <a id='feedbacklink' href="mailto:doc-feedback@databricks.com?subject=Documentation Feedback">Send us feedback</a>
      
    </p>
  </div>
</div>
</nav>
    
    
<main class="wy-grid-for-nav su_nav-grid">
  <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
    <div class="wy-nav-content su__nav_content">
      <div class="rst-content">
        





<div role="navigation" aria-label="breadcrumbs navigation" class="wy-breadcrumbs-wrapper">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>Create a cluster</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>
</div>
        
        <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
          <div itemprop="articleBody">
            
    
  <div class="section" id="create-a-cluster">
<h1>Create a cluster<a class="headerlink" href="#create-a-cluster" title="Permalink to this headline"> </a></h1>
<p>This article explains the configuration options available for cluster creation in the Databricks UI. For other methods, see <a class="reference internal" href="../archive/dev-tools/cli/clusters-cli.html"><span class="doc">Clusters CLI (legacy)</span></a>, the <a class="reference external" href="https://docs.databricks.com/api/workspace/clusters">Clusters API</a>, and <a class="reference internal" href="../dev-tools/terraform/index.html"><span class="doc">Databricks Terraform provider</span></a>.</p>
<p>This article focuses on all-purpose more than job clusters, although many of the configurations and management tools described apply equally to both cluster types. To learn more about creating job clusters, see <a class="reference internal" href="../workflows/jobs/use-compute.html"><span class="doc">Use Databricks compute with your jobs</span></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>These instructions are for Unity Catalog enabled workspaces. For documentation on the non-Unity Catalog legacy UI, see <a class="reference internal" href="../archive/compute/configure.html"><span class="doc">Configure clusters</span></a>.</p>
</div>
<p>The cluster creation UI lets you select the cluster configuration specifics, including:</p>
<ul class="simple">
<li><p>The <a class="reference internal" href="#cluster-policy"><span class="std std-ref">policy</span></a></p></li>
<li><p>The <a class="reference internal" href="#access-mode"><span class="std std-ref">access mode</span></a>, which controls the security features used when interacting with data</p></li>
<li><p>The <a class="reference internal" href="#version"><span class="std std-ref">runtime version</span></a></p></li>
<li><p>The <a class="reference internal" href="#node-types"><span class="std std-ref">cluster worker and driver node types</span></a></p></li>
</ul>
<div class="section" id="create-a-new-cluster">
<h2>Create a new cluster<a class="headerlink" href="#create-a-new-cluster" title="Permalink to this headline"> </a></h2>
<p>To create a new cluster, click <strong>New</strong> &gt; <strong>Cluster</strong> in your workspace sidebar. This takes you to the <strong>New compute</strong> page, where you will select your cluster’s specifications.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The configuration options you see on this page will vary depending on the policies you have access to. If you don’t see a setting in your UI, it’s because your policy does not allow you to configure that setting.</p>
</div>
</div>
<div class="section" id="policies">
<span id="cluster-policy"></span><h2>Policies<a class="headerlink" href="#policies" title="Permalink to this headline"> </a></h2>
<p><a class="reference internal" href="../administration-guide/clusters/policies.html"><span class="doc">Policies</span></a> are a set of rules used by admins to limit the configuration options available to users when they create a cluster. To configure a cluster according to a policy, select a policy from the <strong>Policy</strong> dropdown.</p>
<p>Policies have access control lists that regulate which users and groups have access to the policies.</p>
<p>If a user doesn’t have the <a class="reference internal" href="../security/auth-authz/access-control/cluster-acl.html"><span class="doc">unrestricted cluster creation entitlement</span></a>, then they can only create clusters using their granted policies.</p>
<div class="section" id="personal-compute-policy">
<span id="personal-compute"></span><h3>Personal compute policy<a class="headerlink" href="#personal-compute-policy" title="Permalink to this headline"> </a></h3>
<p>By default, all users have access to the Personal Compute policy, allowing them to create single-machine compute resources. If you don’t see the Personal Compute policy as an option when you create a cluster, then you haven’t been given access to the policy. Contact your workspace administrator to request access to the Personal Compute policy or an appropriate equivalent policy.</p>
</div>
</div>
<div class="section" id="access-modes">
<span id="access-mode"></span><h2>Access modes<a class="headerlink" href="#access-modes" title="Permalink to this headline"> </a></h2>
<p>Cluster access mode is a security feature that determines who can use a cluster and what data they can access via the cluster. When you create any cluster in Databricks, you must select an access mode.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Databricks recommends that you use shared access mode for all workloads. Only use the assigned access mode if your required functionality is not supported by shared access mode.</p>
</div>
<table class="docutils align-default">
<colgroup>
<col style="width: 4%" />
<col style="width: 37%" />
<col style="width: 2%" />
<col style="width: 24%" />
<col style="width: 32%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Access Mode</p></th>
<th class="head"><p>Visible to user</p></th>
<th class="head"><p>UC Support</p></th>
<th class="head"><p>Supported Languages</p></th>
<th class="head"><p>Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Single user</p></td>
<td><p>Always</p></td>
<td><p>Yes</p></td>
<td><p>Python, SQL, Scala, R</p></td>
<td><p>Can be assigned to and used by a single user.</p></td>
</tr>
<tr class="row-odd"><td><p>Shared</p></td>
<td><p>Always (<strong>Premium plan or above required</strong>)</p></td>
<td><p>Yes</p></td>
<td><p>Python (on Databricks Runtime 11.3 LTS and above), SQL, Scala (on Unity Catalog-enabled clusters using Databricks Runtime 13.3 LTS and above)</p></td>
<td><p>Can be used by multiple users with data isolation among users.</p></td>
</tr>
<tr class="row-even"><td><p>No Isolation Shared</p></td>
<td><p>Admins can hide this cluster type by <a class="reference internal" href="../administration-guide/workspace-settings/enforce-user-isolation.html"><span class="doc">enforcing user isolation</span></a> in the admin settings page.</p></td>
<td><p>No</p></td>
<td><p>Python, SQL, Scala, R</p></td>
<td><p>There is a <a class="reference internal" href="../administration-guide/account-settings/no-isolation-shared.html"><span class="doc">related account-level setting for No Isolation Shared clusters</span></a>.</p></td>
</tr>
<tr class="row-odd"><td><p>Custom</p></td>
<td><p>Hidden (For all new clusters)</p></td>
<td><p>No</p></td>
<td><p>Python, SQL, Scala, R</p></td>
<td><p>This option is shown only if you have existing clusters without a specified access mode.</p></td>
</tr>
</tbody>
</table>
<p>You can upgrade an existing cluster to meet the requirements of Unity Catalog by setting its cluster access mode to <strong>Single User</strong> or <strong>Shared</strong>.</p>
<p>All cluster access modes have some limitations. Clusters configured with Unity Catalog have additional limitations and differences in behavior. Structured Streaming has additional limitations on some cluster access modes. See <a class="reference internal" href="access-mode-limitations.html"><span class="doc">Compute access mode limitations</span></a>.</p>
<div class="section" id="do-init-scripts-and-libraries-work-with-unity-catalog-access-modes">
<span id="do-init-scripts-and-libraries-work-with-uc-access-modes"></span><h3>Do init scripts and libraries work with Unity Catalog access modes?<a class="headerlink" href="#do-init-scripts-and-libraries-work-with-unity-catalog-access-modes" title="Permalink to this headline"> </a></h3>
<p>In Databricks Runtime 13.3 LTS and above, init scripts and libraries are supported on all access modes. Requirements and support vary. See <a class="reference internal" href="compatibility.html"><span class="doc">Compute compatibility with libraries and init scripts</span></a>.</p>
</div>
</div>
<div class="section" id="databricks-runtime-versions">
<span id="dbr-versions"></span><span id="version"></span><h2>Databricks Runtime versions<a class="headerlink" href="#databricks-runtime-versions" title="Permalink to this headline"> </a></h2>
<p>Databricks Runtime is the set of core components that run on your clusters. Select the runtime using the <strong>Databricks Runtime Version</strong> dropdown when you create or edit a cluster. For details on specific Databricks Runtime versions, see <a class="reference internal" href="../release-notes/runtime/index.html"><span class="doc">Databricks Runtime release notes versions and compatibility</span></a>.</p>
<div class="section" id="which-databricks-runtime-version-should-you-use">
<span id="which-dbr-version-should-you-use"></span><h3>Which Databricks Runtime version should you use?<a class="headerlink" href="#which-databricks-runtime-version-should-you-use" title="Permalink to this headline"> </a></h3>
<ul class="simple">
<li><p>For all-purpose compute, Databricks recommends using the latest Databricks Runtime version. Using the most current version will ensure you have the latest optimizations and the most up-to-date compatibility between your code and preloaded packages.</p></li>
<li><p>For job clusters running operational workloads, consider using the Long Term Support (LTS) Databricks Runtime version. Using the LTS version will ensure you don’t run into compatibility issues and can thoroughly test your workload before upgrading.</p></li>
<li><p>For data science and machine learning use cases, consider Databricks Runtime ML version.</p></li>
</ul>
<p>All Databricks Runtime versions include Apache Spark. New versions add components and updates that improve usability, performance, and security.</p>
</div>
<div class="section" id="enable-photon-acceleration">
<span id="photon-image"></span><h3>Enable Photon acceleration<a class="headerlink" href="#enable-photon-acceleration" title="Permalink to this headline"> </a></h3>
<p><a class="reference internal" href="photon.html"><span class="doc">Photon</span></a> is enabled by default on clusters running <a class="reference internal" href="../release-notes/runtime/9.1lts.html"><span class="doc">Databricks Runtime 9.1 LTS</span></a> and above.</p>
<p>To enable or disable Photon acceleration, select the <strong>Use Photon Acceleration</strong> checkbox.</p>
</div>
</div>
<div class="section" id="worker-and-driver-node-types">
<span id="node-types"></span><h2>Worker and driver node types<a class="headerlink" href="#worker-and-driver-node-types" title="Permalink to this headline"> </a></h2>
<p>A cluster consists of one driver node and zero or more worker nodes. You can pick separate cloud provider instance types for the driver and worker nodes, although by default the driver node uses the same instance type as the worker node. Different families of instance types fit different use cases, such as memory-intensive or compute-intensive workloads.</p>
<div class="section" id="worker-type">
<h3>Worker type<a class="headerlink" href="#worker-type" title="Permalink to this headline"> </a></h3>
<p>Databricks worker nodes run the Spark executors and other services required for proper functioning clusters. When you distribute your workload with Spark, all the distributed processing happens on worker nodes. Databricks runs one executor per worker node. Therefore, the terms executor and worker are used interchangeably in the context of the Databricks architecture.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To run a Spark job, you need at least one worker node. If a cluster has zero workers, you can run non-Spark commands on the driver node, but Spark commands will fail.</p>
</div>
<div class="section" id="worker-node-ip-addresses">
<h4>Worker node IP addresses<a class="headerlink" href="#worker-node-ip-addresses" title="Permalink to this headline"> </a></h4>
<p>Databricks launches worker nodes with two private IP addresses each. The node’s primary private IP address hosts Databricks internal traffic. The secondary private IP address is used by the Spark container for intra-cluster communication. This model allows Databricks to provide isolation between multiple clusters in the same workspace.</p>
</div>
</div>
<div class="section" id="driver-type">
<h3>Driver type<a class="headerlink" href="#driver-type" title="Permalink to this headline"> </a></h3>
<p>The driver node maintains state information of all notebooks attached to the cluster. The driver node also maintains the SparkContext, interprets all the commands you run from a notebook or a library on the cluster, and runs the Apache Spark master that coordinates with the Spark executors.</p>
<p>The default value of the driver node type is the same as the worker node type. You can choose a larger driver node type with more memory if you are planning to <code class="docutils literal notranslate"><span class="pre">collect()</span></code> a lot of data from Spark workers and analyze them in the notebook.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Since the driver node maintains all of the state information of the notebooks attached, make sure to detach unused notebooks from the driver node.</p>
</div>
</div>
<div class="section" id="gpu-instance-types">
<h3>GPU instance types<a class="headerlink" href="#gpu-instance-types" title="Permalink to this headline"> </a></h3>
<p>For computationally challenging tasks that demand high performance, like those associated with deep learning, Databricks supports clusters accelerated with graphics processing units (GPUs). For more information, see <a class="reference internal" href="gpu.html"><span class="doc">GPU-enabled clusters</span></a>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Databricks no longer supports spinning up clusters using Amazon EC2 P2 instances.</p>
</div>
<div class="section" id="aws-graviton-instance-types">
<span id="gpu-instance-types"></span><h3>AWS Graviton instance types<a class="headerlink" href="#aws-graviton-instance-types" title="Permalink to this headline"> </a></h3>
<p>Databricks clusters support <a class="reference external" href="https://aws.amazon.com/ec2/graviton/">AWS Graviton</a> instances. These instances use AWS-designed Graviton processors that are built on top of the Arm64 instruction set architecture. AWS claims that instance types with these processors have the best price-to-performance ratio of any instance type on Amazon EC2. To use Graviton instance types, select one of the available AWS Graviton instance type for the <strong>Worker type</strong>, <strong>Driver type</strong>, or both.
#### Availability</p>
<p>Databricks supports AWS Graviton-enabled clusters:</p>
<ul class="simple">
<li><p>On <a class="reference internal" href="../release-notes/runtime/9.1lts.html"><span class="doc">Databricks Runtime 9.1 LTS</span></a> and above for non-<a class="reference internal" href="photon.html"><span class="doc">Photon</span></a>, and <a class="reference internal" href="../archive/runtime-release-notes/10.2.html"><span class="doc">Databricks Runtime 10.2 (unsupported)</span></a> and above for Photon.</p></li>
<li><p>In all AWS Regions. Note, however, that not all instance types are available in all Regions. If you select an instance type that is not available in the Region for a workspace, you get a cluster creation failure.</p></li>
<li><p>For AWS Graviton2 and Graviton3 processors.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Delta Live Tables is not supported on Graviton-enabled clusters.</p>
</div>
</div>
</div>
<div class="section" id="enable-autoscaling">
<span id="autoscaling"></span><h2>Enable autoscaling<a class="headerlink" href="#enable-autoscaling" title="Permalink to this headline"> </a></h2>
<p>When <strong>Enable autoscaling</strong> is checked, you can provide a minimum and maximum number of workers for the cluster. Databricks then chooses the appropriate number of workers required to run your job.</p>
<p>To set the minimum and the maximum number of workers your cluster will autoscale between, use the <strong>Min workers</strong> and <strong>Max workers</strong> fields next to the <strong>Worker type</strong> dropdown.</p>
<p>If you don’t enable autoscaling, you will enter a fixed number of workers in the <strong>Workers</strong> field next to the <strong>Worker type</strong> dropdown.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the cluster is running, the cluster detail page displays the number of allocated workers. You can compare number of allocated workers with the worker configuration and make adjustments as needed.</p>
</div>
<div class="section" id="benefits-of-autoscaling">
<h3>Benefits of autoscaling<a class="headerlink" href="#benefits-of-autoscaling" title="Permalink to this headline"> </a></h3>
<p>With autoscaling, Databricks dynamically reallocates workers to account for the characteristics of your job. Certain parts of your pipeline may be more computationally demanding than others, and Databricks automatically adds additional workers during these phases of your job (and removes them when they’re no longer needed).</p>
<p>Autoscaling makes it easier to achieve high cluster utilization because you don’t need to provision the cluster to match a workload. This applies especially to workloads whose requirements change over time (like exploring a dataset during the course of a day), but it can also apply to a one-time shorter workload whose provisioning requirements are unknown. Autoscaling thus offers two advantages:</p>
<ul class="simple">
<li><p>Workloads can run faster compared to a constant-sized under-provisioned cluster.</p></li>
<li><p>Autoscaling clusters can reduce overall costs compared to a statically-sized cluster.</p></li>
</ul>
<p>Depending on the constant size of the cluster and the workload, autoscaling gives you one or both of these benefits at the same time.  The cluster size can go below the minimum number of workers selected when the cloud provider terminates instances. In this case, Databricks continuously retries to re-provision instances in order to maintain the minimum number of workers.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Autoscaling is not available for <code class="docutils literal notranslate"><span class="pre">spark-submit</span></code> jobs.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Compute auto-scaling has limitations scaling down cluster size for Structured Streaming workloads. Databricks recommends using Delta Live Tables with Enhanced Autoscaling for streaming workloads. See <a class="reference internal" href="../delta-live-tables/auto-scaling.html"><span class="doc">What is Enhanced Autoscaling?</span></a>.</p>
</div>
</div>
<div class="section" id="how-autoscaling-behaves">
<h3>How autoscaling behaves<a class="headerlink" href="#how-autoscaling-behaves" title="Permalink to this headline"> </a></h3>
<p>Workspace in the Premium and Enterprise pricing plans use optimized autoscaling. Workspaces on the standard pricing plan use standard autoscaling.</p>
<p>Optimized autoscaling has the following characteristics:</p>
<ul class="simple">
<li><p>Scales up from min to max in 2 steps.</p></li>
<li><p>Can scale down, even if the cluster is not idle, by looking at the shuffle file state.</p></li>
<li><p>Scales down based on a percentage of current nodes.</p></li>
<li><p>On job clusters, scales down if the cluster is underutilized over the last 40 seconds.</p></li>
<li><p>On all-purpose clusters, scales down if the cluster is underutilized over the last 150 seconds.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">spark.databricks.aggressiveWindowDownS</span></code> Spark configuration property specifies in seconds how often a cluster makes down-scaling decisions. Increasing the value causes a cluster to scale down more slowly. The maximum value is 600.</p></li>
</ul>
<p>Standard autoscaling is used in standard plan workspaces. Standard autoscaling has the following characteristics:</p>
<ul class="simple">
<li><p>Starts with adding 8 nodes. Then scales up exponentially, taking as many steps as required to reach the max.</p></li>
<li><p>Scales down when 90% of the nodes are not busy for 10 minutes and the cluster has been idle for at least 30 seconds.</p></li>
<li><p>Scales down exponentially, starting with 1 node.</p></li>
</ul>
</div>
<div class="section" id="autoscaling-with-pools">
<h3>Autoscaling with pools<a class="headerlink" href="#autoscaling-with-pools" title="Permalink to this headline"> </a></h3>
<p>If you are using an <a class="reference internal" href="pools.html"><span class="doc">instance pool</span></a>:</p>
<ul class="simple">
<li><p>Make sure the cluster size requested is less than or equal to the <a class="reference internal" href="pools.html#pool-min"><span class="std std-ref">minimum number of idle instances</span></a> in the pool. If it is larger, cluster startup time will be equivalent to a cluster that doesn’t use a pool.</p></li>
<li><p>Make sure the maximum cluster size is less than or equal to the <a class="reference internal" href="pools.html#pool-max"><span class="std std-ref">maximum capacity</span></a> of the pool. If it is larger, the cluster creation will fail.</p></li>
</ul>
</div>
<div class="section" id="autoscaling-example">
<h3>Autoscaling example<a class="headerlink" href="#autoscaling-example" title="Permalink to this headline"> </a></h3>
<p>If you reconfigure a static cluster to be an autoscaling cluster, Databricks immediately resizes the cluster within the minimum and maximum bounds and then starts autoscaling. As an example, the following table demonstrates what happens to clusters with a certain initial size if you reconfigure a cluster to autoscale between 5 and 10 nodes.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 33%" />
<col style="width: 67%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Initial size</p></th>
<th class="head"><p>Size after reconfiguration</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>6</p></td>
<td><p>6</p></td>
</tr>
<tr class="row-odd"><td><p>12</p></td>
<td><p>10</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>5</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="autoscaling-local-storage">
<h2>Autoscaling local storage<a class="headerlink" href="#autoscaling-local-storage" title="Permalink to this headline"> </a></h2>
<p>If you don’t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance’s local storage).</p>
<p>To configure autoscaling storage, select <strong>Enable autoscaling local storage</strong>.</p>
<p>The EBS volumes attached to an instance are detached only when the instance is returned to AWS. That is, EBS volumes are never detached from an instance as long as it is part of a running cluster. To scale down EBS usage, Databricks recommends using this feature in a cluster configured with <a class="reference internal" href="#autoscaling"><span class="std std-ref">autoscaling compute</span></a> or <a class="reference internal" href="clusters-manage.html#automatic-termination"><span class="std std-ref">automatic termination</span></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Databricks uses Throughput Optimized HDD (st1) to extend the local storage of an instance. The <a class="reference external" href="https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html#limits_ebs">default AWS capacity limit</a> for these volumes is 20 TiB. To avoid hitting this limit, administrators should request an increase in this limit based on their usage requirements.</p>
</div>
</div>
<div class="section" id="automatic-termination">
<h2>Automatic termination<a class="headerlink" href="#automatic-termination" title="Permalink to this headline"> </a></h2>
<p>You can also set auto termination for a cluster. During cluster creation, you can specify an inactivity period in minutes after which you want the cluster to terminate.</p>
<p>If the difference between the current time and the last command run on the cluster is more than the inactivity period specified, Databricks automatically terminates that cluster. For more information on cluster termination, see <a class="reference internal" href="clusters-manage.html#cluster-terminate"><span class="std std-ref">Terminate a cluster</span></a>.</p>
</div>
<div class="section" id="local-disk-encryption">
<span id="local-disk-encrypt"></span><h2>Local disk encryption<a class="headerlink" href="#local-disk-encryption" title="Permalink to this headline"> </a></h2>
<div class="preview admonition">
<p class="admonition-title">Preview</p>
<p>This feature is in <a class="reference internal" href="../release-notes/release-types.html"><span class="doc">Public Preview</span></a>.</p>
</div>
<p>Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or ephemeral data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data that is stored temporarily on your cluster’s local disks, you can enable local disk encryption.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes.</p>
</div>
<p>When local disk encryption is enabled, Databricks generates an encryption key locally that is unique to each cluster node and is used to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk.</p>
<p>To enable local disk encryption, you must use the <a class="reference external" href="https://docs.databricks.com/api/workspace/clusters">Clusters API</a>. During cluster creation or edit, set <code class="docutils literal notranslate"><span class="pre">enable_local_disk_encryption</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p>
</div>
<div class="section" id="automatic-termination">
<span id="instance-profiles"></span><h2>Instance profiles<a class="headerlink" href="#automatic-termination" title="Permalink to this headline"> </a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Databricks recommends using Unity Catalog external locations to connect to S3 instead of instance profiles. Unity Catalog simplifies the security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See <a class="reference internal" href="../connect/unity-catalog/index.html"><span class="doc">Connect to cloud object storage using Unity Catalog</span></a>.</p>
</div>
<p>To securely access AWS resources without using AWS keys, you can launch Databricks clusters with instance profiles. See <a class="reference internal" href="../connect/storage/tutorial-s3-instance-profile.html"><span class="doc">Tutorial: Configure S3 access with an instance profile</span></a> for information about how to create and configure instance profiles. Once you have created an instance profile, you select it in the <strong>Instance Profile</strong> drop-down list.</p>
<p>After you launch your compute, you can verify that you can access the S3 bucket using the following command. If the command succeeds, that compute resource can access the S3 bucket.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span> <span class="n">dbutils</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">ls</span><span class="p">(</span><span class="s2">&quot;s3a://&lt;s3-bucket-name&gt;/&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Once a cluster launches with an instance profile, anyone who has attach permissions to this cluster can access the underlying resources controlled by this role. To guard against unwanted access, you can use <a class="reference internal" href="../security/auth-authz/access-control/cluster-acl.html"><span class="doc">Cluster access control</span></a> to restrict permissions to the cluster.</p>
</div>
</div>
<div class="section" id="cluster-tags">
<span id="tags"></span><h2>Cluster tags<a class="headerlink" href="#cluster-tags" title="Permalink to this headline"> </a></h2>
<p>Cluster tags allow you to easily monitor the cost of cloud resources used by various groups in your organization. You can specify tags as key-value pairs when you create a cluster, and Databricks applies these tags to cloud resources like VMs and disk volumes, as well as DBU usage reports.</p>
<p>For clusters launched from pools, the custom cluster tags are only applied to DBU usage reports and do not propagate to cloud resources.</p>
<p>For detailed information about how pool and cluster tag types work together, see <a class="reference internal" href="../administration-guide/account-settings/usage-detail-tags.html"><span class="doc">Monitor usage using cluster, pool, and workspace tags</span></a></p>
<p>To configure cluster tags:</p>
<ol class="arabic simple">
<li><p>In the <strong>Tags</strong> section, add a key-value pair for each custom tag.</p></li>
<li><p>Click <strong>Add</strong>.</p></li>
</ol>
</div>
<div class="section" id="aws-configurations">
<span id="cluster-tags"></span><span id="cluster-aws-config"></span><h2>AWS configurations<a class="headerlink" href="#aws-configurations" title="Permalink to this headline"> </a></h2>
<p>When you configure a cluster’s AWS instance you can choose the availability zone, the max spot price, and EBS volume type. These settings are under the <strong>Advanced Options</strong> toggle in the <strong>Instances</strong> tab.</p>
<div class="section" id="availability-zones">
<h3>Availability zones<a class="headerlink" href="#availability-zones" title="Permalink to this headline"> </a></h3>
<p>This setting lets you specify which availability zone (AZ) you want the cluster to use. By default, this setting is set to <strong>auto</strong>, where the AZ is automatically selected based on available IPs in the workspace subnets. Auto-AZ retries in other availability zones if AWS returns insufficient capacity errors.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Auto-AZ works only at cluster startup. After the cluster launches, all the nodes stay in the original availability zone until the cluster is terminated or restarted.</p>
</div>
<p>Choosing a specific AZ for a cluster is useful primarily if your organization has purchased reserved instances in specific availability zones. Read more about <a class="reference external" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html">AWS availability zones</a>.</p>
</div>
<div class="section" id="spot-instances">
<h3>Spot instances<a class="headerlink" href="#spot-instances" title="Permalink to this headline"> </a></h3>
<p>You can specify whether to use spot instances and the max spot price to use when launching spot instances as a percentage of the corresponding on-demand price. By default, the max price is 100% of the on-demand price. See <a class="reference external" href="https://aws.amazon.com/ec2/spot/">AWS spot pricing</a>.</p>
</div>
<div class="section" id="ebs-volumes">
<h3>EBS volumes<a class="headerlink" href="#ebs-volumes" title="Permalink to this headline"> </a></h3>
<p>This section describes the default EBS volume settings for worker nodes, how to add shuffle volumes, and how to configure a cluster so that Databricks automatically allocates EBS volumes.</p>
<p>To configure EBS volumes, click the <strong>Instances</strong> tab in the cluster configuration and select an option in the <strong>EBS Volume Type</strong> dropdown list.</p>
<div class="section" id="default-ebs-volumes">
<h4>Default EBS volumes<a class="headerlink" href="#default-ebs-volumes" title="Permalink to this headline"> </a></h4>
<p>Databricks provisions EBS volumes for every worker node as follows:</p>
<ul class="simple">
<li><p>A 30 GB encrypted EBS instance root volume used by the host operating system and Databricks internal services.</p></li>
<li><p>A 150 GB encrypted EBS container root volume used by the Spark worker. This hosts Spark services and logs.</p></li>
<li><p>(HIPAA only) a 75 GB encrypted EBS worker log volume that stores logs for Databricks internal services.</p></li>
</ul>
</div>
<div class="section" id="add-ebs-shuffle-volumes">
<span id="user-configurable-ebs-volumes"></span><h4>Add EBS shuffle volumes<a class="headerlink" href="#add-ebs-shuffle-volumes" title="Permalink to this headline"> </a></h4>
<p>To add shuffle volumes, select <strong>General Purpose SSD</strong> in the <strong>EBS Volume Type</strong> dropdown list.</p>
<p>By default, Spark shuffle outputs go to the instance local disk. For instance types that do not have a local disk, or if you want to increase your Spark shuffle storage space, you can specify additional EBS volumes.
This is particularly useful to prevent out-of-disk space errors when you run Spark jobs that produce large shuffle outputs.</p>
<p>Databricks encrypts these EBS volumes for both on-demand and spot instances. Read more about <a class="reference external" href="https://aws.amazon.com/ebs/features/">AWS EBS volumes</a>.</p>
</div>
<div class="section" id="optionally-encrypt-databricks-ebs-volumes-with-a-customer-managed-key">
<h4>Optionally encrypt Databricks EBS volumes with a customer-managed key<a class="headerlink" href="#optionally-encrypt-databricks-ebs-volumes-with-a-customer-managed-key" title="Permalink to this headline"> </a></h4>
<p>Optionally, you can encrypt cluster EBS volumes with a customer-managed key.</p>
<p>See <a class="reference internal" href="../security/keys/customer-managed-keys-storage-aws.html"><span class="doc">Customer-managed keys for workspace storage</span></a>.</p>
</div>
<div class="section" id="aws-ebs-limits">
<h4>AWS EBS limits<a class="headerlink" href="#aws-ebs-limits" title="Permalink to this headline"> </a></h4>
<p>Ensure that your AWS EBS limits are high enough to satisfy the runtime requirements for all workers in all clusters.
For information on the default EBS limits and how to change them, see <a class="reference external" href="https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html#limits_ebs">Amazon Elastic Block Store (EBS) Limits</a>.</p>
</div>
<div class="section" id="aws-ebs-ssd-volume-type">
<h4>AWS EBS SSD volume type<a class="headerlink" href="#aws-ebs-ssd-volume-type" title="Permalink to this headline"> </a></h4>
<p>You can select either gp2 or gp3 for your AWS EBS SSD volume type. To do this, see <a class="reference internal" href="../administration-guide/clusters/manage-ssd.html"><span class="doc">Manage SSD storage</span></a>. Databricks recommends you switch to gp3 for its cost savings compared to gp2.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>By default, the Databricks configuration sets the gp3 volume’s IOPS and throughput IOPS to match the maximum performance of a gp2 volume with the same volume size.</p>
</div>
<p>For technical information about gp2 and gp3, see <a class="reference external" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html">Amazon EBS volume types</a>.</p>
</div>
</div>
</div>
<div class="section" id="spark-configuration">
<h2>Spark configuration<a class="headerlink" href="#spark-configuration" title="Permalink to this headline"> </a></h2>
<p>To fine-tune Spark jobs, you can provide custom <a class="reference external" href="https://spark.apache.org/docs/latest/configuration.html">Spark configuration properties</a> in a cluster configuration.</p>
<ol class="arabic">
<li><p>On the cluster configuration page, click the <strong>Advanced Options</strong> toggle.</p></li>
<li><p>Click the <strong>Spark</strong> tab.</p>
<div class="figure align-default">
<img alt="Spark configuration" src="../_images/spark-config-aws.png" />
</div>
<p>In <strong>Spark config</strong>, enter the configuration properties as one key-value pair per line.</p>
</li>
</ol>
<p>When you configure a cluster using the <a class="reference external" href="https://docs.databricks.com/api/workspace/clusters">Clusters API</a>, set Spark properties in the <code class="docutils literal notranslate"><span class="pre">spark_conf</span></code> field in the <a class="reference external" href="https://docs.databricks.com/api/workspace/clusters/create">Create new cluster API</a> or <a class="reference external" href="https://docs.databricks.com/api/workspace/clusters/edit">Update cluster configuration API</a>.</p>
<p>To enforce Spark configurations on clusters, workspace admins can use <a class="reference internal" href="../administration-guide/clusters/policies.html"><span class="doc">cluster policies</span></a>.</p>
</div>
<div class="section" id="retrieve-a-spark-configuration-property-from-a-secret">
<h2>Retrieve a Spark configuration property from a secret<a class="headerlink" href="#retrieve-a-spark-configuration-property-from-a-secret" title="Permalink to this headline"> </a></h2>
<p>Databricks recommends storing sensitive information, such as passwords, in a <a class="reference internal" href="../security/secrets/secrets.html"><span class="doc">secret</span></a> instead of plaintext. To reference a secret in the Spark configuration, use the following syntax:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="na">spark.&lt;property-name&gt; {{secrets/&lt;scope-name&gt;/&lt;secret-name&gt;}}</span>
</pre></div>
</div>
<p>For example, to set a Spark configuration property called <code class="docutils literal notranslate"><span class="pre">password</span></code> to the value of the secret stored in <code class="docutils literal notranslate"><span class="pre">secrets/acme_app/password</span></code>:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="na">spark.password {{secrets/acme-app/password}}</span>
</pre></div>
</div>
<p>For more information, see <a class="reference internal" href="../security/secrets/secrets.html#path-value"><span class="std std-ref">Syntax for referencing secrets in a Spark configuration property or environment variable</span></a>.</p>
</div>
<div class="section" id="environment-variables">
<span id="env-var"></span><h2>Environment variables<a class="headerlink" href="#environment-variables" title="Permalink to this headline"> </a></h2>
<p>You can configure custom environment variables that you can access from <a class="reference internal" href="../init-scripts/index.html"><span class="doc">init scripts</span></a> running on a cluster. Databricks also provides predefined <a class="reference internal" href="../init-scripts/environment-variables.html"><span class="doc">environment variables</span></a> that you can use in init scripts. You cannot override these predefined environment variables.</p>
<ol class="arabic">
<li><p>On the cluster configuration page, click the <strong>Advanced Options</strong> toggle.</p></li>
<li><p>Click the <strong>Spark</strong> tab.</p></li>
<li><p>Set the environment variables in the <strong>Environment Variables</strong> field.</p>
<div class="figure align-default">
<img alt="Environment Variables field" src="../_images/environment-variables.png" />
</div>
</li>
</ol>
<p>You can also set environment variables using the  <code class="docutils literal notranslate"><span class="pre">spark_env_vars</span></code> field in the <a class="reference external" href="https://docs.databricks.com/api/workspace/clusters/create">Create new cluster API</a> or <a class="reference external" href="https://docs.databricks.com/api/workspace/clusters/edit">Update cluster configuration API</a>.</p>
</div>
<div class="section" id="cluster-log-delivery">
<h2>Cluster log delivery<a class="headerlink" href="#cluster-log-delivery" title="Permalink to this headline"> </a></h2>
<p>When you create a cluster, you can specify a location to deliver the logs for the Spark driver node, worker nodes, and events. Logs are delivered every five minutes and archived hourly in your chosen destination. When a cluster is terminated, Databricks guarantees to deliver all logs generated up until the cluster was terminated.</p>
<p>The destination of the logs depends on the cluster ID. If the specified destination is
<code class="docutils literal notranslate"><span class="pre">dbfs:/cluster-log-delivery</span></code>, cluster logs for <code class="docutils literal notranslate"><span class="pre">0630-191345-leap375</span></code> are delivered to
<code class="docutils literal notranslate"><span class="pre">dbfs:/cluster-log-delivery/0630-191345-leap375</span></code>.</p>
<p>To configure the log delivery location:</p>
<ol class="arabic simple">
<li><p>On the cluster configuration page, click the <strong>Advanced Options</strong> toggle.</p></li>
<li><p>Click the <strong>Logging</strong> tab.</p></li>
<li><p>Select a destination type.</p></li>
<li><p>Enter the cluster log path.</p></li>
</ol>
<div class="section" id="s3-bucket-destinations">
<h3>S3 bucket destinations<a class="headerlink" href="#s3-bucket-destinations" title="Permalink to this headline"> </a></h3>
<p>If you choose an S3 destination, you must configure the cluster with an instance profile that can access the bucket.
This instance profile must have both the <code class="docutils literal notranslate"><span class="pre">PutObject</span></code> and <code class="docutils literal notranslate"><span class="pre">PutObjectAcl</span></code> permissions. An example instance profile
has been included for your convenience. See <a class="reference internal" href="../connect/storage/tutorial-s3-instance-profile.html"><span class="doc">Tutorial: Configure S3 access with an instance profile</span></a> for instructions on how to set up an instance profile.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;Version&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2012-10-17&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;Statement&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;Effect&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Allow&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;Action&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="s2">&quot;s3:ListBucket&quot;</span>
<span class="w">      </span><span class="p">],</span>
<span class="w">      </span><span class="nt">&quot;Resource&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="s2">&quot;arn:aws:s3:::&lt;my-s3-bucket&gt;&quot;</span>
<span class="w">      </span><span class="p">]</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;Effect&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Allow&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;Action&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="s2">&quot;s3:PutObject&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;s3:PutObjectAcl&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;s3:GetObject&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;s3:DeleteObject&quot;</span>
<span class="w">      </span><span class="p">],</span>
<span class="w">      </span><span class="nt">&quot;Resource&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="s2">&quot;arn:aws:s3:::&lt;my-s3-bucket&gt;/*&quot;</span>
<span class="w">      </span><span class="p">]</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This feature is also available in the REST API. See the <a class="reference external" href="https://docs.databricks.com/api/workspace/clusters">Clusters API</a>.</p>
</div>
<div class="toctree-wrapper compound">
</div>
</div>
</div>


    
          </div>
        </div>
        <div  class="suapp-rating">
  <div id="suPageRateApp">
     <su-app></su-app>
   </div> 
 </div>
<hr> 
<footer>
  <div role="contentinfo">
      <p class="copyright">
          &copy; Databricks 2023. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the <a href="http://www.apache.org/">Apache Software Foundation</a>.
      </p>
      <p> 
        
          <a id='feedbacklink' href="mailto:doc-feedback@databricks.com?subject=Documentation Feedback">Send us feedback</a>
        
     | <a href="https://databricks.com/privacy-policy">Privacy Policy</a> | <a href="https://databricks.com/terms-of-use">Terms of Use</a></p>

  </div> 

</footer>
      </div>
    </div>
  </section>
</main>

  </page>
  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT: '../',
      VERSION: '1.0',
      COLLAPSE_INDEX: false,
      FILE_SUFFIX: '.html',
      HAS_SOURCE: 'false'
    };
  </script>
  <script type="text/javascript" src="../_static/jquery.js"></script>
  <script type="text/javascript" src="../_static/underscore.js"></script>
  <script type="text/javascript" src="../_static/doctools.js"></script>
  <script type="text/javascript" src="../_static/language_data.js"></script>
  

  <script type="text/javascript" src="../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../_static/js/jquery.waypoints.min.js"></script>

  <!-- Select2 (https://select2.org/) -->
  <link href="https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/css/select2.min.css" rel="stylesheet" />
  <script src="https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/js/select2.min.js"></script>
  <!-- End Select2 -->

  
  
  <script type="text/javascript" src="../_static/js/localized.js"></script>
  <script type="text/javascript" src="../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  
 



  <script>
  window.__searchunifyLoaderConfig = JSON.parse('{"clients": {"en": "02c2e804-27e9-11ee-aefb-0242ac120011", "ja": "6a42c3f2-2820-11ee-aefb-0242ac120011", "pt": "6a86badd-2821-11ee-aefb-0242ac120011"}}')
</script>
<script type="text/javascript" src="../_static/js/search-loader.js"></script>
</body>
<script type='text/javascript'>
  window.onload = function () {
    var description = document.querySelector('meta[name="description"]').getAttribute("content");
    let titleText = document.querySelector('h1').textContent;
    document.querySelector('meta[property="og:title"]').setAttribute("content", titleText);
    document.querySelector('meta[property="og:description"]').setAttribute("content", description);
    document.querySelector('meta[property="twitter:description"]').setAttribute("content", description);
  };
</script>

</html>