

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en-US" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en-US"> <!--<![endif]-->

<head>
  <!-- cookie consent -->
  
    <!-- Combined Onetrust and Rudderstack Implementation Scripts -->
    <!-- Onetrust Initialization -->
    <script type="text/javascript" src="https://cdn.cookielaw.org/consent/92466579-1717-44d3-809d-a05fb02843ed-test/OtAutoBlock.js"></script>
    <script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="92466579-1717-44d3-809d-a05fb02843ed-test"></script>
    <link rel="stylesheet" id="db-onetrust-style" href="https://www.databricks.com/wp-content/uploads/db_onetrust.css" media="all" />
    <!-- Setting Rudderstack Write Key -->
    <script>window.rudderstackKey = "2SOR9fvSr5Fi6tN2ihPbVHnX1SZ" </script>
    <!-- Rudderstack Initialization + Onetrust Integration + Rudderstack Custom Events -->
    <script type="text/javascript" src="https://www.databricks.com/sites/default/files/rudderstack/v1/db-rudderstack-events.js"></script>

  <!-- cookie consent -->

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="X-UA-Compatible" content="IE=9" />
  <meta content="This article describes how to load data to Databricks with Delta Live Tables." name="description" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0">
  <meta property="og:image" content="https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:title" content="Load data with Delta Live Tables">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://docs.databricks.com">
  <meta property="og:description" content="" id="og-description">
  <meta name="twitter:image" content="https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png">
  <meta name="twitter:site" content="@databricks">
  <meta name="twitter:creator" content="@databricks">
  <meta property="twitter:description" content="">
  
  <title>Load data with Delta Live Tables &#124; Databricks on AWS</title>
  
  
  <link rel="canonical" href="https://docs.databricks.com/en/delta-live-tables/load.html">
  <!-- Start hreflang tag -->
  <link rel="alternate" hreflang="en" href="https://docs.databricks.com/en/delta-live-tables/load.html" />
<link rel="alternate" hreflang="x-default" href="https://docs.databricks.com/en/delta-live-tables/load.html" />
  <!-- End hreflang tag -->
  
  
  <link rel="shortcut icon" href="../_static/favicon.ico" />
  

  

  

  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;
j.setAttributeNode(d.createAttribute('data-ot-ignore'));
f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-T85FQ33');</script>
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;
j.setAttributeNode(d.createAttribute('data-ot-ignore'));
f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-TWTKQQ');</script>
    
  <!-- End Google Tag Manager -->


  <!-- MaxMind / GEO IP -->
  <script src="//js.maxmind.com/js/apis/geoip2/v2.1/geoip2.js" type="text/javascript"></script>
  <!-- End MaxMind / GEO IP -->

  
  
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600&display=swap" rel="stylesheet">
  <link rel="preload" href="../_static/fonts/DMSans-Bold.ttf" as="font">
  <link rel="preload" href="../_static/fonts/DMSans-Regular.ttf" as="font">
  <link rel="preload" href="../_static/fonts/DMMono-Regular.ttf" as="font">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/cloud-provider-selector.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/translation-selector.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/searchunify/main.css" type="text/css" />

  
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="top" title="Databricks on AWS" href="../index.html" /> 
</head>

<body class="wy-body-for-nav" role="document">

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T85FQ33"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TWTKQQ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  
  <nav class="wy-nav-top header su_header" role="navigation" aria-label="top navigation">
    
<nav class="wy-nav-top header su_header" role="navigation" aria-label="top navigation">
  <div class="container-logo">
    <ul class="mobile-menu-toggle">
        <li class="menu-toggle">
            <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
            
            <a href="https://www.databricks.com/" class="wy-nav-top-logo"><img src="../_static/small-scale-lockup-full-color-rgb.svg" width="137" height="21"
              alt="Databricks" /></a>   
               
              </li>
    </ul>
    <ul class="su_nav-menu">
      <li class="menu-toggle">
        <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
        
          
        
        <a href="https://www.databricks.com/" class="wy-nav-top-logo"><img src="../_static/small-scale-lockup-full-color-rgb.svg" width="137" height="21"
            alt="Databricks" /></a></li>
        <!-- 
<li><a href="https://help.databricks.com/s/">Help Center</a></li>
<li class="active"><a href="https://docs.databricks.com/en/">Documentation</a></li>
<li><a href="https://kb.databricks.com/">Knowledge Base</a></li>
 -->
    </ul>
  </div>
  <div class="su_nav-right">
    <ul class="su_link-mobile">
  <!-- Mobile header code can go here -->
</ul>
<ul class="right-try-list">
   
</ul>
  </div>
</nav>
  </nav>

  <div class="su_sub-header">
    <div class="container">
      <div class="su_sub-header-inner">
        <!-- <div class="su_subnav-menu-right">
  <div id="auto" style="width: 100%;">
    <div ng-controller="SearchautoController">
      <div bind-html-compile="autocompleteHtml">
        <form class="su__search-box-1" disabled="disabled">
          <input class="su__search-input" type="search" name="Search box" id="su__search-b" placeholder="Search Documentation" disabled="disabled"/>
          <button class="su__search-button" type="submit" class="button button-success" disabled="disabled">
            <svg width="24" height="24" viewBox="0 0 24 24">
              <path
                d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"
                fill="#333"></path>
            </svg>
          </button>
        </form>
      </div>
    </div>
  </div>
</div> -->
        <div class="search-lng-gap"></div>
        <div style="margin-left: 16px; margin-right: 16px;">
          <!-- <select name="lng selector" id="lng-selector">
    <option value="../../en/delta-live-tables/load.html" class="notranslate">English</option>
    <option value="../../ja/delta-live-tables/load.html" class="notranslate">日本語</option>
    <option value="../../pt/delta-live-tables/load.html" class="notranslate">Português (Brasil)</option>
</select> -->
        </div>
        <div class="cloud-selector-container">
          <!-- <select name="cloud provider selector" id="cloud-provider-selector">
    <option value="aws" selected class="notranslate">
        Amazon Web Services
    </option>
    <option value="azure"  class="notranslate">
        Microsoft Azure
    </option>
    <option value="gcp"  class="notranslate">
        Google Cloud Platform
    </option>
</select> -->
        </div>
      </div>
    </div>
  </div>
  <page class="js-page-container">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side su_nav-side">
<div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    

    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../index.html" class="main-navigation-home">Databricks on AWS</a>
    

    
      

      
        <p class="caption"><span class="caption-text">Load &amp; manage data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../rag-temp/index.html">RAG Studio</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    
  <p class="build_info notranslate"data-last-edit="December 23, 2023">
    Updated Jan 11, 2024
  </p>
<script>
  window.addEventListener('DOMContentLoaded',function(){
    var h1=document.querySelector('h1');
    var bi=document.querySelector('[data-last-edit]');
    if(h1 && bi){
      var ver = document.createElement('p');
      ver.className = 'version_info';
      ver.textContent = bi.getAttribute('data-last-edit');
      h1.parentElement.insertBefore(ver, h1.nextElementSibling);
    }
  });
</script>

    <p>
      
        <a id='feedbacklink' href="mailto:doc-feedback@databricks.com?subject=Documentation Feedback">Send us feedback</a>
      
    </p>
  </div>
</div>
</nav>
    
    
<main class="wy-grid-for-nav su_nav-grid">
  <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
    <div class="wy-nav-content su__nav_content">
      <div class="rst-content">
        





<div role="navigation" aria-label="breadcrumbs navigation" class="wy-breadcrumbs-wrapper">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>Load data with Delta Live Tables</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>
</div>
        
        <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
          <div itemprop="articleBody">
            
    
  <div class="section" id="load-data-with-delta-live-tables">
<span id="load-data-with-dlt"></span><h1>Load data with Delta Live Tables<a class="headerlink" href="#load-data-with-delta-live-tables" title="Permalink to this headline"> </a></h1>
<p>You can load data from any data source supported by Apache Spark on Databricks using Delta Live Tables. You can define datasets (tables and views) in Delta Live Tables against any query that returns a Spark DataFrame, including streaming DataFrames and Pandas for Spark DataFrames. For data ingestion tasks, Databricks recommends using streaming tables for most use cases. Streaming tables are good for ingesting data from cloud object storage using Auto Loader or from message buses like Kafka. The examples below demonstrate some common patterns.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Not all data sources have SQL support. You can mix SQL and Python notebooks in a Delta Live Tables pipeline to use SQL for all operations beyond ingestion.</p>
</div>
<p>For details on working with libraries not packaged in Delta Live Tables by default, see <a class="reference internal" href="external-dependencies.html"><span class="doc">Pipeline dependencies</span></a>.</p>
<div class="toctree-wrapper compound">
</div>
<div class="section" id="load-files-from-cloud-object-storage">
<h2>Load files from cloud object storage<a class="headerlink" href="#load-files-from-cloud-object-storage" title="Permalink to this headline"> </a></h2>
<p>Databricks recommends using Auto Loader with Delta Live Tables for most data ingestion tasks from cloud object storage. Auto Loader and Delta Live Tables are designed to incrementally and idempotently load ever-growing data as it arrives in cloud storage. The following examples use Auto Loader to create datasets from CSV and JSON files:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To load files with Auto Loader in a Unity Catalog enabled pipeline, you must use <a class="reference internal" href="../connect/unity-catalog/external-locations.html"><span class="doc">external locations</span></a>. To learn more about using Unity Catalog with Delta Live Tables, see <a class="reference internal" href="unity-catalog.html"><span class="doc">Use Unity Catalog with your Delta Live Tables pipelines</span></a>.</p>
</div>
<div class="js-code-language-tabs js-code-language-tabs--literal compound">
<div class="compound-first highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@dlt</span><span class="o">.</span><span class="n">table</span>
<span class="k">def</span> <span class="nf">customers</span><span class="p">():</span>
  <span class="k">return</span> <span class="p">(</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;cloudFiles&quot;</span><span class="p">)</span>
      <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;cloudFiles.format&quot;</span><span class="p">,</span> <span class="s2">&quot;csv&quot;</span><span class="p">)</span>
      <span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;/databricks-datasets/retail-org/customers/&quot;</span><span class="p">)</span>
  <span class="p">)</span>

<span class="nd">@dlt</span><span class="o">.</span><span class="n">table</span>
<span class="k">def</span> <span class="nf">sales_orders_raw</span><span class="p">():</span>
  <span class="k">return</span> <span class="p">(</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;cloudFiles&quot;</span><span class="p">)</span>
      <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;cloudFiles.format&quot;</span><span class="p">,</span> <span class="s2">&quot;json&quot;</span><span class="p">)</span>
      <span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;/databricks-datasets/retail-org/sales_orders/&quot;</span><span class="p">)</span>
  <span class="p">)</span>
</pre></div>
</div>
<div class="compound-last highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">CREATE</span><span class="w"> </span><span class="k">OR</span><span class="w"> </span><span class="n">REFRESH</span><span class="w"> </span><span class="n">STREAMING</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">customers</span>
<span class="k">AS</span><span class="w"> </span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">cloud_files</span><span class="p">(</span><span class="ss">&quot;/databricks-datasets/retail-org/customers/&quot;</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;csv&quot;</span><span class="p">)</span>

<span class="k">CREATE</span><span class="w"> </span><span class="k">OR</span><span class="w"> </span><span class="n">REFRESH</span><span class="w"> </span><span class="n">STREAMING</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">sales_orders_raw</span>
<span class="k">AS</span><span class="w"> </span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">cloud_files</span><span class="p">(</span><span class="ss">&quot;/databricks-datasets/retail-org/sales_orders/&quot;</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;json&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>See <a class="reference internal" href="../ingestion/auto-loader/index.html"><span class="doc">What is Auto Loader?</span></a> and <a class="reference internal" href="sql-ref.html#auto-loader-sql"><span class="std std-ref">Auto Loader SQL syntax</span></a>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use Auto Loader with file notifications and run a full refresh for your pipeline or streaming table, you must manually clean up your resources. You can use the <a class="reference internal" href="../ingestion/auto-loader/file-notification-mode.html#cloud-resource-management"><span class="std std-ref">CloudFilesResourceManager</span></a> in a notebook to perform cleanup.</p>
</div>
</div>
<div class="section" id="load-data-from-a-message-bus">
<h2>Load data from a message bus<a class="headerlink" href="#load-data-from-a-message-bus" title="Permalink to this headline"> </a></h2>
<p>You can configure Delta Live Tables pipelines to ingest data from message buses with streaming tables. Databricks recommends combining streaming tables with continuous execution and enhanced autoscaling to provide the most efficient ingestion for low-latency loading from message buses. See <a class="reference internal" href="auto-scaling.html"><span class="doc">What is Enhanced Autoscaling?</span></a>.</p>
<p>For example, the following code configures a streaming table to ingest data from Kafka:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">dlt</span>

<span class="nd">@dlt</span><span class="o">.</span><span class="n">table</span>
<span class="k">def</span> <span class="nf">kafka_raw</span><span class="p">():</span>
  <span class="k">return</span> <span class="p">(</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span>
      <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;kafka&quot;</span><span class="p">)</span>
      <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;kafka.bootstrap.servers&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;server:ip&gt;&quot;</span><span class="p">)</span>
      <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;subscribe&quot;</span><span class="p">,</span> <span class="s2">&quot;topic1&quot;</span><span class="p">)</span>
      <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;startingOffsets&quot;</span><span class="p">,</span> <span class="s2">&quot;latest&quot;</span><span class="p">)</span>
      <span class="o">.</span><span class="n">load</span><span class="p">()</span>
  <span class="p">)</span>
</pre></div>
</div>
<p>You can write downstream operations in pure SQL to perform streaming transformations on this data, as in the following example:</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">CREATE</span><span class="w"> </span><span class="k">OR</span><span class="w"> </span><span class="n">REFRESH</span><span class="w"> </span><span class="n">STREAMING</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">streaming_silver_table</span>
<span class="k">AS</span><span class="w"> </span><span class="k">SELECT</span>
<span class="w">  </span><span class="o">*</span>
<span class="k">FROM</span>
<span class="w">  </span><span class="n">STREAM</span><span class="p">(</span><span class="n">LIVE</span><span class="p">.</span><span class="n">kafka_raw</span><span class="p">)</span>
<span class="k">WHERE</span><span class="w"> </span><span class="p">...</span>
</pre></div>
</div>
<p>For an example of working with Event Hubs, see <a class="reference internal" href="event-hubs.html"><span class="doc">Use Azure Event Hubs as a Delta Live Tables data source</span></a>.</p>
<p>See <a class="reference internal" href="../connect/streaming/index.html"><span class="doc">Configure streaming data sources</span></a>.</p>
</div>
<div class="section" id="load-data-from-external-systems">
<h2>Load data from external systems<a class="headerlink" href="#load-data-from-external-systems" title="Permalink to this headline"> </a></h2>
<p>Delta Live Tables supports loading data from any data source supported by Databricks. See <a class="reference internal" href="../connect/index.html"><span class="doc">Connect to data sources</span></a>. You can also load external data using Lakehouse Federation for <a class="reference internal" href="../query-federation/index.html#connection-types"><span class="std std-ref">supported data sources</span></a>. Because Lakehouse Federation requires Databricks Runtime 13.1 or above, to use Lakehouse Federation your pipeline must be configured to use the <a class="reference internal" href="properties.html#config-settings"><span class="std std-ref">preview channel</span></a>.</p>
<p>Some data sources do not have equivalent support in SQL. If you cannot use Lakehouse Federation with one of these data sources, you can use a standalone Python notebook to ingest data from the source. This notebook can then be added as a source library with SQL notebooks to build a Delta Live Tables pipeline. The following example declares a materialized view to access the current state of data in a remote PostgreSQL table:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">dlt</span>

<span class="nd">@dlt</span><span class="o">.</span><span class="n">table</span>
<span class="k">def</span> <span class="nf">postgres_raw</span><span class="p">():</span>
  <span class="k">return</span> <span class="p">(</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">read</span>
      <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;postgresql&quot;</span><span class="p">)</span>
      <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;dbtable&quot;</span><span class="p">,</span> <span class="n">table_name</span><span class="p">)</span>
      <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;host&quot;</span><span class="p">,</span> <span class="n">database_host_url</span><span class="p">)</span>
      <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;port&quot;</span><span class="p">,</span> <span class="mi">5432</span><span class="p">)</span>
      <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;database&quot;</span><span class="p">,</span> <span class="n">database_name</span><span class="p">)</span>
      <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="n">username</span><span class="p">)</span>
      <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;password&quot;</span><span class="p">,</span> <span class="n">password</span><span class="p">)</span>
      <span class="o">.</span><span class="n">load</span><span class="p">()</span>
  <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="load-small-or-static-datasets-from-cloud-object-storage">
<h2>Load small or static datasets from cloud object storage<a class="headerlink" href="#load-small-or-static-datasets-from-cloud-object-storage" title="Permalink to this headline"> </a></h2>
<p>You can load small or static datasets using Apache Spark load syntax. Delta Live Tables supports all of the file formats supported by Apache Spark on Databricks. For a full list, see <a class="reference internal" href="../query/formats/index.html"><span class="doc">Data format options</span></a>.</p>
<p>The following examples demonstrate loading JSON to create Delta Live Tables tables:</p>
<div class="js-code-language-tabs js-code-language-tabs--literal compound">
<div class="compound-first highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@dlt</span><span class="o">.</span><span class="n">table</span>
<span class="k">def</span> <span class="nf">clickstream_raw</span><span class="p">():</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;json&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json&quot;</span><span class="p">))</span>
</pre></div>
</div>
<div class="compound-last highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">CREATE</span><span class="w"> </span><span class="k">OR</span><span class="w"> </span><span class="n">REFRESH</span><span class="w"> </span><span class="n">LIVE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">clickstream_raw</span>
<span class="k">AS</span><span class="w"> </span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">json</span><span class="p">.</span><span class="o">`/</span><span class="n">databricks</span><span class="o">-</span><span class="n">datasets</span><span class="o">/</span><span class="n">wikipedia</span><span class="o">-</span><span class="n">datasets</span><span class="o">/</span><span class="k">data</span><span class="o">-</span><span class="mi">001</span><span class="o">/</span><span class="n">clickstream</span><span class="o">/</span><span class="n">raw</span><span class="o">-</span><span class="n">uncompressed</span><span class="o">-</span><span class="n">json</span><span class="o">/</span><span class="mi">2015</span><span class="n">_2_clickstream</span><span class="p">.</span><span class="n">json</span><span class="o">`</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">SELECT</span> <span class="pre">*</span> <span class="pre">FROM</span> <span class="pre">format.`path`;</span></code> SQL construct is common to all SQL environments on Databricks. It is the recommended pattern for direct file access using SQL with Delta Live Tables.</p>
</div>
</div>
<div class="section" id="securely-access-storage-credentials-with-secrets-in-a-pipeline">
<span id="configure-secrets"></span><h2>Securely access storage credentials with secrets in a pipeline<a class="headerlink" href="#securely-access-storage-credentials-with-secrets-in-a-pipeline" title="Permalink to this headline"> </a></h2>
<p>You can use Databricks <a class="reference internal" href="../security/secrets/index.html"><span class="doc">secrets</span></a> to store credentials such as access keys or passwords. To configure the secret in your pipeline, use a Spark property in the pipeline settings cluster configuration. See <a class="reference internal" href="settings.html#cluster-config"><span class="std std-ref">Configure your compute settings</span></a>.</p>
<p>The following example uses a secret to store an access key required to read input data from an Azure Data Lake Storage Gen2 (ADLS Gen2) storage account using <a class="reference internal" href="../ingestion/auto-loader/index.html"><span class="doc">Auto Loader</span></a>. You can use this same method to configure any secret required by your pipeline, for example, AWS keys to access S3, or the password to an Apache Hive metastore.</p>
<p>To learn more about working with Azure Data Lake Storage Gen2, see <a class="reference internal" href="../connect/storage/azure-storage.html"><span class="doc">Connect to Azure Data Lake Storage Gen2 and Blob Storage</span></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You must add the <code class="docutils literal notranslate"><span class="pre">spark.hadoop.</span></code> prefix to the <code class="docutils literal notranslate"><span class="pre">spark_conf</span></code> configuration key that sets the secret value.</p>
</div>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;43246596-a63f-11ec-b909-0242ac120002&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;clusters&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;spark_conf&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">          </span><span class="nt">&quot;spark.hadoop.fs.azure.account.key.&lt;storage-account-name&gt;.dfs.core.windows.net&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;{{secrets/&lt;scope-name&gt;/&lt;secret-name&gt;}}&quot;</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="nt">&quot;autoscale&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">          </span><span class="nt">&quot;min_workers&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">          </span><span class="nt">&quot;max_workers&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span>
<span class="w">          </span><span class="nt">&quot;mode&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;ENHANCED&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;development&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;continuous&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;libraries&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;notebook&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">          </span><span class="nt">&quot;path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/Users/user@databricks.com/DLT Notebooks/Delta Live Tables quickstart&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;DLT quickstart using ADLS2&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Replace</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;storage-account-name&gt;</span></code> with the ADLS Gen2 storage account name.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;scope-name&gt;</span></code> with the Databricks secret scope name.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;secret-name&gt;</span></code> with the name of the key containing the Azure storage account access key.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">dlt</span>

<span class="n">json_path</span> <span class="o">=</span> <span class="s2">&quot;abfss://&lt;container-name&gt;@&lt;storage-account-name&gt;.dfs.core.windows.net/&lt;path-to-input-dataset&gt;&quot;</span>
<span class="nd">@dlt</span><span class="o">.</span><span class="n">create_table</span><span class="p">(</span>
  <span class="n">comment</span><span class="o">=</span><span class="s2">&quot;Data ingested from an ADLS2 storage account.&quot;</span>
<span class="p">)</span>
<span class="k">def</span> <span class="nf">read_from_ADLS2</span><span class="p">():</span>
  <span class="k">return</span> <span class="p">(</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;cloudFiles&quot;</span><span class="p">)</span>
      <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;cloudFiles.format&quot;</span><span class="p">,</span> <span class="s2">&quot;json&quot;</span><span class="p">)</span>
      <span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">json_path</span><span class="p">)</span>
  <span class="p">)</span>
</pre></div>
</div>
<p>Replace</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;container-name&gt;</span></code> with the name of the Azure storage account container that stores the input data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;storage-account-name&gt;</span></code> with the ADLS Gen2 storage account name.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;path-to-input-dataset&gt;</span></code> with the path to the input dataset.</p></li>
</ul>
</div>
</div>


    
          </div>
        </div>
        <div  class="suapp-rating">
  <div id="suPageRateApp">
     <su-app></su-app>
   </div> 
 </div>
<hr> 
<footer>
  <div role="contentinfo">
      <p class="copyright">
          &copy; Databricks 2023. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the <a href="http://www.apache.org/">Apache Software Foundation</a>.
      </p>
      <p> 
        
          <a id='feedbacklink' href="mailto:doc-feedback@databricks.com?subject=Documentation Feedback">Send us feedback</a>
        
     | <a href="https://databricks.com/privacy-policy">Privacy Policy</a> | <a href="https://databricks.com/terms-of-use">Terms of Use</a></p>

  </div> 

</footer>
      </div>
    </div>
  </section>
</main>

  </page>
  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT: '../',
      VERSION: '1.0',
      COLLAPSE_INDEX: false,
      FILE_SUFFIX: '.html',
      HAS_SOURCE: 'false'
    };
  </script>
  <script type="text/javascript" src="../_static/jquery.js"></script>
  <script type="text/javascript" src="../_static/underscore.js"></script>
  <script type="text/javascript" src="../_static/doctools.js"></script>
  <script type="text/javascript" src="../_static/language_data.js"></script>
  

  <script type="text/javascript" src="../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../_static/js/jquery.waypoints.min.js"></script>

  <!-- Select2 (https://select2.org/) -->
  <link href="https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/css/select2.min.css" rel="stylesheet" />
  <script src="https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/js/select2.min.js"></script>
  <!-- End Select2 -->

  
  
  <script type="text/javascript" src="../_static/js/localized.js"></script>
  <script type="text/javascript" src="../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  
 



  <script>
  window.__searchunifyLoaderConfig = JSON.parse('{"clients": {"en": "02c2e804-27e9-11ee-aefb-0242ac120011", "ja": "6a42c3f2-2820-11ee-aefb-0242ac120011", "pt": "6a86badd-2821-11ee-aefb-0242ac120011"}}')
</script>
<script type="text/javascript" src="../_static/js/search-loader.js"></script>
</body>
<script type='text/javascript'>
  window.onload = function () {
    var description = document.querySelector('meta[name="description"]').getAttribute("content");
    let titleText = document.querySelector('h1').textContent;
    document.querySelector('meta[property="og:title"]').setAttribute("content", titleText);
    document.querySelector('meta[property="og:description"]').setAttribute("content", description);
    document.querySelector('meta[property="twitter:description"]').setAttribute("content", description);
  };
</script>

</html>