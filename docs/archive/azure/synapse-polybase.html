

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en-US" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en-US"> <!--<![endif]-->

<head>
  <!-- cookie consent -->
  
    <!-- Combined Onetrust and Rudderstack Implementation Scripts -->
    <!-- Onetrust Initialization -->
    <script type="text/javascript" src="https://cdn.cookielaw.org/consent/92466579-1717-44d3-809d-a05fb02843ed-test/OtAutoBlock.js"></script>
    <script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="92466579-1717-44d3-809d-a05fb02843ed-test"></script>
    <link rel="stylesheet" id="db-onetrust-style" href="https://www.databricks.com/wp-content/uploads/db_onetrust.css" media="all" />
    <!-- Setting Rudderstack Write Key -->
    <script>window.rudderstackKey = "2SOR9fvSr5Fi6tN2ihPbVHnX1SZ" </script>
    <!-- Rudderstack Initialization + Onetrust Integration + Rudderstack Custom Events -->
    <script type="text/javascript" src="https://www.databricks.com/sites/default/files/rudderstack/v1/db-rudderstack-events.js"></script>

  <!-- cookie consent -->

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="X-UA-Compatible" content="IE=9" />
  <meta content="View configuration information for the legacy PolyBase connector for Databricks and Synapse." name="description" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0">
  <meta property="og:image" content="https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:title" content="Connecting Databricks and Azure Synapse with PolyBase (legacy)">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://docs.databricks.com">
  <meta property="og:description" content="" id="og-description">
  <meta name="twitter:image" content="https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png">
  <meta name="twitter:site" content="@databricks">
  <meta name="twitter:creator" content="@databricks">
  <meta property="twitter:description" content="">
  
  <title>Connecting Databricks and Azure Synapse with PolyBase (legacy) &#124; Databricks on AWS</title>
  
  
  <link rel="canonical" href="https://docs.databricks.com/en/archive/azure/synapse-polybase.html">
  <!-- Start hreflang tag -->
  <link rel="alternate" hreflang="en" href="https://docs.databricks.com/en/archive/azure/synapse-polybase.html" />
<link rel="alternate" hreflang="x-default" href="https://docs.databricks.com/en/archive/azure/synapse-polybase.html" />
  <!-- End hreflang tag -->
  
  
  <link rel="shortcut icon" href="../../_static/favicon.ico" />
  

  

  

  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;
j.setAttributeNode(d.createAttribute('data-ot-ignore'));
f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-T85FQ33');</script>
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;
j.setAttributeNode(d.createAttribute('data-ot-ignore'));
f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-TWTKQQ');</script>
    
  <!-- End Google Tag Manager -->


  <!-- MaxMind / GEO IP -->
  <script src="//js.maxmind.com/js/apis/geoip2/v2.1/geoip2.js" type="text/javascript"></script>
  <!-- End MaxMind / GEO IP -->

  
  
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600&display=swap" rel="stylesheet">
  <link rel="preload" href="../../_static/fonts/DMSans-Bold.ttf" as="font">
  <link rel="preload" href="../../_static/fonts/DMSans-Regular.ttf" as="font">
  <link rel="preload" href="../../_static/fonts/DMMono-Regular.ttf" as="font">
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/cloud-provider-selector.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/translation-selector.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/searchunify/main.css" type="text/css" />

  
  <link rel="index" title="Index" href="../../genindex.html" />
  <link rel="search" title="Search" href="../../search.html" />
  <link rel="top" title="Databricks on AWS" href="../../index.html" /> 
</head>

<body class="wy-body-for-nav" role="document">

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T85FQ33"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TWTKQQ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  
  <nav class="wy-nav-top header su_header" role="navigation" aria-label="top navigation">
    
<nav class="wy-nav-top header su_header" role="navigation" aria-label="top navigation">
  <div class="container-logo">
    <ul class="mobile-menu-toggle">
        <li class="menu-toggle">
            <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
            
            <a href="https://www.databricks.com/" class="wy-nav-top-logo"><img src="../../_static/small-scale-lockup-full-color-rgb.svg" width="137" height="21"
              alt="Databricks" /></a>   
               
              </li>
    </ul>
    <ul class="su_nav-menu">
      <li class="menu-toggle">
        <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
        
          
        
        <a href="https://www.databricks.com/" class="wy-nav-top-logo"><img src="../../_static/small-scale-lockup-full-color-rgb.svg" width="137" height="21"
            alt="Databricks" /></a></li>
        <!-- 
<li><a href="https://help.databricks.com/s/">Help Center</a></li>
<li class="active"><a href="https://docs.databricks.com/en/">Documentation</a></li>
<li><a href="https://kb.databricks.com/">Knowledge Base</a></li>
 -->
    </ul>
  </div>
  <div class="su_nav-right">
    <ul class="su_link-mobile">
  <!-- Mobile header code can go here -->
</ul>
<ul class="right-try-list">
   
</ul>
  </div>
</nav>
  </nav>

  <div class="su_sub-header">
    <div class="container">
      <div class="su_sub-header-inner">
        <!-- <div class="su_subnav-menu-right">
  <div id="auto" style="width: 100%;">
    <div ng-controller="SearchautoController">
      <div bind-html-compile="autocompleteHtml">
        <form class="su__search-box-1" disabled="disabled">
          <input class="su__search-input" type="search" name="Search box" id="su__search-b" placeholder="Search Documentation" disabled="disabled"/>
          <button class="su__search-button" type="submit" class="button button-success" disabled="disabled">
            <svg width="24" height="24" viewBox="0 0 24 24">
              <path
                d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"
                fill="#333"></path>
            </svg>
          </button>
        </form>
      </div>
    </div>
  </div>
</div> -->
        <div class="search-lng-gap"></div>
        <div style="margin-left: 16px; margin-right: 16px;">
          <!-- <select name="lng selector" id="lng-selector">
    <option value="../../../en/archive/azure/synapse-polybase.html" class="notranslate">English</option>
    <option value="../../../ja/archive/azure/synapse-polybase.html" class="notranslate">日本語</option>
    <option value="../../../pt/archive/azure/synapse-polybase.html" class="notranslate">Português (Brasil)</option>
</select> -->
        </div>
        <div class="cloud-selector-container">
          <!-- <select name="cloud provider selector" id="cloud-provider-selector">
    <option value="aws" selected class="notranslate">
        Amazon Web Services
    </option>
    <option value="azure"  class="notranslate">
        Microsoft Azure
    </option>
    <option value="gcp"  class="notranslate">
        Google Cloud Platform
    </option>
</select> -->
        </div>
      </div>
    </div>
  </div>
  <page class="js-page-container">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side su_nav-side">
<div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    

    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../index.html" class="main-navigation-home">Databricks on AWS</a>
    

    
      

      
        <p class="caption"><span class="caption-text">Load &amp; manage data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../rag-temp/index.html">RAG Studio</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    
  <p class="build_info notranslate"data-last-edit="December 23, 2023">
    Updated Jan 11, 2024
  </p>
<script>
  window.addEventListener('DOMContentLoaded',function(){
    var h1=document.querySelector('h1');
    var bi=document.querySelector('[data-last-edit]');
    if(h1 && bi){
      var ver = document.createElement('p');
      ver.className = 'version_info';
      ver.textContent = bi.getAttribute('data-last-edit');
      h1.parentElement.insertBefore(ver, h1.nextElementSibling);
    }
  });
</script>

    <p>
      
        <a id='feedbacklink' href="mailto:doc-feedback@databricks.com?subject=Documentation Feedback">Send us feedback</a>
      
    </p>
  </div>
</div>
</nav>
    
    
<main class="wy-grid-for-nav su_nav-grid">
  <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
    <div class="wy-nav-content su__nav_content">
      <div class="rst-content">
        





<div role="navigation" aria-label="breadcrumbs navigation" class="wy-breadcrumbs-wrapper">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>Connecting Databricks and Azure Synapse with PolyBase (legacy)</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>
</div>
        
        <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
          <div itemprop="articleBody">
            
    
  <div class="section" id="connecting-databricks-and-azure-synapse-with-polybase-legacy">
<h1>Connecting Databricks and Azure Synapse with PolyBase (legacy)<a class="headerlink" href="#connecting-databricks-and-azure-synapse-with-polybase-legacy" title="Permalink to this headline"> </a></h1>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>This documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. See <a class="reference internal" href="../../connect/external-systems/synapse-analytics.html"><span class="doc">Query data in Azure Synapse Analytics</span></a>.</p>
</div>
<p>Databricks recommends using the default <code class="docutils literal notranslate"><span class="pre">COPY</span></code> functionality with Azure Data Lake Storage Gen2 for connections to Azure Synapse. This article includes legacy documentation around PolyBase and blob storage.</p>
<p><a class="reference external" href="https://azure.microsoft.com/services/synapse-analytics/">Azure Synapse Analytics</a> (formerly SQL Data Warehouse) is a cloud-based enterprise data warehouse that leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data. Use Azure as a key component of a big data solution. Import big data into Azure with simple <a class="reference external" href="https://learn.microsoft.com/azure/synapse-analytics/sql-data-warehouse/load-data-wideworldimportersdw">PolyBase</a> T-SQL queries, or <a class="reference external" href="https://learn.microsoft.com/sql/t-sql/statements/copy-into-transact-sql">COPY</a> statement and then use the power of MPP to run high-performance analytics. As you integrate and analyze, the data warehouse will become the single version of truth your business can count on for insights.</p>
<p>You can access Azure Synapse from Databricks using the Azure Synapse connector, a data source implementation for Apache Spark that uses <a class="reference external" href="https://azure.microsoft.com/services/storage/blobs/">Azure Blob storage</a>, and PolyBase or the <code class="docutils literal notranslate"><span class="pre">COPY</span></code> statement in Azure Synapse to transfer large volumes of data efficiently between a Databricks cluster and an Azure Synapse instance.</p>
<p>Both the Databricks cluster and the Azure Synapse instance access a common Blob storage container to exchange data between these two systems. In Databricks, Apache Spark jobs are triggered by the Azure Synapse connector to read data from and write data to the Blob storage container. On the Azure Synapse side, data loading and unloading operations performed by PolyBase are triggered by the Azure Synapse connector through JDBC. In Databricks Runtime 7.0 and above, <code class="docutils literal notranslate"><span class="pre">COPY</span></code> is used by default to load data into Azure Synapse by the Azure Synapse connector through JDBC.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">COPY</span></code> is available only on Azure Synapse Gen2 instances, which provide <a class="reference external" href="https://learn.microsoft.com/azure/sql-data-warehouse/upgrade-to-latest-generation">better performance</a>. If your database still uses Gen1 instances, we recommend that you migrate the database to Gen2.</p>
</div>
<p>The Azure Synapse connector is more suited to ETL than to interactive queries, because each query execution can extract large amounts of data to Blob storage. If you plan to perform several queries against the same Azure Synapse table, we recommend that you save the extracted data in a format such as Parquet.</p>
<div class="section" id="requirements">
<h2>Requirements<a class="headerlink" href="#requirements" title="Permalink to this headline"> </a></h2>
<p>An Azure Synapse <a class="reference external" href="https://learn.microsoft.com/sql/relational-databases/security/encryption/create-a-database-master-key">database master key</a>.</p>
</div>
<div class="section" id="authentication">
<span id="azure-credentials"></span><h2>Authentication<a class="headerlink" href="#authentication" title="Permalink to this headline"> </a></h2>
<p>The Azure Synapse connector uses three types of network connections:</p>
<ul class="simple">
<li><p>Spark driver to Azure Synapse</p></li>
<li><p>Spark driver and executors to Azure storage account</p></li>
<li><p>Azure Synapse to Azure storage account</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>                                 ┌─────────┐
      ┌─────────────────────────&gt;│ STORAGE │&lt;────────────────────────┐
      │   Storage acc key /      │ ACCOUNT │  Storage acc key /      │
      │   Managed Service ID /   └─────────┘  OAuth 2.0 /            │
      │                               │                              │
      │                               │                              │
      │                               │ Storage acc key /            │
      │                               │ OAuth 2.0 /                  │
      │                               │                              │
      v                               v                       ┌──────v────┐
┌──────────┐                      ┌──────────┐                │┌──────────┴┐
│ Synapse  │                      │  Spark   │                ││ Spark     │
│ Analytics│&lt;────────────────────&gt;│  Driver  │&lt;───────────────&gt;│ Executors │
└──────────┘  JDBC with           └──────────┘    Configured   └───────────┘
              username &amp; password /                in Spark
</pre></div>
</div>
<p>The following sections describe each connection’s authentication configuration options.</p>
<div class="section" id="spark-driver-to-azure-synapse">
<h3>Spark driver to Azure Synapse<a class="headerlink" href="#spark-driver-to-azure-synapse" title="Permalink to this headline"> </a></h3>
<p>The Spark driver can connect to Azure Synapse using JDBC with a username and password or OAuth 2.0 with a service principal for authentication.</p>
<div class="section" id="username-and-password">
<h4>Username and password<a class="headerlink" href="#username-and-password" title="Permalink to this headline"> </a></h4>
<p>We recommend that you use the connection strings provided by Azure portal for both authentication types, which enable
Secure Sockets Layer (SSL) encryption for all data sent between the Spark driver and the Azure Synapse
instance through the JDBC connection. To verify that the SSL encryption is enabled, you can search for
<code class="docutils literal notranslate"><span class="pre">encrypt=true</span></code> in the connection string.</p>
<p>To allow the Spark driver to reach Azure Synapse, we recommend that you
set <strong>Allow Azure services and resources to access this workspace</strong> to <strong>ON</strong> on the Networking pane under Security of the Azure Synapse workspace through the Azure portal.
This setting allows communications from all Azure IP addresses and all Azure subnets, which
allows Spark drivers to reach the Azure Synapse instance.</p>
</div>
<div class="section" id="oauth-20-with-a-service-principal">
<h4>OAuth 2.0 with a service principal<a class="headerlink" href="#oauth-20-with-a-service-principal" title="Permalink to this headline"> </a></h4>
<p>You can authenticate to Azure Synapse Analytics using a service principal with access to the underlying storage account. For more information on using service principal credentials to access an Azure storage account, see <a class="reference internal" href="../../connect/storage/azure-storage.html"><span class="doc">Connect to Azure Data Lake Storage Gen2 and Blob Storage</span></a>. You must set the <code class="docutils literal notranslate"><span class="pre">enableServicePrincipalAuth</span></code> option to <code class="docutils literal notranslate"><span class="pre">true</span></code> in the connection configuration <a class="reference internal" href="#parameters"><span class="std std-ref">Parameters</span></a> to enable the connector to authenticate with a service principal.</p>
<p>You can optionally use a different service principal for the Azure Synapse Analytics connection. An example that configures service principal credentials for the storage account and optional service principal credentials for Synapse:</p>
<div class="js-code-language-tabs js-code-language-tabs--literal compound">
<div class="compound-first highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="c1">; Defining the Service Principal credentials for the Azure storage account</span>
<span class="na">fs.azure.account.auth.type OAuth</span>
<span class="na">fs.azure.account.oauth.provider.type org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider</span>
<span class="na">fs.azure.account.oauth2.client.id &lt;application-id&gt;</span>
<span class="na">fs.azure.account.oauth2.client.secret &lt;service-credential&gt;</span>
<span class="na">fs.azure.account.oauth2.client.endpoint https</span><span class="o">:</span><span class="s">//login.microsoftonline.com/&lt;directory-id&gt;/oauth2/token</span>

<span class="c1">; Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials)</span>
<span class="na">spark.databricks.sqldw.jdbc.service.principal.client.id &lt;application-id&gt;</span>
<span class="na">spark.databricks.sqldw.jdbc.service.principal.client.secret &lt;service-credential&gt;</span>
</pre></div>
</div>
<div class="compound-middle highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="c1">// Defining the Service Principal credentials for the Azure storage account</span>
<span class="n">spark</span><span class="p">.</span><span class="n">conf</span><span class="p">.</span><span class="n">set</span><span class="p">(</span><span class="s">&quot;fs.azure.account.auth.type&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;OAuth&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="p">.</span><span class="n">conf</span><span class="p">.</span><span class="n">set</span><span class="p">(</span><span class="s">&quot;fs.azure.account.oauth.provider.type&quot;</span><span class="p">,</span><span class="w">  </span><span class="s">&quot;org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="p">.</span><span class="n">conf</span><span class="p">.</span><span class="n">set</span><span class="p">(</span><span class="s">&quot;fs.azure.account.oauth2.client.id&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;application-id&gt;&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="p">.</span><span class="n">conf</span><span class="p">.</span><span class="n">set</span><span class="p">(</span><span class="s">&quot;fs.azure.account.oauth2.client.secret&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;service-credential&gt;&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="p">.</span><span class="n">conf</span><span class="p">.</span><span class="n">set</span><span class="p">(</span><span class="s">&quot;fs.azure.account.oauth2.client.endpoint&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;https://login.microsoftonline.com/&lt;directory-id&gt;/oauth2/token&quot;</span><span class="p">)</span>

<span class="c1">// Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials)</span>
<span class="n">spark</span><span class="p">.</span><span class="n">conf</span><span class="p">.</span><span class="n">set</span><span class="p">(</span><span class="s">&quot;spark.databricks.sqldw.jdbc.service.principal.client.id&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;application-id&gt;&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="p">.</span><span class="n">conf</span><span class="p">.</span><span class="n">set</span><span class="p">(</span><span class="s">&quot;spark.databricks.sqldw.jdbc.service.principal.client.secret&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;service-credential&gt;&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="compound-middle highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Defining the service principal credentials for the Azure storage account</span>
<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;fs.azure.account.auth.type&quot;</span><span class="p">,</span> <span class="s2">&quot;OAuth&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;fs.azure.account.oauth.provider.type&quot;</span><span class="p">,</span>  <span class="s2">&quot;org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;fs.azure.account.oauth2.client.id&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;application-id&gt;&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;fs.azure.account.oauth2.client.secret&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;service-credential&gt;&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;fs.azure.account.oauth2.client.endpoint&quot;</span><span class="p">,</span> <span class="s2">&quot;https://login.microsoftonline.com/&lt;directory-id&gt;/oauth2/token&quot;</span><span class="p">)</span>

<span class="c1"># Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.databricks.sqldw.jdbc.service.principal.client.id&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;application-id&gt;&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.databricks.sqldw.jdbc.service.principal.client.secret&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;service-credential&gt;&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="compound-last highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load SparkR</span>
<span class="nf">library</span><span class="p">(</span><span class="n">SparkR</span><span class="p">)</span>
<span class="n">conf</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="nf">sparkR.session</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;conf&quot;</span><span class="p">)</span>

<span class="c1"># Defining the service principal credentials for the Azure storage account</span>
<span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;set&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;fs.azure.account.auth.type&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;OAuth&quot;</span><span class="p">)</span>
<span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;set&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;fs.azure.account.oauth.provider.type&quot;</span><span class="p">,</span><span class="w">  </span><span class="s">&quot;org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider&quot;</span><span class="p">)</span>
<span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;set&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;fs.azure.account.oauth2.client.id&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;application-id&gt;&quot;</span><span class="p">)</span>
<span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;set&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;fs.azure.account.oauth2.client.secret&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;service-credential&gt;&quot;</span><span class="p">)</span>
<span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;set&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;fs.azure.account.oauth2.client.endpoint&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;https://login.microsoftonline.com/&lt;directory-id&gt;/oauth2/token&quot;</span><span class="p">)</span>

<span class="c1"># Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials)</span>
<span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;set&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;spark.databricks.sqldw.jdbc.service.principal.client.id&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;application-id&gt;&quot;</span><span class="p">)</span>
<span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;set&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;spark.databricks.sqldw.jdbc.service.principal.client.secret&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;service-credential&gt;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="spark-driver-and-executors-to-azure-storage-account">
<h3>Spark driver and executors to Azure storage account<a class="headerlink" href="#spark-driver-and-executors-to-azure-storage-account" title="Permalink to this headline"> </a></h3>
<p>The Azure storage container acts as an intermediary to store bulk data when reading from or writing to Azure Synapse. Spark connects to <a class="reference internal" href="../../connect/storage/azure-storage.html"><span class="doc">ADLS Gen2 or Blob Storage</span></a> using the <code class="docutils literal notranslate"><span class="pre">abfss</span></code> driver.</p>
<p>The following authentication options are available:</p>
<ul class="simple">
<li><p>Storage account access key and secret</p></li>
<li><p>OAuth 2.0 authentication. For more information about OAuth 2.0 and service principals, see <a class="reference internal" href="../../connect/storage/aad-storage-service-principal.html"><span class="doc">Access storage with Microsoft Entra ID (formerly Azure Active Directory) using a service principal</span></a>.</p></li>
</ul>
<p>The examples below illustrate these two ways using the storage account access key approach. The same applies to OAuth 2.0 configuration.</p>
<div class="section" id="notebook-session-configuration-preferred">
<h4>Notebook session configuration (preferred)<a class="headerlink" href="#notebook-session-configuration-preferred" title="Permalink to this headline"> </a></h4>
<p>Using this approach, the account access key is set in the session configuration associated with the notebook that runs the command. This configuration does not affect other notebooks attached to the same cluster. <code class="docutils literal notranslate"><span class="pre">spark</span></code> is the <code class="docutils literal notranslate"><span class="pre">SparkSession</span></code> object provided in the notebook.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
  <span class="s2">&quot;fs.azure.account.key.&lt;your-storage-account-name&gt;.dfs.core.windows.net&quot;</span><span class="p">,</span>
  <span class="s2">&quot;&lt;your-storage-account-access-key&gt;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="global-hadoop-configuration">
<h4>Global Hadoop configuration<a class="headerlink" href="#global-hadoop-configuration" title="Permalink to this headline"> </a></h4>
<p>This approach updates the global Hadoop configuration associated with the <code class="docutils literal notranslate"><span class="pre">SparkContext</span></code> object shared by all notebooks.</p>
<div class="js-code-language-tabs compound">
<div class="compound-first compound" lang="scala">
<div class="highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="n">sc</span><span class="p">.</span><span class="n">hadoopConfiguration</span><span class="p">.</span><span class="n">set</span><span class="p">(</span>
<span class="w">  </span><span class="s">&quot;fs.azure.account.key.&lt;your-storage-account-name&gt;.dfs.core.windows.net&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="s">&quot;&lt;your-storage-account-access-key&gt;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="compound-last compound" lang="python">
<p class="compound-first"><code class="docutils literal notranslate"><span class="pre">hadoopConfiguration</span></code> is not exposed in all versions of PySpark. Although the following command relies on some Spark internals, it should work with all PySpark versions and is unlikely to break or change in the future:</p>
<div class="compound-last highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sc</span><span class="o">.</span><span class="n">_jsc</span><span class="o">.</span><span class="n">hadoopConfiguration</span><span class="p">()</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
  <span class="s2">&quot;fs.azure.account.key.&lt;your-storage-account-name&gt;.dfs.core.windows.net&quot;</span><span class="p">,</span>
  <span class="s2">&quot;&lt;your-storage-account-access-key&gt;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="azure-synapse-to-azure-storage-account">
<h3>Azure Synapse to Azure storage account<a class="headerlink" href="#azure-synapse-to-azure-storage-account" title="Permalink to this headline"> </a></h3>
<p>Azure Synapse also connects to a storage account during loading and unloading of temporary data.</p>
<p>In case you have set up an account key and secret for the storage account, you can set <code class="docutils literal notranslate"><span class="pre">forwardSparkAzureStorageCredentials</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code>, in which case
Azure Synapse connector automatically discovers the account access key set in the notebook session configuration or
the global Hadoop configuration and forwards the storage account access key to the connected Azure Synapse instance by creating a temporary Azure
<a class="reference external" href="https://learn.microsoft.com/sql/t-sql/statements/create-database-scoped-credential-transact-sql">database scoped credential</a>.</p>
<p>Alternatively, if you use ADLS Gen2 with OAuth 2.0 authentication or your Azure Synapse instance is configured to have a Managed Service Identity (typically in conjunction with a
<a class="reference external" href="https://azure.microsoft.com/blog/general-availability-of-vnet-service-endpoints-for-azure-sql-data-warehouse/">VNet + Service Endpoints setup</a>), you must set <code class="docutils literal notranslate"><span class="pre">useAzureMSI</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code>. In this case the connector will specify <code class="docutils literal notranslate"><span class="pre">IDENTITY</span> <span class="pre">=</span> <span class="pre">'Managed</span> <span class="pre">Service</span> <span class="pre">Identity'</span></code> for the databased scoped credential and no <code class="docutils literal notranslate"><span class="pre">SECRET</span></code>.</p>
</div>
</div>
<div class="section" id="streaming-support">
<span id="streaming_support"></span><h2>Streaming support<a class="headerlink" href="#streaming-support" title="Permalink to this headline"> </a></h2>
<p>The Azure Synapse connector offers efficient and scalable Structured Streaming write support for Azure Synapse that
provides consistent user experience with batch writes, and uses PolyBase or <code class="docutils literal notranslate"><span class="pre">COPY</span></code> for large data transfers
between a Databricks cluster and Azure Synapse instance. Similar to the batch writes, streaming is designed largely
for ETL, thus providing higher latency that may not be suitable for real-time data processing in some cases.</p>
<div class="section" id="fault-tolerance-semantics">
<h3>Fault tolerance semantics<a class="headerlink" href="#fault-tolerance-semantics" title="Permalink to this headline"> </a></h3>
<p>By default, Azure Synapse Streaming offers end-to-end <em>exactly-once</em> guarantee for writing data into an Azure Synapse table by
reliably tracking progress of the query using a combination of checkpoint location in DBFS, checkpoint table in Azure Synapse,
and locking mechanism to ensure that streaming can handle any types of failures, retries, and query restarts.
Optionally, you can select less restrictive at-least-once semantics for Azure Synapse Streaming by setting
<code class="docutils literal notranslate"><span class="pre">spark.databricks.sqldw.streaming.exactlyOnce.enabled</span></code> option to <code class="docutils literal notranslate"><span class="pre">false</span></code>, in which case data duplication
could occur in the event of intermittent connection failures to Azure Synapse or unexpected query termination.</p>
</div>
</div>
<div class="section" id="usage-batch">
<h2>Usage (Batch)<a class="headerlink" href="#usage-batch" title="Permalink to this headline"> </a></h2>
<p>You can use this connector via the data source API in Scala, Python, SQL, and R notebooks.</p>
<div class="js-code-language-tabs js-code-language-tabs--literal compound">
<div class="compound-first highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="c1">// Otherwise, set up the Blob storage account access key in the notebook session conf.</span>
<span class="n">spark</span><span class="p">.</span><span class="n">conf</span><span class="p">.</span><span class="n">set</span><span class="p">(</span>
<span class="w">  </span><span class="s">&quot;fs.azure.account.key.&lt;your-storage-account-name&gt;.dfs.core.windows.net&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="s">&quot;&lt;your-storage-account-access-key&gt;&quot;</span><span class="p">)</span>

<span class="c1">// Get some data from an Azure Synapse table.</span>
<span class="kd">val</span><span class="w"> </span><span class="n">df</span><span class="p">:</span><span class="w"> </span><span class="nc">DataFrame</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">spark</span><span class="p">.</span><span class="n">read</span>
<span class="w">  </span><span class="p">.</span><span class="n">format</span><span class="p">(</span><span class="s">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;url&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;tempDir&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;forwardSparkAzureStorageCredentials&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;true&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;dbTable&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;your-table-name&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">load</span><span class="p">()</span>

<span class="c1">// Load data from an Azure Synapse query.</span>
<span class="kd">val</span><span class="w"> </span><span class="n">df</span><span class="p">:</span><span class="w"> </span><span class="nc">DataFrame</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">spark</span><span class="p">.</span><span class="n">read</span>
<span class="w">  </span><span class="p">.</span><span class="n">format</span><span class="p">(</span><span class="s">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;url&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;tempDir&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;forwardSparkAzureStorageCredentials&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;true&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;query&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;select x, count(*) as cnt from table group by x&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">load</span><span class="p">()</span>

<span class="c1">// Apply some transformations to the data, then use the</span>
<span class="c1">// Data Source API to write the data back to another table in Azure Synapse.</span>
<span class="n">df</span><span class="p">.</span><span class="n">write</span>
<span class="w">  </span><span class="p">.</span><span class="n">format</span><span class="p">(</span><span class="s">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;url&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;forwardSparkAzureStorageCredentials&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;true&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;dbTable&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;your-table-name&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;tempDir&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">save</span><span class="p">()</span>
</pre></div>
</div>
<div class="compound-middle highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="c1"># Otherwise, set up the Blob storage account access key in the notebook session conf.</span>
<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
  <span class="s2">&quot;fs.azure.account.key.&lt;your-storage-account-name&gt;.dfs.core.windows.net&quot;</span><span class="p">,</span>
  <span class="s2">&quot;&lt;your-storage-account-access-key&gt;&quot;</span><span class="p">)</span>

<span class="c1"># Get some data from an Azure Synapse table.</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span> \
  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;url&quot;</span><span class="p">,</span> <span class="s2">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;tempDir&quot;</span><span class="p">,</span> <span class="s2">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;forwardSparkAzureStorageCredentials&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;dbTable&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;your-table-name&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">load</span><span class="p">()</span>

<span class="c1"># Load data from an Azure Synapse query.</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span> \
  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;url&quot;</span><span class="p">,</span> <span class="s2">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;tempDir&quot;</span><span class="p">,</span> <span class="s2">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;forwardSparkAzureStorageCredentials&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;query&quot;</span><span class="p">,</span> <span class="s2">&quot;select x, count(*) as cnt from table group by x&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">load</span><span class="p">()</span>

<span class="c1"># Apply some transformations to the data, then use the</span>
<span class="c1"># Data Source API to write the data back to another table in Azure Synapse.</span>
<span class="n">df</span><span class="o">.</span><span class="n">write</span> \
  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;url&quot;</span><span class="p">,</span> <span class="s2">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;forwardSparkAzureStorageCredentials&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;dbTable&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;your-table-name&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;tempDir&quot;</span><span class="p">,</span> <span class="s2">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">save</span><span class="p">()</span>
</pre></div>
</div>
<div class="compound-middle highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="c1">-- Otherwise, set up the Blob storage account access key in the notebook session conf.</span>
<span class="k">SET</span><span class="w"> </span><span class="n">fs</span><span class="p">.</span><span class="n">azure</span><span class="p">.</span><span class="n">account</span><span class="p">.</span><span class="k">key</span><span class="p">.</span><span class="o">&lt;</span><span class="n">your</span><span class="o">-</span><span class="k">storage</span><span class="o">-</span><span class="n">account</span><span class="o">-</span><span class="n">name</span><span class="o">&gt;</span><span class="p">.</span><span class="n">dfs</span><span class="p">.</span><span class="n">core</span><span class="p">.</span><span class="n">windows</span><span class="p">.</span><span class="n">net</span><span class="o">=&lt;</span><span class="n">your</span><span class="o">-</span><span class="k">storage</span><span class="o">-</span><span class="n">account</span><span class="o">-</span><span class="k">access</span><span class="o">-</span><span class="k">key</span><span class="o">&gt;</span><span class="p">;</span>

<span class="c1">-- Read data using SQL.</span>
<span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">example_table_in_spark_read</span>
<span class="k">USING</span><span class="w"> </span><span class="n">com</span><span class="p">.</span><span class="n">databricks</span><span class="p">.</span><span class="n">spark</span><span class="p">.</span><span class="n">sqldw</span>
<span class="k">OPTIONS</span><span class="w"> </span><span class="p">(</span>
<span class="w">  </span><span class="n">url</span><span class="w"> </span><span class="s1">&#39;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&#39;</span><span class="p">,</span>
<span class="w">  </span><span class="n">forwardSparkAzureStorageCredentials</span><span class="w"> </span><span class="s1">&#39;true&#39;</span><span class="p">,</span>
<span class="w">  </span><span class="n">dbTable</span><span class="w"> </span><span class="s1">&#39;&lt;your-table-name&gt;&#39;</span><span class="p">,</span>
<span class="w">  </span><span class="n">tempDir</span><span class="w"> </span><span class="s1">&#39;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&#39;</span>
<span class="p">);</span>

<span class="c1">-- Write data using SQL.</span>
<span class="c1">-- Create a new table, throwing an error if a table with the same name already exists:</span>
<span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">example_table_in_spark_write</span>
<span class="k">USING</span><span class="w"> </span><span class="n">com</span><span class="p">.</span><span class="n">databricks</span><span class="p">.</span><span class="n">spark</span><span class="p">.</span><span class="n">sqldw</span>
<span class="k">OPTIONS</span><span class="w"> </span><span class="p">(</span>
<span class="w">  </span><span class="n">url</span><span class="w"> </span><span class="s1">&#39;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&#39;</span><span class="p">,</span>
<span class="w">  </span><span class="n">forwardSparkAzureStorageCredentials</span><span class="w"> </span><span class="s1">&#39;true&#39;</span><span class="p">,</span>
<span class="w">  </span><span class="n">dbTable</span><span class="w"> </span><span class="s1">&#39;&lt;your-table-name&gt;&#39;</span><span class="p">,</span>
<span class="w">  </span><span class="n">tempDir</span><span class="w"> </span><span class="s1">&#39;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&#39;</span>
<span class="p">)</span>
<span class="k">AS</span><span class="w"> </span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">table_to_save_in_spark</span><span class="p">;</span>
</pre></div>
</div>
<div class="compound-last highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load SparkR</span>
<span class="nf">library</span><span class="p">(</span><span class="n">SparkR</span><span class="p">)</span>

<span class="c1"># Otherwise, set up the Blob storage account access key in the notebook session conf.</span>
<span class="n">conf</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="nf">sparkR.session</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;conf&quot;</span><span class="p">)</span>
<span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;set&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;fs.azure.account.key.&lt;your-storage-account-name&gt;.dfs.core.windows.net&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;your-storage-account-access-key&gt;&quot;</span><span class="p">)</span>

<span class="c1"># Get some data from an Azure Synapse table.</span>
<span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">read.df</span><span class="p">(</span>
<span class="w">   </span><span class="n">source</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">url</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">forward_spark_azure_storage_credentials</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;true&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">dbTable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&lt;your-table-name&gt;&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">tempDir</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span>

<span class="c1"># Load data from an Azure Synapse query.</span>
<span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">read.df</span><span class="p">(</span>
<span class="w">   </span><span class="n">source</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">url</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">forward_spark_azure_storage_credentials</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;true&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">query</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;select x, count(*) as cnt from table group by x&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">tempDir</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span>

<span class="c1"># Apply some transformations to the data, then use the</span>
<span class="c1"># Data Source API to write the data back to another table in Azure Synapse.</span>
<span class="nf">write.df</span><span class="p">(</span>
<span class="w">  </span><span class="n">df</span><span class="p">,</span>
<span class="w">  </span><span class="n">source</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="n">url</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="n">forward_spark_azure_storage_credentials</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;true&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="n">dbTable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&lt;your-table-name&gt;&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="n">tempDir</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="usage-streaming">
<h2>Usage (Streaming)<a class="headerlink" href="#usage-streaming" title="Permalink to this headline"> </a></h2>
<p>You can write data using Structured Streaming in Scala and Python notebooks.</p>
<div class="js-code-language-tabs js-code-language-tabs--literal compound">
<div class="compound-first highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="c1">// Set up the Blob storage account access key in the notebook session conf.</span>
<span class="n">spark</span><span class="p">.</span><span class="n">conf</span><span class="p">.</span><span class="n">set</span><span class="p">(</span>
<span class="w">  </span><span class="s">&quot;fs.azure.account.key.&lt;your-storage-account-name&gt;.dfs.core.windows.net&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="s">&quot;&lt;your-storage-account-access-key&gt;&quot;</span><span class="p">)</span>

<span class="c1">// Prepare streaming source; this could be Kafka or a simple rate stream.</span>
<span class="kd">val</span><span class="w"> </span><span class="n">df</span><span class="p">:</span><span class="w"> </span><span class="nc">DataFrame</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">spark</span><span class="p">.</span><span class="n">readStream</span>
<span class="w">  </span><span class="p">.</span><span class="n">format</span><span class="p">(</span><span class="s">&quot;rate&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;rowsPerSecond&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;100000&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;numPartitions&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;16&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">load</span><span class="p">()</span>

<span class="c1">// Apply some transformations to the data then use</span>
<span class="c1">// Structured Streaming API to continuously write the data to a table in Azure Synapse.</span>
<span class="n">df</span><span class="p">.</span><span class="n">writeStream</span>
<span class="w">  </span><span class="p">.</span><span class="n">format</span><span class="p">(</span><span class="s">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;url&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;tempDir&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;forwardSparkAzureStorageCredentials&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;true&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;dbTable&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;your-table-name&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;checkpointLocation&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;/tmp_checkpoint_location&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">start</span><span class="p">()</span>
</pre></div>
</div>
<div class="compound-last highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set up the Blob storage account access key in the notebook session conf.</span>
<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
  <span class="s2">&quot;fs.azure.account.key.&lt;your-storage-account-name&gt;.dfs.core.windows.net&quot;</span><span class="p">,</span>
  <span class="s2">&quot;&lt;your-storage-account-access-key&gt;&quot;</span><span class="p">)</span>

<span class="c1"># Prepare streaming source; this could be Kafka or a simple rate stream.</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span> \
  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;rate&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;rowsPerSecond&quot;</span><span class="p">,</span> <span class="s2">&quot;100000&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;numPartitions&quot;</span><span class="p">,</span> <span class="s2">&quot;16&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">load</span><span class="p">()</span>

<span class="c1"># Apply some transformations to the data then use</span>
<span class="c1"># Structured Streaming API to continuously write the data to a table in Azure Synapse.</span>
<span class="n">df</span><span class="o">.</span><span class="n">writeStream</span> \
  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;url&quot;</span><span class="p">,</span> <span class="s2">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;tempDir&quot;</span><span class="p">,</span> <span class="s2">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;forwardSparkAzureStorageCredentials&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;dbTable&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;your-table-name&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;checkpointLocation&quot;</span><span class="p">,</span> <span class="s2">&quot;/tmp_checkpoint_location&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">start</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="configuration">
<h2>Configuration<a class="headerlink" href="#configuration" title="Permalink to this headline"> </a></h2>
<p>This section describes how to configure write semantics for the connector, required permissions, and miscellaneous configuration parameters.</p>
<div class="contents local topic" id="in-this-section">
<p class="topic-title first">In this section:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#supported-save-modes-for-batch-writes" id="id1">Supported save modes for batch writes</a></p></li>
<li><p><a class="reference internal" href="#supported-output-modes-for-streaming-writes" id="id2">Supported output modes for streaming writes</a></p></li>
<li><p><a class="reference internal" href="#write-semantics" id="id3">Write semantics</a></p></li>
<li><p><a class="reference internal" href="#required-azure-synapse-permissions-for-polybase" id="id4">Required Azure Synapse permissions for PolyBase</a></p></li>
<li><p><a class="reference internal" href="#required-azure-synapse-permissions-for-the-copy-statement" id="id5">Required Azure Synapse permissions for the <code class="docutils literal notranslate"><span class="pre">COPY</span></code> statement</a></p></li>
<li><p><a class="reference internal" href="#parameters" id="id6">Parameters</a></p></li>
<li><p><a class="reference internal" href="#query-pushdown-into-azure-synapse" id="id7">Query pushdown into Azure Synapse</a></p></li>
<li><p><a class="reference internal" href="#temporary-data-management" id="id8">Temporary data management</a></p></li>
<li><p><a class="reference internal" href="#temporary-object-management" id="id9">Temporary object management</a></p></li>
<li><p><a class="reference internal" href="#streaming-checkpoint-table-management" id="id10">Streaming checkpoint table management</a></p></li>
</ul>
</div>
<div class="section" id="supported-save-modes-for-batch-writes">
<h3><a class="toc-backref" href="#id1">Supported save modes for batch writes</a><a class="headerlink" href="#supported-save-modes-for-batch-writes" title="Permalink to this headline"> </a></h3>
<p>The Azure Synapse connector supports <code class="docutils literal notranslate"><span class="pre">ErrorIfExists</span></code>, <code class="docutils literal notranslate"><span class="pre">Ignore</span></code>, <code class="docutils literal notranslate"><span class="pre">Append</span></code>, and <code class="docutils literal notranslate"><span class="pre">Overwrite</span></code> save modes with the default mode being <code class="docutils literal notranslate"><span class="pre">ErrorIfExists</span></code>.
For more information on supported save modes in Apache Spark,
see <a class="reference external" href="https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes">Spark SQL documentation on Save Modes</a>.</p>
</div>
<div class="section" id="supported-output-modes-for-streaming-writes">
<h3><a class="toc-backref" href="#id2">Supported output modes for streaming writes</a><a class="headerlink" href="#supported-output-modes-for-streaming-writes" title="Permalink to this headline"> </a></h3>
<p>The Azure Synapse connector supports <code class="docutils literal notranslate"><span class="pre">Append</span></code> and <code class="docutils literal notranslate"><span class="pre">Complete</span></code> output modes for record appends and aggregations.
For more details on output modes and compatibility matrix, see the
<a class="reference external" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes">Structured Streaming guide</a>.</p>
</div>
<div class="section" id="write-semantics">
<h3><a class="toc-backref" href="#id3">Write semantics</a><a class="headerlink" href="#write-semantics" title="Permalink to this headline"> </a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">COPY</span></code> is available in Databricks Runtime 7.0 and above.</p>
</div>
<p>In addition to PolyBase, the Azure Synapse connector supports the <code class="docutils literal notranslate"><span class="pre">COPY</span></code>  statement. The <code class="docutils literal notranslate"><span class="pre">COPY</span></code>
statement offers a more convenient way of loading data into Azure Synapse without the need to
create an external table, requires fewer permissions to load data, and improves the performance of
data ingestion into Azure Synapse.</p>
<p>By default, the connector automatically discovers the best write semantics (<code class="docutils literal notranslate"><span class="pre">COPY</span></code> when targeting an
Azure Synapse Gen2 instance, PolyBase otherwise). You can also specify the write semantics with the
following configuration:</p>
<div class="js-code-language-tabs js-code-language-tabs--literal compound">
<div class="compound-first highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="c1">// Configure the write semantics for Azure Synapse connector in the notebook session conf.</span>
<span class="n">spark</span><span class="p">.</span><span class="n">conf</span><span class="p">.</span><span class="n">set</span><span class="p">(</span><span class="s">&quot;spark.databricks.sqldw.writeSemantics&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;write-semantics&gt;&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="compound-middle highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Configure the write semantics for Azure Synapse connector in the notebook session conf.</span>
<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.databricks.sqldw.writeSemantics&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;write-semantics&gt;&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="compound-middle highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="c1">-- Configure the write semantics for Azure Synapse connector in the notebook session conf.</span>
<span class="k">SET</span><span class="w"> </span><span class="n">spark</span><span class="p">.</span><span class="n">databricks</span><span class="p">.</span><span class="n">sqldw</span><span class="p">.</span><span class="n">writeSemantics</span><span class="o">=&lt;</span><span class="k">write</span><span class="o">-</span><span class="n">semantics</span><span class="o">&gt;</span><span class="p">;</span>
</pre></div>
</div>
<div class="compound-last highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load SparkR</span>
<span class="nf">library</span><span class="p">(</span><span class="n">SparkR</span><span class="p">)</span>

<span class="c1"># Configure the write semantics for Azure Synapse connector in the notebook session conf.</span>
<span class="n">conf</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="nf">sparkR.session</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;conf&quot;</span><span class="p">)</span>
<span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;set&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;spark.databricks.sqldw.writeSemantics&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;write-semantics&gt;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">&lt;write-semantics&gt;</span></code> is either <code class="docutils literal notranslate"><span class="pre">polybase</span></code> to use PolyBase, or <code class="docutils literal notranslate"><span class="pre">copy</span></code> to use the <code class="docutils literal notranslate"><span class="pre">COPY</span></code> statement.</p>
</div>
<div class="section" id="required-azure-synapse-permissions-for-polybase">
<span id="dw-polybase-permissions"></span><h3><a class="toc-backref" href="#id4">Required Azure Synapse permissions for PolyBase</a><a class="headerlink" href="#required-azure-synapse-permissions-for-polybase" title="Permalink to this headline"> </a></h3>
<p>When you use PolyBase, the Azure Synapse connector requires the JDBC connection user to have permission to run the following commands
in the connected Azure Synapse instance:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://learn.microsoft.com/sql/t-sql/statements/create-database-scoped-credential-transact-sql">CREATE DATABASE SCOPED CREDENTIAL</a></p></li>
<li><p><a class="reference external" href="https://learn.microsoft.com/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#create-external-data-source">CREATE EXTERNAL DATA SOURCE</a></p></li>
<li><p><a class="reference external" href="https://learn.microsoft.com/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#create-external-file-format">CREATE EXTERNAL FILE FORMAT</a></p></li>
<li><p><a class="reference external" href="https://learn.microsoft.com/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#create-external-table">CREATE EXTERNAL TABLE</a></p></li>
</ul>
<p>As a prerequisite for the first command, the connector expects that a database master key already exists for the specified Azure Synapse instance. If not, you can create a key using the <a class="reference external" href="https://learn.microsoft.com/sql/t-sql/statements/create-master-key-transact-sql">CREATE MASTER KEY</a> command.</p>
<p>Additionally, to read the Azure Synapse table set through <code class="docutils literal notranslate"><span class="pre">dbTable</span></code> or tables referred in <code class="docutils literal notranslate"><span class="pre">query</span></code>, the JDBC user must have permission to access needed Azure Synapse tables. To write data back to an Azure Synapse table set through <code class="docutils literal notranslate"><span class="pre">dbTable</span></code>, the JDBC user must have permission to write to this Azure Synapse table.</p>
<p>The following table summarizes the required permissions for all operations with PolyBase:</p>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 27%" />
<col style="width: 36%" />
<col style="width: 36%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Operation</p></th>
<th class="head"><p>Permissions</p></th>
<th class="head"><p>Permissions when using <a class="reference internal" href="#dw-polybase-external-data-source-permissions"><span class="std std-ref">external data source</span></a></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Batch write</p></td>
<td><p>CONTROL</p></td>
<td><p>See <a class="reference internal" href="#dw-polybase-external-data-source-permissions-table-writes"><span class="std std-ref">Batch write</span></a></p></td>
</tr>
<tr class="row-odd"><td><p>Streaming write</p></td>
<td><p>CONTROL</p></td>
<td><p>See <a class="reference internal" href="#dw-polybase-external-data-source-permissions-table-writes"><span class="std std-ref">Streaming write</span></a></p></td>
</tr>
<tr class="row-even"><td><p>Read</p></td>
<td><p>CONTROL</p></td>
<td><p>See <a class="reference internal" href="#dw-polybase-external-data-source-permissions-table-reads"><span class="std std-ref">Read</span></a></p></td>
</tr>
</tbody>
</table>
<div class="section" id="required-azure-synapse-permissions-for-polybase-with-the-external-data-source-option">
<span id="dw-polybase-external-data-source-permissions"></span><h4>Required Azure Synapse permissions for PolyBase with the external data source option<a class="headerlink" href="#required-azure-synapse-permissions-for-polybase-with-the-external-data-source-option" title="Permalink to this headline"> </a></h4>
<p>You can use PolyBase with a pre-provisioned external data source. See the <code class="docutils literal notranslate"><span class="pre">externalDataSource</span></code> parameter in <a class="reference internal" href="#parameters"><span class="std std-ref">Parameters</span></a> for more information.</p>
<p>To use PolyBase with a pre-provisioned external data source, the Azure Synapse connector requires the JDBC connection user to have permission to run the following commands in the connected Azure Synapse instance:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://learn.microsoft.com/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#create-external-file-format">CREATE EXTERNAL FILE FORMAT</a></p></li>
<li><p><a class="reference external" href="https://learn.microsoft.com/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#create-external-table">CREATE EXTERNAL TABLE</a></p></li>
</ul>
<p>To create an external data source, you should first create a database scoped credential. The following links describe how to create a scoped credential for service principals and an external data source for an ABFS location:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://learn.microsoft.com/sql/t-sql/statements/create-database-scoped-credential-transact-sql?view=sql-server-ver15#c-creating-a-database-scoped-credential-for-polybase-connectivity-to-azure-data-lake-store">CREATE DATABASE SCOPED CREDENTIAL</a></p></li>
<li><p><a class="reference external" href="https://learn.microsoft.com/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#create-external-data-source&amp;tabs=dedicated#h-create-external-data-source-to-access-data-in-azure-storage-using-the-abfs-interface">CREATE EXTERNAL DATA SOURCE</a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The external data source location must point to a container. The connector will not work if the location is a directory in a container.</p>
</div>
<p>The following table summarizes the permissions for PolyBase write operations with the external data source option:</p>
<table class="colwidths-given docutils align-default" id="dw-polybase-external-data-source-permissions-table-writes">
<colgroup>
<col style="width: 27%" />
<col style="width: 36%" />
<col style="width: 36%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Operation</p></th>
<th class="head"><p>Permissions (insert into an existing table)</p></th>
<th class="head"><p>Permissions (insert into a new table)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Batch write</p></td>
<td><p>ADMINISTER DATABASE BULK OPERATIONS</p>
<p>INSERT</p>
<p>CREATE TABLE</p>
<p>ALTER ANY SCHEMA</p>
<p>ALTER ANY EXTERNAL DATA SOURCE</p>
<p>ALTER ANY EXTERNAL FILE FORMAT</p>
</td>
<td><p>ADMINISTER DATABASE BULK OPERATIONS</p>
<p>INSERT</p>
<p>CREATE TABLE</p>
<p>ALTER ANY SCHEMA</p>
<p>ALTER ANY EXTERNAL DATA SOURCE</p>
<p>ALTER ANY EXTERNAL FILE FORMAT</p>
</td>
</tr>
<tr class="row-odd"><td><p>Streaming write</p></td>
<td><p>ADMINISTER DATABASE BULK OPERATIONS</p>
<p>INSERT</p>
<p>CREATE TABLE</p>
<p>ALTER ANY SCHEMA</p>
<p>ALTER ANY EXTERNAL DATA SOURCE</p>
<p>ALTER ANY EXTERNAL FILE FORMAT</p>
</td>
<td><p>ADMINISTER DATABASE BULK OPERATIONS</p>
<p>INSERT</p>
<p>CREATE TABLE</p>
<p>ALTER ANY SCHEMA</p>
<p>ALTER ANY EXTERNAL DATA SOURCE</p>
<p>ALTER ANY EXTERNAL FILE FORMAT</p>
</td>
</tr>
</tbody>
</table>
<p>The following table summarizes the permissions for PolyBase read operations with external data source option:</p>
<table class="colwidths-given docutils align-default" id="dw-polybase-external-data-source-permissions-table-reads">
<colgroup>
<col style="width: 43%" />
<col style="width: 57%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Operation</p></th>
<th class="head"><p>Permissions</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Read</p></td>
<td><p>CREATE TABLE</p>
<p>ALTER ANY SCHEMA</p>
<p>ALTER ANY EXTERNAL DATA SOURCE</p>
<p>ALTER ANY EXTERNAL FILE FORMAT</p>
</td>
</tr>
</tbody>
</table>
<p>You can use this connector to read via the data source API in Scala, Python, SQL, and R notebooks.</p>
<div class="js-code-language-tabs js-code-language-tabs--literal compound">
<div class="compound-first highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="c1">// Get some data from an Azure Synapse table.</span>
<span class="kd">val</span><span class="w"> </span><span class="n">df</span><span class="p">:</span><span class="w"> </span><span class="nc">DataFrame</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">spark</span><span class="p">.</span><span class="n">read</span>
<span class="w">  </span><span class="p">.</span><span class="n">format</span><span class="p">(</span><span class="s">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;url&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;tempDir&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;externalDataSource&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;your-pre-provisioned-data-source&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;dbTable&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;your-table-name&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">load</span><span class="p">()</span>
</pre></div>
</div>
<div class="compound-middle highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get some data from an Azure Synapse table.</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span> \
  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;url&quot;</span><span class="p">,</span> <span class="s2">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;tempDir&quot;</span><span class="p">,</span> <span class="s2">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;externalDataSource&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;your-pre-provisioned-data-source&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;dbTable&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;your-table-name&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">load</span><span class="p">()</span>
</pre></div>
</div>
<div class="compound-middle highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="c1">-- Read data using SQL.</span>
<span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">example_table_in_spark_read</span>
<span class="k">USING</span><span class="w"> </span><span class="n">com</span><span class="p">.</span><span class="n">databricks</span><span class="p">.</span><span class="n">spark</span><span class="p">.</span><span class="n">sqldw</span>
<span class="k">OPTIONS</span><span class="w"> </span><span class="p">(</span>
<span class="w">  </span><span class="n">url</span><span class="w"> </span><span class="s1">&#39;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&#39;</span><span class="p">,</span>
<span class="w">  </span><span class="n">forwardSparkAzureStorageCredentials</span><span class="w"> </span><span class="s1">&#39;true&#39;</span><span class="p">,</span>
<span class="w">  </span><span class="n">dbTable</span><span class="w"> </span><span class="s1">&#39;&lt;your-table-name&gt;&#39;</span><span class="p">,</span>
<span class="w">  </span><span class="n">tempDir</span><span class="w"> </span><span class="s1">&#39;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&#39;</span><span class="p">,</span>
<span class="w">  </span><span class="n">externalDataSource</span><span class="w"> </span><span class="s1">&#39;&lt;your-pre-provisioned-data-source&gt;&#39;</span>
<span class="p">);</span>
</pre></div>
</div>
<div class="compound-last highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get some data from an Azure Synapse table.</span>
<span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">read.df</span><span class="p">(</span>
<span class="w">   </span><span class="n">source</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">url</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">forward_spark_azure_storage_credentials</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;true&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">dbTable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&lt;your-table-name&gt;&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">tempDir</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span>
<span class="w">   </span><span class="n">externalDataSource</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&lt;your-pre-provisioned-data-source&gt;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="required-azure-synapse-permissions-for-the-copy-statement">
<span id="dw-copy-permissions"></span><h3><a class="toc-backref" href="#id5">Required Azure Synapse permissions for the <code class="docutils literal notranslate"><span class="pre">COPY</span></code> statement</a><a class="headerlink" href="#required-azure-synapse-permissions-for-the-copy-statement" title="Permalink to this headline"> </a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Available in Databricks Runtime 7.0 and above.</p>
</div>
<p>When you use the <code class="docutils literal notranslate"><span class="pre">COPY</span></code> statement, the Azure Synapse connector requires the JDBC connection user to have permission
to run the following commands in the connected Azure Synapse instance:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://learn.microsoft.com/sql/t-sql/statements/copy-into-transact-sql">COPY INTO</a></p></li>
</ul>
<p>If the destination table does not exist in Azure Synapse, permission to run the following command is required in addition to the command above:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://learn.microsoft.com/sql/t-sql/statements/create-table-azure-sql-data-warehouse">CREATE TABLE</a></p></li>
</ul>
<p>The following table summarizes the permissions for batch and streaming writes with <code class="docutils literal notranslate"><span class="pre">COPY</span></code>:</p>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 27%" />
<col style="width: 36%" />
<col style="width: 36%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Operation</p></th>
<th class="head"><p>Permissions (insert into an existing table)</p></th>
<th class="head"><p>Permissions (insert into a new table)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Batch write</p></td>
<td><p>ADMINISTER DATABASE BULK OPERATIONS</p>
<p>INSERT</p>
</td>
<td><p>ADMINISTER DATABASE BULK OPERATIONS</p>
<p>INSERT</p>
<p>CREATE TABLE</p>
<p>ALTER ON SCHEMA :: dbo</p>
</td>
</tr>
<tr class="row-odd"><td><p>Streaming write</p></td>
<td><p>ADMINISTER DATABASE BULK OPERATIONS</p>
<p>INSERT</p>
</td>
<td><p>ADMINISTER DATABASE BULK OPERATIONS</p>
<p>INSERT</p>
<p>CREATE TABLE</p>
<p>ALTER ON SCHEMA :: dbo</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="parameters">
<h3><a class="toc-backref" href="#id6">Parameters</a><a class="headerlink" href="#parameters" title="Permalink to this headline"> </a></h3>
<p>
The parameter map or <code class="docutils literal notranslate"><span class="pre">OPTIONS</span></code> provided in Spark SQL support the following settings:</p>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 33%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 44%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Required</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">dbTable</span></code></p></td>
<td><p>Yes, unless <code class="docutils literal notranslate"><span class="pre">query</span></code> is specified</p></td>
<td><p>No default</p></td>
<td><p>The table to create or read from in Azure Synapse. This parameter is required
when saving data back to Azure Synapse.</p>
<p>You can also use <code class="docutils literal notranslate"><span class="pre">{SCHEMA</span> <span class="pre">NAME}.{TABLE</span> <span class="pre">NAME}</span></code> to access a table in a given
schema. If schema name is not provided, the default schema associated with the
JDBC user is used.</p>
<p>The previously supported <code class="docutils literal notranslate"><span class="pre">dbtable</span></code> variant is deprecated and will be ignored in future releases.
Use the “camel case” name instead.</p>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">query</span></code></p></td>
<td><p>Yes, unless <code class="docutils literal notranslate"><span class="pre">dbTable</span></code> is specified</p></td>
<td><p>No default</p></td>
<td><p>The query to read from in Azure Synapse.</p>
<p>For tables referred in the query, you can also use <code class="docutils literal notranslate"><span class="pre">{SCHEMA</span> <span class="pre">NAME}.{TABLE</span> <span class="pre">NAME}</span></code>
to access a table in a given schema.
If schema name is not provided, the default schema associated with the
JDBC user is used.</p>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">user</span></code></p></td>
<td><p>No</p></td>
<td><p>No default</p></td>
<td><p>The Azure Synapse username. Must be used in tandem with <code class="docutils literal notranslate"><span class="pre">password</span></code> option.
Can only be used if the user and password are not passed in the URL.
Passing both will result in an error.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">password</span></code></p></td>
<td><p>No</p></td>
<td><p>No default</p></td>
<td><p>The Azure Synapse password. Must be used in tandem with <code class="docutils literal notranslate"><span class="pre">user</span></code> option.
Can only be used if the user and password are not passed in the URL.
Passing both will result in an error.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">url</span></code></p></td>
<td><p>Yes</p></td>
<td><p>No default</p></td>
<td><p>A JDBC URL with <code class="docutils literal notranslate"><span class="pre">sqlserver</span></code> set as the subprotocol. It is recommended
to use the connection string provided by Azure portal. Setting
<code class="docutils literal notranslate"><span class="pre">encrypt=true</span></code> is strongly recommended, because it enables SSL
encryption of the JDBC connection. If <code class="docutils literal notranslate"><span class="pre">user</span></code> and <code class="docutils literal notranslate"><span class="pre">password</span></code>
are set separately, you do not need to include them in the URL.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">jdbcDriver</span></code></p></td>
<td><p>No</p></td>
<td><p>Determined by the JDBC URL’s subprotocol</p></td>
<td><p>The class name of the JDBC driver to use. This class must be on the classpath. In most cases,
it should not be necessary to specify this option, as the appropriate driver classname should
automatically be determined by the JDBC URL’s subprotocol.</p>
<p>The previously supported <code class="docutils literal notranslate"><span class="pre">jdbc_driver</span></code> variant is deprecated and will be ignored in future releases.
Use the “camel case” name instead.</p>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">tempDir</span></code></p></td>
<td><p>Yes</p></td>
<td><p>No default</p></td>
<td><p>A <code class="docutils literal notranslate"><span class="pre">abfss</span></code> URI. We recommend you use a dedicated Blob storage container for the Azure Synapse.</p>
<p>The previously supported <code class="docutils literal notranslate"><span class="pre">tempdir</span></code> variant is deprecated and will be ignored in future releases.
Use the “camel case” name instead.</p>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">tempFormat</span></code></p></td>
<td><p>No</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">PARQUET</span></code></p></td>
<td><p>The format in which to save temporary files to the blob store when writing to Azure Synapse. Defaults to <code class="docutils literal notranslate"><span class="pre">PARQUET</span></code>; no other values are allowed right now.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">tempCompression</span></code></p></td>
<td><p>No</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">SNAPPY</span></code></p></td>
<td><p>The compression algorithm to be used to encode/decode temporary by both Spark and Azure Synapse. Currently supported values are: <code class="docutils literal notranslate"><span class="pre">UNCOMPRESSED</span></code>, <code class="docutils literal notranslate"><span class="pre">SNAPPY</span></code> and <code class="docutils literal notranslate"><span class="pre">GZIP</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">forwardSparkAzureStorageCredentials</span></code></p></td>
<td><p>No</p></td>
<td><p>false</p></td>
<td><p>If <code class="docutils literal notranslate"><span class="pre">true</span></code>, the library automatically discovers the credentials that Spark is using
to connect to the Blob storage container and forwards those credentials to Azure Synapse over JDBC. These credentials
are sent as part of the JDBC query. Therefore it is strongly recommended that you enable SSL
encryption of the JDBC connection when you use this option.</p>
<p>The current version of Azure Synapse connector requires (exactly) one of <code class="docutils literal notranslate"><span class="pre">forwardSparkAzureStorageCredentials</span></code>, <code class="docutils literal notranslate"><span class="pre">enableServicePrincipalAuth</span></code>,
or <code class="docutils literal notranslate"><span class="pre">useAzureMSI</span></code> to be explicitly set to <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p>
<p>The previously supported <code class="docutils literal notranslate"><span class="pre">forward_spark_azure_storage_credentials</span></code> variant is deprecated and will be ignored in future releases.
Use the “camel case” name instead.</p>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">useAzureMSI</span></code></p></td>
<td><p>No</p></td>
<td><p>false</p></td>
<td><p>If <code class="docutils literal notranslate"><span class="pre">true</span></code>, the library will specify <code class="docutils literal notranslate"><span class="pre">IDENTITY</span> <span class="pre">=</span> <span class="pre">'Managed</span> <span class="pre">Service</span> <span class="pre">Identity'</span></code> and no <code class="docutils literal notranslate"><span class="pre">SECRET</span></code>
for the database scoped credentials it creates.</p>
<p>The current version of Azure Synapse connector requires (exactly) one of <code class="docutils literal notranslate"><span class="pre">forwardSparkAzureStorageCredentials</span></code>, <code class="docutils literal notranslate"><span class="pre">enableServicePrincipalAuth</span></code>,
or <code class="docutils literal notranslate"><span class="pre">useAzureMSI</span></code> to be explicitly set to <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">enableServicePrincipalAuth</span></code></p></td>
<td><p>No</p></td>
<td><p>false</p></td>
<td><p>If <code class="docutils literal notranslate"><span class="pre">true</span></code>, the library will use the provided service principal credentials to connect to the Azure storage account and Azure Synapse Analytics over JDBC.</p>
<p>The current version of Azure Synapse connector requires (exactly) one of <code class="docutils literal notranslate"><span class="pre">forwardSparkAzureStorageCredentials</span></code>, <code class="docutils literal notranslate"><span class="pre">enableServicePrincipalAuth</span></code>,
or <code class="docutils literal notranslate"><span class="pre">useAzureMSI</span></code> to be explicitly set to <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">tableOptions</span></code></p></td>
<td><p>No</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CLUSTERED</span> <span class="pre">COLUMNSTORE</span> <span class="pre">INDEX</span></code>, <code class="docutils literal notranslate"><span class="pre">DISTRIBUTION</span> <span class="pre">=</span> <span class="pre">ROUND_ROBIN</span></code></p></td>
<td><p>A string used to specify <a class="reference external" href="https://learn.microsoft.com/sql/t-sql/statements/create-table-azure-sql-data-warehouse">table options</a>
when creating the Azure Synapse table set through <code class="docutils literal notranslate"><span class="pre">dbTable</span></code>.
This string is passed literally to the <code class="docutils literal notranslate"><span class="pre">WITH</span></code> clause of the <code class="docutils literal notranslate"><span class="pre">CREATE</span> <span class="pre">TABLE</span></code> SQL statement
that is issued against Azure Synapse.</p>
<p>The previously supported <code class="docutils literal notranslate"><span class="pre">table_options</span></code> variant is deprecated and will be ignored in future releases.
Use the “camel case” name instead.</p>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">preActions</span></code></p></td>
<td><p>No</p></td>
<td><p>No default (empty string)</p></td>
<td><p>A <code class="docutils literal notranslate"><span class="pre">;</span></code> separated list of SQL commands to be executed in Azure Synapse before writing data
to the Azure Synapse instance. These SQL commands are required to be valid commands accepted by Azure Synapse.</p>
<p>If any of these commands fail, it is treated as an error and the write operation
is not executed.</p>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">postActions</span></code></p></td>
<td><p>No</p></td>
<td><p>No default (empty string)</p></td>
<td><p>A <code class="docutils literal notranslate"><span class="pre">;</span></code> separated list of SQL commands to be executed in Azure Synapse
after the connector successfully writes data to the Azure Synapse instance.
These SQL commands are required to be valid commands accepted by Azure Synapse.</p>
<p>If any of these commands fail, it is treated as an error and
you’ll get an exception after the data is successfully written to the Azure Synapse instance.</p>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">maxStrLength</span></code></p></td>
<td><p>No</p></td>
<td><p>256</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">StringType</span></code> in Spark is mapped to the <code class="docutils literal notranslate"><span class="pre">NVARCHAR(maxStrLength)</span></code> type in Azure Synapse. You can use <code class="docutils literal notranslate"><span class="pre">maxStrLength</span></code>
to set the string length for all <code class="docutils literal notranslate"><span class="pre">NVARCHAR(maxStrLength)</span></code> type columns that are in the table with name
<code class="docutils literal notranslate"><span class="pre">dbTable</span></code> in Azure Synapse.</p>
<p>The previously supported <code class="docutils literal notranslate"><span class="pre">maxstrlength</span></code> variant is deprecated and will be ignored in future releases.
Use the “camel case” name instead.</p>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">checkpointLocation</span></code></p></td>
<td><p>Yes</p></td>
<td><p>No default</p></td>
<td><p>Location on DBFS that will be used by Structured Streaming to write metadata and checkpoint information.
See <a class="reference external" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#recovering-from-failures-with-checkpointing">Recovering from Failures with Checkpointing</a>
in Structured Streaming programming guide.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">numStreamingTempDirsToKeep</span></code></p></td>
<td><p>No</p></td>
<td><p>0</p></td>
<td><p>Indicates how many (latest) temporary directories to keep for periodic cleanup of micro batches
in streaming. When set to <code class="docutils literal notranslate"><span class="pre">0</span></code>, directory deletion is triggered immediately after micro batch
is committed, otherwise provided number of latest micro batches is kept and the rest of directories
is removed. Use <code class="docutils literal notranslate"><span class="pre">-1</span></code> to disable periodic cleanup.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">applicationName</span></code></p></td>
<td><p>No</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Databricks-User-Query</span></code></p></td>
<td><p>The tag of the connection for each query. If not specified or the value is an empty string,
the default value of the tag is added the JDBC URL. The default value prevents the Azure DB
Monitoring tool from raising spurious SQL injection alerts against queries.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">maxbinlength</span></code></p></td>
<td><p>No</p></td>
<td><p>No default</p></td>
<td><p>Control the column length of <code class="docutils literal notranslate"><span class="pre">BinaryType</span></code> columns. This parameter is translated as <code class="docutils literal notranslate"><span class="pre">VARBINARY(maxbinlength)</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">identityInsert</span></code></p></td>
<td><p>No</p></td>
<td><p>false</p></td>
<td><p>Setting to <code class="docutils literal notranslate"><span class="pre">true</span></code> enables <code class="docutils literal notranslate"><span class="pre">IDENTITY_INSERT</span></code> mode, which inserts a DataFrame provided value in the identity column of the Azure Synapse table.</p>
<p>See <a class="reference external" href="https://learn.microsoft.com/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-identity#explicitly-inserting-values-into-an-identity-column">Explicitly inserting values into an IDENTITY column</a>.</p>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">externalDataSource</span></code></p></td>
<td><p>No</p></td>
<td><p>No default</p></td>
<td><p>A pre-provisioned external data source to read data from Azure Synapse. An external data source can only be used with PolyBase and removes the CONTROL permission requirement since the connector does not need to create a scoped credential and an external data source to load data.</p>
<p>For example usage and the list of permissions required when using an external data source, see <a class="reference internal" href="#dw-polybase-external-data-source-permissions"><span class="std std-ref">Required Azure Synapse permissions for PolyBase with the external data source option</span></a>.</p>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">maxErrors</span></code></p></td>
<td><p>No</p></td>
<td><p>0</p></td>
<td><p>The maximum number of rows that can be rejected during reads and writes before the loading operation (either PolyBase or COPY) is cancelled. The rejected rows will be ignored. For example, if two out of ten records have errors, only eight records will be processed.</p>
<p>See <a class="reference external" href="https://learn.microsoft.com/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&amp;tabs=dedicated#reject_value--reject_value-1">REJECT_VALUE documentation in CREATE EXTERNAL TABLE</a> and <a class="reference external" href="https://learn.microsoft.com/sql/t-sql/statements/copy-into-transact-sql?view=azure-sqldw-latest#maxerrors--max_errors">MAXERRORS documentation in COPY</a>.</p>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tableOptions</span></code>, <code class="docutils literal notranslate"><span class="pre">preActions</span></code>, <code class="docutils literal notranslate"><span class="pre">postActions</span></code>, and <code class="docutils literal notranslate"><span class="pre">maxStrLength</span></code> are relevant only when writing data from Databricks to a new table in Azure Synapse.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">externalDataSource</span></code> is relevant only when reading data from Azure Synapse and writing data from Databricks to a new table in Azure Synapse with PolyBase semantics. You should not specify other storage authentication types while using <code class="docutils literal notranslate"><span class="pre">externalDataSource</span></code> such as <code class="docutils literal notranslate"><span class="pre">forwardSparkAzureStorageCredentials</span></code> or <code class="docutils literal notranslate"><span class="pre">useAzureMSI</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">checkpointLocation</span></code> and <code class="docutils literal notranslate"><span class="pre">numStreamingTempDirsToKeep</span></code> are relevant only for streaming writes from Databricks to a new table in Azure Synapse.</p></li>
<li><p>Even though all data source option names are case-insensitive, we recommend that you specify them in “camel case” for clarity.</p></li>
</ul>
</div>
<p></p>
</div>
<div class="section" id="query-pushdown-into-azure-synapse">
<span id="sql-dw-query-pushdown"></span><h3><a class="toc-backref" href="#id7">Query pushdown into Azure Synapse</a><a class="headerlink" href="#query-pushdown-into-azure-synapse" title="Permalink to this headline"> </a></h3>
<p>The Azure Synapse connector implements a set of optimization rules
to push the following operators down into Azure Synapse:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Filter</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Project</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Limit</span></code></p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">Project</span></code> and <code class="docutils literal notranslate"><span class="pre">Filter</span></code> operators support the following expressions:</p>
<ul class="simple">
<li><p>Most boolean logic operators</p></li>
<li><p>Comparisons</p></li>
<li><p>Basic arithmetic operations</p></li>
<li><p>Numeric and string casts</p></li>
</ul>
<p>For the <code class="docutils literal notranslate"><span class="pre">Limit</span></code> operator, pushdown is supported only when there is no ordering specified. For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">SELECT</span> <span class="pre">TOP(10)</span> <span class="pre">*</span> <span class="pre">FROM</span> <span class="pre">table</span></code>, but not <code class="docutils literal notranslate"><span class="pre">SELECT</span> <span class="pre">TOP(10)</span> <span class="pre">*</span> <span class="pre">FROM</span> <span class="pre">table</span> <span class="pre">ORDER</span> <span class="pre">BY</span> <span class="pre">col</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Azure Synapse connector does not push down expressions operating on strings, dates, or timestamps.</p>
</div>
<p>Query pushdown built with the Azure Synapse connector is enabled by default.
You can disable it by setting <code class="docutils literal notranslate"><span class="pre">spark.databricks.sqldw.pushdown</span></code> to <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p>
</div>
<div class="section" id="temporary-data-management">
<span id="temp-data-mgmt"></span><h3><a class="toc-backref" href="#id8">Temporary data management</a><a class="headerlink" href="#temporary-data-management" title="Permalink to this headline"> </a></h3>
<p>The Azure Synapse connector <em>does not</em> delete the temporary files that it creates in the Blob storage container.
Therefore we recommend that you periodically delete temporary files under the user-supplied <code class="docutils literal notranslate"><span class="pre">tempDir</span></code> location.</p>
<p>To facilitate data cleanup, the Azure Synapse connector does not store data files directly under <code class="docutils literal notranslate"><span class="pre">tempDir</span></code>,
but instead creates a subdirectory of the form: <code class="docutils literal notranslate"><span class="pre">&lt;tempDir&gt;/&lt;yyyy-MM-dd&gt;/&lt;HH-mm-ss-SSS&gt;/&lt;randomUUID&gt;/</span></code>.
You can set up periodic jobs (using the Databricks <a class="reference internal" href="../../workflows/jobs/create-run-jobs.html"><span class="doc">jobs</span></a> feature or otherwise) to recursively delete any subdirectories that are older than a given threshold (for example, 2 days), with the assumption that there cannot be Spark jobs running longer than that threshold.</p>
<p>A simpler alternative is to periodically drop the whole container and create a new one with the same name.
This requires that you use a dedicated container for the temporary data produced by the Azure Synapse connector and that
you can find a time window in which you can guarantee that no queries involving the connector are running.</p>
<p></p>
<p></p>
<p></p>
<p></p>
</div>
<div class="section" id="temporary-object-management">
<span id="temp-object-mgmt"></span><h3><a class="toc-backref" href="#id9">Temporary object management</a><a class="headerlink" href="#temporary-object-management" title="Permalink to this headline"> </a></h3>
<p>The Azure Synapse connector automates data transfer between a Databricks cluster and an Azure Synapse instance.
For reading data from an Azure Synapse table or query or writing data to an Azure Synapse table,
the Azure Synapse connector creates temporary objects, including <code class="docutils literal notranslate"><span class="pre">DATABASE</span> <span class="pre">SCOPED</span> <span class="pre">CREDENTIAL</span></code>, <code class="docutils literal notranslate"><span class="pre">EXTERNAL</span> <span class="pre">DATA</span> <span class="pre">SOURCE</span></code>, <code class="docutils literal notranslate"><span class="pre">EXTERNAL</span> <span class="pre">FILE</span> <span class="pre">FORMAT</span></code>,
and <code class="docutils literal notranslate"><span class="pre">EXTERNAL</span> <span class="pre">TABLE</span></code> behind the scenes. These objects live
only throughout the duration of the corresponding Spark job and should automatically be dropped thereafter.</p>
<p>When a cluster is running a query using the Azure Synapse connector, if the Spark driver process crashes or is forcefully restarted, or if the cluster
is forcefully terminated or restarted, temporary objects might not be dropped.
To facilitate identification and manual deletion of these objects, the Azure Synapse connector prefixes the names of all intermediate temporary objects created in the Azure Synapse instance with a tag of the form: <code class="docutils literal notranslate"><span class="pre">tmp_databricks_&lt;yyyy_MM_dd_HH_mm_ss_SSS&gt;_&lt;randomUUID&gt;_&lt;internalObject&gt;</span></code>.</p>
<p>We recommend that you periodically look for leaked objects using queries such as the following:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">SELECT</span> <span class="pre">*</span> <span class="pre">FROM</span> <span class="pre">sys.database_scoped_credentials</span> <span class="pre">WHERE</span> <span class="pre">name</span> <span class="pre">LIKE</span> <span class="pre">'tmp_databricks_%'</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SELECT</span> <span class="pre">*</span> <span class="pre">FROM</span> <span class="pre">sys.external_data_sources</span> <span class="pre">WHERE</span> <span class="pre">name</span> <span class="pre">LIKE</span> <span class="pre">'tmp_databricks_%'</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SELECT</span> <span class="pre">*</span> <span class="pre">FROM</span> <span class="pre">sys.external_file_formats</span> <span class="pre">WHERE</span> <span class="pre">name</span> <span class="pre">LIKE</span> <span class="pre">'tmp_databricks_%'</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SELECT</span> <span class="pre">*</span> <span class="pre">FROM</span> <span class="pre">sys.external_tables</span> <span class="pre">WHERE</span> <span class="pre">name</span> <span class="pre">LIKE</span> <span class="pre">'tmp_databricks_%'</span></code></p></li>
</ul>
</div>
<div class="section" id="streaming-checkpoint-table-management">
<span id="streaming-checkpoint-table-mgmt"></span><h3><a class="toc-backref" href="#id10">Streaming checkpoint table management</a><a class="headerlink" href="#streaming-checkpoint-table-management" title="Permalink to this headline"> </a></h3>
<p>The Azure Synapse connector <em>does not</em> delete the streaming checkpoint table that is created when new streaming query is started.
This behavior is consistent with the <code class="docutils literal notranslate"><span class="pre">checkpointLocation</span></code> on DBFS. Therefore we recommend that you periodically delete
checkpoint tables at the same time as removing checkpoint locations on DBFS for queries that are not going to be run in the future or already have checkpoint location removed.</p>
<p>By default, all checkpoint tables have the name <code class="docutils literal notranslate"><span class="pre">&lt;prefix&gt;_&lt;query-id&gt;</span></code>, where <code class="docutils literal notranslate"><span class="pre">&lt;prefix&gt;</span></code> is a configurable prefix with default value <code class="docutils literal notranslate"><span class="pre">databricks_streaming_checkpoint</span></code> and <code class="docutils literal notranslate"><span class="pre">query_id</span></code> is a streaming query ID with <code class="docutils literal notranslate"><span class="pre">_</span></code> characters removed. To find all checkpoint tables for stale or deleted streaming queries, run the query:</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">sys</span><span class="p">.</span><span class="n">tables</span><span class="w"> </span><span class="k">WHERE</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="k">LIKE</span><span class="w"> </span><span class="s1">&#39;databricks_streaming_checkpoint%&#39;</span>
</pre></div>
</div>
<p>You can configure the prefix with the Spark SQL configuration option <code class="docutils literal notranslate"><span class="pre">spark.databricks.sqldw.streaming.exactlyOnce.checkpointTableNamePrefix</span></code>.</p>
</div>
</div>
<div class="section" id="frequently-asked-questions-faq">
<h2>Frequently asked questions (FAQ)<a class="headerlink" href="#frequently-asked-questions-faq" title="Permalink to this headline"> </a></h2>
<p><strong>I received an error while using the Azure Synapse connector. How can I tell if this error is from Azure Synapse or Databricks?</strong></p>
<p>To help you debug errors, any exception thrown by code that is specific to the Azure Synapse connector is wrapped in an exception extending the <code class="docutils literal notranslate"><span class="pre">SqlDWException</span></code> trait. Exceptions also make the following distinction:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">SqlDWConnectorException</span></code> represents an error thrown by the Azure Synapse connector</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SqlDWSideException</span></code> represents an error thrown by the connected Azure Synapse instance</p></li>
</ul>
<p><strong>What should I do if my query failed with the error “No access key found in the session conf or the global Hadoop conf”?</strong></p>
<p>This error means that Azure Synapse connector could not find the
storage account access key in the notebook session configuration or global Hadoop configuration for the storage account specified in <code class="docutils literal notranslate"><span class="pre">tempDir</span></code>.
See <a class="reference internal" href="#usage-batch"><span class="std std-ref">Usage (Batch)</span></a> for examples of how to configure Storage Account access properly. If a Spark table is created using Azure Synapse connector,
you must still provide the storage account access credentials in order to read or write to the Spark table.</p>
<p><strong>Can I use a Shared Access Signature (SAS) to access the Blob storage container specified by <code class="docutils literal notranslate"><span class="pre">tempDir</span></code>?</strong></p>
<p>Azure Synapse does not support using <a class="reference external" href="https://learn.microsoft.com/sql/t-sql/statements/create-database-scoped-credential-transact-sql#b-creating-a-database-scoped-credential-for-a-shared-access-signature">SAS to access Blob storage</a>. Therefore the Azure Synapse connector does not support <a class="reference external" href="https://learn.microsoft.com/azure/storage/common/storage-dotnet-shared-access-signature-part-1">SAS</a> to access the Blob storage container specified by <code class="docutils literal notranslate"><span class="pre">tempDir</span></code>.</p>
<p><strong>I created a Spark table using Azure Synapse connector with the <code class="docutils literal notranslate"><span class="pre">dbTable</span></code> option, wrote some data to this Spark table, and then dropped this Spark table. Will the table created at the Azure Synapse side be dropped?</strong></p>
<p>No. Azure Synapse is considered an external data source. The Azure Synapse table with the name set through <code class="docutils literal notranslate"><span class="pre">dbTable</span></code> is not dropped when
the Spark table is dropped.</p>
<p><strong>When writing a DataFrame to Azure Synapse, why do I need to say <code class="docutils literal notranslate"><span class="pre">.option(&quot;dbTable&quot;,</span> <span class="pre">tableName).save()</span></code> instead of just <code class="docutils literal notranslate"><span class="pre">.saveAsTable(tableName)</span></code>?</strong></p>
<p>That is because we want to make the following distinction clear: <code class="docutils literal notranslate"><span class="pre">.option(&quot;dbTable&quot;,</span> <span class="pre">tableName)</span></code> refers to the <em>database</em> (that is, Azure Synapse) table, whereas <code class="docutils literal notranslate"><span class="pre">.saveAsTable(tableName)</span></code> refers to the Spark table. In fact, you could even combine the two: <code class="docutils literal notranslate"><span class="pre">df.write.</span> <span class="pre">...</span> <span class="pre">.option(&quot;dbTable&quot;,</span> <span class="pre">tableNameDW).saveAsTable(tableNameSpark)</span></code> which creates a table in Azure Synapse called <code class="docutils literal notranslate"><span class="pre">tableNameDW</span></code> and an external table in Spark called <code class="docutils literal notranslate"><span class="pre">tableNameSpark</span></code> that is backed by the Azure Synapse table.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Beware of the following difference between <code class="docutils literal notranslate"><span class="pre">.save()</span></code> and <code class="docutils literal notranslate"><span class="pre">.saveAsTable()</span></code>:</p>
<ul class="simple">
<li><p>For <code class="docutils literal notranslate"><span class="pre">df.write.</span> <span class="pre">...</span> <span class="pre">.option(&quot;dbTable&quot;,</span> <span class="pre">tableNameDW).mode(writeMode).save()</span></code>, <code class="docutils literal notranslate"><span class="pre">writeMode</span></code> acts on the Azure Synapse table, as expected.</p></li>
<li><p>For <code class="docutils literal notranslate"><span class="pre">df.write.</span> <span class="pre">...</span> <span class="pre">.option(&quot;dbTable&quot;,</span> <span class="pre">tableNameDW).mode(writeMode).saveAsTable(tableNameSpark)</span></code>, <code class="docutils literal notranslate"><span class="pre">writeMode</span></code> acts on the Spark table, whereas <code class="docutils literal notranslate"><span class="pre">tableNameDW</span></code> <em>is silently overwritten</em> if it already exists in Azure Synapse.</p></li>
</ul>
<p>This behavior is no different from writing to any other data source. It is just a caveat of the Spark <a class="reference external" href="https://github.com/apache/spark/blob/v2.3.0/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala">DataFrameWriter API</a>.</p>
</div>
<p></p>
</div>
</div>


    
          </div>
        </div>
        <div  class="suapp-rating">
  <div id="suPageRateApp">
     <su-app></su-app>
   </div> 
 </div>
<hr> 
<footer>
  <div role="contentinfo">
      <p class="copyright">
          &copy; Databricks 2023. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the <a href="http://www.apache.org/">Apache Software Foundation</a>.
      </p>
      <p> 
        
          <a id='feedbacklink' href="mailto:doc-feedback@databricks.com?subject=Documentation Feedback">Send us feedback</a>
        
     | <a href="https://databricks.com/privacy-policy">Privacy Policy</a> | <a href="https://databricks.com/terms-of-use">Terms of Use</a></p>

  </div> 

</footer>
      </div>
    </div>
  </section>
</main>

  </page>
  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT: '../../',
      VERSION: '1.0',
      COLLAPSE_INDEX: false,
      FILE_SUFFIX: '.html',
      HAS_SOURCE: 'false'
    };
  </script>
  <script type="text/javascript" src="../../_static/jquery.js"></script>
  <script type="text/javascript" src="../../_static/underscore.js"></script>
  <script type="text/javascript" src="../../_static/doctools.js"></script>
  <script type="text/javascript" src="../../_static/language_data.js"></script>
  

  <script type="text/javascript" src="../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../_static/js/jquery.waypoints.min.js"></script>

  <!-- Select2 (https://select2.org/) -->
  <link href="https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/css/select2.min.css" rel="stylesheet" />
  <script src="https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/js/select2.min.js"></script>
  <!-- End Select2 -->

  
  
  <script type="text/javascript" src="../../_static/js/localized.js"></script>
  <script type="text/javascript" src="../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  
 



  <script>
  window.__searchunifyLoaderConfig = JSON.parse('{"clients": {"en": "02c2e804-27e9-11ee-aefb-0242ac120011", "ja": "6a42c3f2-2820-11ee-aefb-0242ac120011", "pt": "6a86badd-2821-11ee-aefb-0242ac120011"}}')
</script>
<script type="text/javascript" src="../../_static/js/search-loader.js"></script>
</body>
<script type='text/javascript'>
  window.onload = function () {
    var description = document.querySelector('meta[name="description"]').getAttribute("content");
    let titleText = document.querySelector('h1').textContent;
    document.querySelector('meta[property="og:title"]').setAttribute("content", titleText);
    document.querySelector('meta[property="og:description"]').setAttribute("content", description);
    document.querySelector('meta[property="twitter:description"]').setAttribute("content", description);
  };
</script>

</html>