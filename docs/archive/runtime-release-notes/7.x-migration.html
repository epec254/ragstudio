

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en-US" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en-US"> <!--<![endif]-->

<head>
  <!-- cookie consent -->
  
    <!-- Combined Onetrust and Rudderstack Implementation Scripts -->
    <!-- Onetrust Initialization -->
    <script type="text/javascript" src="https://cdn.cookielaw.org/consent/92466579-1717-44d3-809d-a05fb02843ed-test/OtAutoBlock.js"></script>
    <script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="92466579-1717-44d3-809d-a05fb02843ed-test"></script>
    <link rel="stylesheet" id="db-onetrust-style" href="https://www.databricks.com/wp-content/uploads/db_onetrust.css" media="all" />
    <!-- Setting Rudderstack Write Key -->
    <script>window.rudderstackKey = "2SOR9fvSr5Fi6tN2ihPbVHnX1SZ" </script>
    <!-- Rudderstack Initialization + Onetrust Integration + Rudderstack Custom Events -->
    <script type="text/javascript" src="https://www.databricks.com/sites/default/files/rudderstack/v1/db-rudderstack-events.js"></script>

  <!-- cookie consent -->

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="X-UA-Compatible" content="IE=9" />
  <meta content="Learn how to migrate from Databricks Runtime 6.x (on Apache Spark 2.4) to  Databricks Runtime 7.x (on Apache Spark 3.0)." name="description" />
<meta content="noindex" name="robots" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0">
  <meta property="og:image" content="https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:title" content="Databricks Runtime 7.x migration guide (unsupported)">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://docs.databricks.com">
  <meta property="og:description" content="" id="og-description">
  <meta name="twitter:image" content="https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png">
  <meta name="twitter:site" content="@databricks">
  <meta name="twitter:creator" content="@databricks">
  <meta property="twitter:description" content="">
  
  <title>Databricks Runtime 7.x migration guide (unsupported) &#124; Databricks on AWS</title>
  
  
  <link rel="canonical" href="https://docs.databricks.com/en/archive/runtime-release-notes/7.x-migration.html">
  <!-- Start hreflang tag -->
  <link rel="alternate" hreflang="en" href="https://docs.databricks.com/en/archive/runtime-release-notes/7.x-migration.html" />
<link rel="alternate" hreflang="x-default" href="https://docs.databricks.com/en/archive/runtime-release-notes/7.x-migration.html" />
  <!-- End hreflang tag -->
  
  
  <link rel="shortcut icon" href="../../_static/favicon.ico" />
  

  

  

  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;
j.setAttributeNode(d.createAttribute('data-ot-ignore'));
f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-T85FQ33');</script>
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;
j.setAttributeNode(d.createAttribute('data-ot-ignore'));
f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-TWTKQQ');</script>
    
  <!-- End Google Tag Manager -->


  <!-- MaxMind / GEO IP -->
  <script src="//js.maxmind.com/js/apis/geoip2/v2.1/geoip2.js" type="text/javascript"></script>
  <!-- End MaxMind / GEO IP -->

  
  
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600&display=swap" rel="stylesheet">
  <link rel="preload" href="../../_static/fonts/DMSans-Bold.ttf" as="font">
  <link rel="preload" href="../../_static/fonts/DMSans-Regular.ttf" as="font">
  <link rel="preload" href="../../_static/fonts/DMMono-Regular.ttf" as="font">
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/cloud-provider-selector.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/translation-selector.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/searchunify/main.css" type="text/css" />

  
  <link rel="index" title="Index" href="../../genindex.html" />
  <link rel="search" title="Search" href="../../search.html" />
  <link rel="top" title="Databricks on AWS" href="../../index.html" /> 
</head>

<body class="wy-body-for-nav" role="document">

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T85FQ33"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TWTKQQ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  
  <nav class="wy-nav-top header su_header" role="navigation" aria-label="top navigation">
    
<nav class="wy-nav-top header su_header" role="navigation" aria-label="top navigation">
  <div class="container-logo">
    <ul class="mobile-menu-toggle">
        <li class="menu-toggle">
            <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
            
            <a href="https://www.databricks.com/" class="wy-nav-top-logo"><img src="../../_static/small-scale-lockup-full-color-rgb.svg" width="137" height="21"
              alt="Databricks" /></a>   
               
              </li>
    </ul>
    <ul class="su_nav-menu">
      <li class="menu-toggle">
        <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
        
          
        
        <a href="https://www.databricks.com/" class="wy-nav-top-logo"><img src="../../_static/small-scale-lockup-full-color-rgb.svg" width="137" height="21"
            alt="Databricks" /></a></li>
        <!-- 
<li><a href="https://help.databricks.com/s/">Help Center</a></li>
<li class="active"><a href="https://docs.databricks.com/en/">Documentation</a></li>
<li><a href="https://kb.databricks.com/">Knowledge Base</a></li>
 -->
    </ul>
  </div>
  <div class="su_nav-right">
    <ul class="su_link-mobile">
  <!-- Mobile header code can go here -->
</ul>
<ul class="right-try-list">
   
</ul>
  </div>
</nav>
  </nav>

  <div class="su_sub-header">
    <div class="container">
      <div class="su_sub-header-inner">
        <!-- <div class="su_subnav-menu-right">
  <div id="auto" style="width: 100%;">
    <div ng-controller="SearchautoController">
      <div bind-html-compile="autocompleteHtml">
        <form class="su__search-box-1" disabled="disabled">
          <input class="su__search-input" type="search" name="Search box" id="su__search-b" placeholder="Search Documentation" disabled="disabled"/>
          <button class="su__search-button" type="submit" class="button button-success" disabled="disabled">
            <svg width="24" height="24" viewBox="0 0 24 24">
              <path
                d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"
                fill="#333"></path>
            </svg>
          </button>
        </form>
      </div>
    </div>
  </div>
</div> -->
        <div class="search-lng-gap"></div>
        <div style="margin-left: 16px; margin-right: 16px;">
          <!-- <select name="lng selector" id="lng-selector">
    <option value="../../../en/archive/runtime-release-notes/7.x-migration.html" class="notranslate">English</option>
    <option value="../../../ja/archive/runtime-release-notes/7.x-migration.html" class="notranslate">日本語</option>
    <option value="../../../pt/archive/runtime-release-notes/7.x-migration.html" class="notranslate">Português (Brasil)</option>
</select> -->
        </div>
        <div class="cloud-selector-container">
          <!-- <select name="cloud provider selector" id="cloud-provider-selector">
    <option value="aws" selected class="notranslate">
        Amazon Web Services
    </option>
    <option value="azure"  class="notranslate">
        Microsoft Azure
    </option>
    <option value="gcp"  class="notranslate">
        Google Cloud Platform
    </option>
</select> -->
        </div>
      </div>
    </div>
  </div>
  <page class="js-page-container">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side su_nav-side">
<div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    

    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../index.html" class="main-navigation-home">Databricks on AWS</a>
    

    
      

      
        <p class="caption"><span class="caption-text">Load &amp; manage data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../rag-temp/index.html">RAG Studio</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    
  <p class="build_info notranslate"data-last-edit="December 23, 2023">
    Updated Jan 11, 2024
  </p>
<script>
  window.addEventListener('DOMContentLoaded',function(){
    var h1=document.querySelector('h1');
    var bi=document.querySelector('[data-last-edit]');
    if(h1 && bi){
      var ver = document.createElement('p');
      ver.className = 'version_info';
      ver.textContent = bi.getAttribute('data-last-edit');
      h1.parentElement.insertBefore(ver, h1.nextElementSibling);
    }
  });
</script>

    <p>
      
        <a id='feedbacklink' href="mailto:doc-feedback@databricks.com?subject=Documentation Feedback">Send us feedback</a>
      
    </p>
  </div>
</div>
</nav>
    
    
<main class="wy-grid-for-nav su_nav-grid">
  <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
    <div class="wy-nav-content su__nav_content">
      <div class="rst-content">
        





<div role="navigation" aria-label="breadcrumbs navigation" class="wy-breadcrumbs-wrapper">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>Databricks Runtime 7.x migration guide (unsupported)</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>
</div>
        
        <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
          <div itemprop="articleBody">
            
    
  <div class="section" id="databricks-runtime-7x-migration-guide-unsupported">
<span id="dbr-7x-migration-guide-unsupported"></span><h1>Databricks Runtime 7.x migration guide (unsupported)<a class="headerlink" href="#databricks-runtime-7x-migration-guide-unsupported" title="Permalink to this headline"> </a></h1>
<p>This guide provides guidance to help you migrate your Databricks workloads from Databricks Runtime 6.x, built on Apache Spark 2.4, to <a class="reference internal" href="7.3lts.html"><span class="doc">Databricks Runtime 7.3 LTS (unsupported)</span></a>, both built on Spark 3.0.</p>
<p>This guide lists the <a class="reference internal" href="#behavior-changes"><span class="std std-ref">Spark 3.0 behavior changes</span></a> that might require you to update Databricks workloads. Some of those changes include complete removal of Python 2 support, the upgrade to Scala 2.12, full support for JDK 11, and the switch from the Gregorian to the Proleptic calendar for dates and timestamps.</p>
<p>This guide is a companion to the <a class="reference internal" href="7.3-migration.html"><span class="doc">Databricks Runtime 7.3 LTS (unsupported) migration guide</span></a>.</p>
<p>For information on migrating between Databricks Runtime versions, see the <a class="reference external" href="https://dbrmg.databricks.com/?cloud_provider=aws">Databricks Runtime migration guide</a>.</p>
<div class="section" id="new-features-and-improvements-available-on-databricks-runtime-7x">
<span id="new-features-and-improvements-available-on-dbr-7x"></span><h2>New features and improvements available on Databricks Runtime 7.x<a class="headerlink" href="#new-features-and-improvements-available-on-databricks-runtime-7x" title="Permalink to this headline"> </a></h2>
<p>For a list of new features, improvements, and library upgrades included in Databricks Runtime 7.3 LTS, see the release notes for each Databricks Runtime version above the one you are migrating from. Supported Databricks Runtime 7.x versions include:</p>
<ul class="simple">
<li><p><a class="reference internal" href="7.3lts.html"><span class="doc">Databricks Runtime 7.3 LTS (unsupported)</span></a></p></li>
</ul>
<p>Post-release maintenance updates are listed in <a class="reference internal" href="maintenance-updates-archive.html"><span class="doc">Maintenance updates for Databricks Runtime (archived)</span></a>.</p>
</div>
<div class="section" id="databricks-runtime-73-lts-system-environment">
<span id="dbr-73-lts-system-environment"></span><h2>Databricks Runtime 7.3 LTS system environment<a class="headerlink" href="#databricks-runtime-73-lts-system-environment" title="Permalink to this headline"> </a></h2>
<ul class="simple">
<li><p><strong>Operating System</strong>: Ubuntu 18.04.5 LTS</p></li>
<li><p><strong>Java</strong>:</p>
<ul>
<li><p><strong>7.3 LTS:</strong> Zulu 8.48.0.53-CA-linux64 (build 1.8.0_265-b11)</p></li>
</ul>
</li>
<li><p><strong>Scala</strong>: 2.12.10</p></li>
<li><p><strong>Python</strong>: 3.7.5</p></li>
<li><p><strong>R</strong>: 3.6.3 (2020-02-29)</p></li>
<li><p><strong>Delta Lake</strong> 0.7.0</p></li>
</ul>
</div>
<div class="section" id="major-apache-spark-30-behavior-changes">
<span id="major-as-30-behavior-changes"></span><span id="behavior-changes"></span><h2>Major Apache Spark 3.0 behavior changes<a class="headerlink" href="#major-apache-spark-30-behavior-changes" title="Permalink to this headline"> </a></h2>
<p>The following behavior changes from Spark 2.4 to Spark 3.0 might require you to update Databricks workloads when you migrate from Databricks Runtime 6.x to Databricks Runtime 7.x.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This article provides a list of the important Spark behavior changes for you to consider when you migrate to Databricks Runtime 7.x. For a complete list of behavior changes, see the <a class="reference external" href="https://spark.apache.org/docs/3.0.1/migration-guide.html">Spark 3.0.1 migration guide</a>.</p>
</div>
<div class="section" id="core">
<h3>Core<a class="headerlink" href="#core" title="Permalink to this headline"> </a></h3>
<ul class="simple">
<li><p>In Spark 3.0, the deprecated accumulator v1 is removed.</p></li>
<li><p>Event log file will be written as UTF-8 encoding, and Spark History Server will replay event log files as UTF-8 encoding. Previously Spark wrote the event log file as default charset of driver JVM process, so Spark History Server of Spark 2.x is needed to read the old event log files in case of incompatible encoding.</p></li>
<li><p>A new protocol for fetching shuffle blocks is used. It is recommended that external shuffle services be upgraded when running Spark 3.0 apps. You can still use old external shuffle services by setting the configuration <code class="docutils literal notranslate"><span class="pre">spark.shuffle.useOldFetchProtocol</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code>. Otherwise, Spark may run into errors with messages like <code class="docutils literal notranslate"><span class="pre">IllegalArgumentException:</span> <span class="pre">Unexpected</span> <span class="pre">message</span> <span class="pre">type:</span> <span class="pre">&lt;number&gt;</span></code>.</p></li>
</ul>
</div>
<div class="section" id="pyspark">
<h3>PySpark<a class="headerlink" href="#pyspark" title="Permalink to this headline"> </a></h3>
<ul class="simple">
<li><p>In Spark 3.0, <code class="docutils literal notranslate"><span class="pre">Column.getItem</span></code> is fixed such that it does not call <code class="docutils literal notranslate"><span class="pre">Column.apply</span></code>. Consequently, if <code class="docutils literal notranslate"><span class="pre">Column</span></code> is used as an argument to <code class="docutils literal notranslate"><span class="pre">getItem</span></code>, the indexing operator should be used. For example, <code class="docutils literal notranslate"><span class="pre">map_col.getItem(col('id'))</span></code> should be replaced with <code class="docutils literal notranslate"><span class="pre">map_col[col('id')]</span></code>.</p></li>
<li><p>As of Spark 3.0, <code class="docutils literal notranslate"><span class="pre">Row</span></code> field names are no longer sorted alphabetically when constructing with named arguments for Python versions 3.6 and above, and the order of fields will match that as entered. To enable sorted fields by default, as in Spark 2.4, set the environment variable <code class="docutils literal notranslate"><span class="pre">PYSPARK_ROW_FIELD_SORTING_ENABLED</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code> for both executors and driver. This environment variable must be consistent on all executors and driver. Otherwise, it may cause failures or incorrect answers. For Python versions lower than 3.6, the field names are sorted alphabetically as the only option.</p></li>
<li><p>Deprecated Python 2 support (<a class="reference external" href="https://issues.apache.org/jira/browse/SPARK-27884">SPARK-27884</a>).</p></li>
</ul>
</div>
<div class="section" id="structured-streaming">
<h3>Structured Streaming<a class="headerlink" href="#structured-streaming" title="Permalink to this headline"> </a></h3>
<ul class="simple">
<li><p>In Spark 3.0, Structured Streaming forces the source schema into nullable when file-based datasources such as text, json, csv, parquet and orc are used via <code class="docutils literal notranslate"><span class="pre">spark.readStream(...)</span></code>. Previously, it respected the nullability in source schema; however, it caused issues tricky to debug with NPE. To restore the previous behavior, set <code class="docutils literal notranslate"><span class="pre">spark.sql.streaming.fileSource.schema.forceNullable</span></code> to <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p>Spark 3.0 fixes the correctness issue on Stream-stream outer join, which changes the schema of state. See <a class="reference external" href="https://issues.apache.org/jira/browse/SPARK-26154">SPARK-26154</a> for more details. If you start your query from checkpoint constructed from Spark 2.x which uses stream-stream outer join, Spark 3.0 fails the query. To recalculate outputs, discard the checkpoint and replay previous inputs.</p></li>
<li><p>In Spark 3.0, the deprecated class <code class="docutils literal notranslate"><span class="pre">org.apache.spark.sql.streaming.ProcessingTime</span></code> has been removed. Use <code class="docutils literal notranslate"><span class="pre">org.apache.spark.sql.streaming.Trigger.ProcessingTime</span></code> instead. Likewise, <code class="docutils literal notranslate"><span class="pre">org.apache.spark.sql.execution.streaming.continuous.ContinuousTrigger</span></code> has been removed in favor of <code class="docutils literal notranslate"><span class="pre">Trigger.Continuous</span></code>, and <code class="docutils literal notranslate"><span class="pre">org.apache.spark.sql.execution.streaming.OneTimeTrigger</span></code> has been hidden in favor of <code class="docutils literal notranslate"><span class="pre">Trigger.Once</span></code>. See <a class="reference external" href="https://issues.apache.org/jira/browse/SPARK-28199">SPARK-28199</a>.</p></li>
</ul>
</div>
<div class="section" id="sql-datasets-and-dataframe">
<h3>SQL, Datasets, and DataFrame<a class="headerlink" href="#sql-datasets-and-dataframe" title="Permalink to this headline"> </a></h3>
<ul class="simple">
<li><p>In Spark 3.0, when inserting a value into a table column with a different data type, the type coercion is performed as per ANSI SQL standard. Certain unreasonable type conversions such as converting <code class="docutils literal notranslate"><span class="pre">string</span></code> to <code class="docutils literal notranslate"><span class="pre">int</span></code> and <code class="docutils literal notranslate"><span class="pre">double</span></code> to <code class="docutils literal notranslate"><span class="pre">boolean</span></code> are disallowed. A runtime exception will be thrown if the value is out-of-range for the data type of the column. In Spark version 2.4 and earlier, type conversions during table insertion are allowed as long as they are valid <code class="docutils literal notranslate"><span class="pre">Cast</span></code>. When inserting an out-of-range value to a integral field, the low-order bits of the value is inserted(the same as Java/Scala numeric type casting). For example, if 257 is inserted to a field of byte type, the result is 1. The behavior is controlled by the option <code class="docutils literal notranslate"><span class="pre">spark.sql.storeAssignmentPolicy</span></code>, with a default value as “ANSI”. Setting the option to “Legacy” restores the previous behavior.</p></li>
<li><p>In Spark 3.0, when casting string value to integral types (tinyint, smallint, int and bigint), datetime types (date, timestamp and interval) and boolean type, the leading and trailing whitespaces (&lt;= ACSII 32) are trimmed before being converted to these type values, for example <code class="docutils literal notranslate"><span class="pre">cast('</span> <span class="pre">1\t'</span> <span class="pre">as</span> <span class="pre">int)</span></code> returns <code class="docutils literal notranslate"><span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">cast('</span> <span class="pre">1\t'</span> <span class="pre">as</span> <span class="pre">boolean)</span></code> returns <code class="docutils literal notranslate"><span class="pre">true</span></code>, <code class="docutils literal notranslate"><span class="pre">cast('2019-10-10\t</span> <span class="pre">as</span> <span class="pre">date)</span></code> returns the date value <code class="docutils literal notranslate"><span class="pre">2019-10-10</span></code>. In Spark version 2.4 and earlier, while casting string to integrals and booleans, it will not trim the whitespaces from both ends, the foregoing results will be <code class="docutils literal notranslate"><span class="pre">null</span></code>, while to datetimes, only the trailing spaces (= ASCII 32) will be removed. See <a class="reference external" href="https://databricks.com/blog/2020/07/22/a-comprehensive-look-at-dates-and-timestamps-in-apache-spark-3-0.html">https://databricks.com/blog/2020/07/22/a-comprehensive-look-at-dates-and-timestamps-in-apache-spark-3-0.html</a>.</p></li>
<li><p>In Spark 3.0, the deprecated methods <code class="docutils literal notranslate"><span class="pre">SQLContext.createExternalTable</span></code> and <code class="docutils literal notranslate"><span class="pre">SparkSession.createExternalTable</span></code> have been removed in favor of their replacement, <code class="docutils literal notranslate"><span class="pre">createTable</span></code>.</p></li>
<li><p>In Spark 3.0, configuration <code class="docutils literal notranslate"><span class="pre">spark.sql.crossJoin.enabled</span></code> becomes internal configuration, and is true by default, so by default Spark won’t raise an exception on SQL with implicit cross joins.</p></li>
<li><p>In Spark 3.0, we reversed argument order of the trim function from <code class="docutils literal notranslate"><span class="pre">TRIM(trimStr,</span> <span class="pre">str)</span></code> to <code class="docutils literal notranslate"><span class="pre">TRIM(str,</span> <span class="pre">trimStr)</span></code> to be compatible with other databases.</p></li>
<li><p>In Spark version 2.4 and earlier, SQL queries such as <code class="docutils literal notranslate"><span class="pre">FROM</span> <span class="pre">&lt;table&gt;</span></code> or <code class="docutils literal notranslate"><span class="pre">FROM</span> <span class="pre">&lt;table&gt;</span> <span class="pre">UNION</span> <span class="pre">ALL</span> <span class="pre">FROM</span> <span class="pre">&lt;table&gt;</span></code> are supported by accident. In hive-style <code class="docutils literal notranslate"><span class="pre">FROM</span> <span class="pre">&lt;table&gt;</span> <span class="pre">SELECT</span> <span class="pre">&lt;expr&gt;</span></code>, the <code class="docutils literal notranslate"><span class="pre">SELECT</span></code> clause is not negligible. Neither Hive nor Presto support this syntax. Therefore we will treat these queries as invalid since Spark 3.0.</p></li>
<li><p>Since Spark 3.0, the Dataset and DataFrame API <code class="docutils literal notranslate"><span class="pre">unionAll</span></code> is not deprecated any more. It is an alias for <code class="docutils literal notranslate"><span class="pre">union</span></code>.</p></li>
<li><p>In Spark version 2.4 and earlier, the parser of JSON data source treats empty strings as null for some data types such as <code class="docutils literal notranslate"><span class="pre">IntegerType</span></code>. For <code class="docutils literal notranslate"><span class="pre">FloatType</span></code> and <code class="docutils literal notranslate"><span class="pre">DoubleType</span></code>, it fails on empty strings and throws exceptions. Since Spark 3.0, we disallow empty strings and will throw exceptions for data types except for <code class="docutils literal notranslate"><span class="pre">StringType</span></code> and <code class="docutils literal notranslate"><span class="pre">BinaryType</span></code>.</p></li>
<li><p>Since Spark 3.0, the <code class="docutils literal notranslate"><span class="pre">from_json</span></code> functions support two modes - <code class="docutils literal notranslate"><span class="pre">PERMISSIVE</span></code> and <code class="docutils literal notranslate"><span class="pre">FAILFAST</span></code>. The modes can be set via the <code class="docutils literal notranslate"><span class="pre">mode</span></code> option. The default mode became <code class="docutils literal notranslate"><span class="pre">PERMISSIVE</span></code>. In previous versions, behavior of <code class="docutils literal notranslate"><span class="pre">from_json</span></code> did not conform to either <code class="docutils literal notranslate"><span class="pre">PERMISSIVE</span></code> or <code class="docutils literal notranslate"><span class="pre">FAILFAST,</span></code> especially in processing of malformed JSON records. For example, the JSON string <code class="docutils literal notranslate"><span class="pre">{&quot;a&quot;</span> <span class="pre">1}</span></code> with the schema <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">INT</span></code> is converted to <code class="docutils literal notranslate"><span class="pre">null</span></code> by previous versions but Spark 3.0 converts it to <code class="docutils literal notranslate"><span class="pre">Row(null)</span></code>.</p></li>
</ul>
<div class="section" id="ddl-statements">
<h4>DDL Statements<a class="headerlink" href="#ddl-statements" title="Permalink to this headline"> </a></h4>
<ul class="simple">
<li><p>In Spark 3.0, <code class="docutils literal notranslate"><span class="pre">CREATE</span> <span class="pre">TABLE</span></code> without a specific provider uses the value of <code class="docutils literal notranslate"><span class="pre">spark.sql.sources.default</span></code> as its provider. In Spark version 2.4 and below, it was Hive. To restore the behavior before Spark 3.0, you can set <code class="docutils literal notranslate"><span class="pre">spark.sql.legacy.createHiveTableByDefault.enabled</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p></li>
<li><p>In Spark 3.0, when inserting a value into a table column with a different data type, the type coercion is performed as per ANSI SQL standard. Certain unreasonable type conversions such as converting <code class="docutils literal notranslate"><span class="pre">string</span></code> to <code class="docutils literal notranslate"><span class="pre">int</span></code> and <code class="docutils literal notranslate"><span class="pre">double</span></code> to <code class="docutils literal notranslate"><span class="pre">boolean</span></code> are disallowed. A runtime exception is thrown if the value is out-of-range for the data type of the column. In Spark version 2.4 and below, type conversions during table insertion are allowed as long as they are valid <code class="docutils literal notranslate"><span class="pre">Cast</span></code>. When inserting an out-of-range value to a integral field, the low-order bits of the value is inserted(the same as Java/Scala numeric type casting). For example, if 257 is inserted to a field of byte type, the result is 1. The behavior is controlled by the option <code class="docutils literal notranslate"><span class="pre">spark.sql.storeAssignmentPolicy</span></code>, with a default value as “ANSI”. Setting the option as “Legacy” restores the previous behavior.</p></li>
<li><p>In Spark 3.0, <code class="docutils literal notranslate"><span class="pre">SHOW</span> <span class="pre">CREATE</span> <span class="pre">TABLE</span></code> always returns Spark DDL, even when the given table is a Hive SerDe table. For generating Hive DDL, use <code class="docutils literal notranslate"><span class="pre">SHOW</span> <span class="pre">CREATE</span> <span class="pre">TABLE</span> <span class="pre">AS</span> <span class="pre">SERDE</span></code> command instead.</p></li>
<li><p>In Spark 3.0, column of <code class="docutils literal notranslate"><span class="pre">CHAR</span></code> type is not allowed in non-Hive-Serde tables, and <code class="docutils literal notranslate"><span class="pre">CREATE/ALTER</span> <span class="pre">TABLE</span></code> commands will fail if <code class="docutils literal notranslate"><span class="pre">CHAR</span></code> type is detected. Please use <code class="docutils literal notranslate"><span class="pre">STRING</span></code> type instead. In Spark version 2.4 and below, <code class="docutils literal notranslate"><span class="pre">CHAR</span></code> type is treated as <code class="docutils literal notranslate"><span class="pre">STRING</span></code> type and the length parameter is simply ignored.</p></li>
</ul>
</div>
<div class="section" id="udfs-and-built-in-functions">
<h4>UDFs and Built-in Functions<a class="headerlink" href="#udfs-and-built-in-functions" title="Permalink to this headline"> </a></h4>
<ul class="simple">
<li><p>In Spark 3.0, using <code class="docutils literal notranslate"><span class="pre">org.apache.spark.sql.functions.udf(AnyRef,</span> <span class="pre">DataType)</span></code> is not allowed by default. Set <code class="docutils literal notranslate"><span class="pre">spark.sql.legacy.allowUntypedScalaUDF</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code> to keep using it. In Spark version 2.4 and below, if <code class="docutils literal notranslate"><span class="pre">org.apache.spark.sql.functions.udf(AnyRef,</span> <span class="pre">DataType)</span></code> gets a Scala closure with primitive-type argument, the returned UDF returns null if the input values is null. However, in Spark 3.0, the UDF returns the default value of the Java type if the input value is null. For example, <code class="docutils literal notranslate"><span class="pre">val</span> <span class="pre">f</span> <span class="pre">=</span> <span class="pre">udf((x:</span> <span class="pre">Int)</span> <span class="pre">=&gt;</span> <span class="pre">x,</span> <span class="pre">IntegerType),</span> <span class="pre">f($&quot;x&quot;)</span></code> returns null in Spark 2.4 and below if column x is null, and returns 0 in Spark 3.0. This behavior change is introduced because Spark 3.0 is built with Scala 2.12 by default.</p></li>
<li><p>In Spark version 2.4 and below, you can create a map with duplicated keys via built-in functions like <code class="docutils literal notranslate"><span class="pre">CreateMap</span></code>, <code class="docutils literal notranslate"><span class="pre">StringToMap</span></code>, etc. The behavior of map with duplicated keys is undefined, for example, map look up respects the duplicated key appears first, <code class="docutils literal notranslate"><span class="pre">Dataset.collect</span></code> only keeps the duplicated key appears last, <code class="docutils literal notranslate"><span class="pre">MapKeys</span></code> returns duplicated keys, etc. In Spark 3.0, Spark throws <code class="docutils literal notranslate"><span class="pre">RuntimeException</span></code> when duplicated keys are found. You can set <code class="docutils literal notranslate"><span class="pre">spark.sql.mapKeyDedupPolicy</span></code> to <code class="docutils literal notranslate"><span class="pre">LAST_WIN</span></code> to deduplicate map keys with last wins policy. Users may still read map values with duplicated keys from data sources which do not enforce it (for example, Parquet), the behavior is undefined.</p></li>
</ul>
</div>
<div class="section" id="data-sources">
<h4>Data Sources<a class="headerlink" href="#data-sources" title="Permalink to this headline"> </a></h4>
<ul class="simple">
<li><p>In Spark version 2.4 and below, partition column value is converted as null if it can’t be cast to a corresponding user provided schema. In 3.0, partition column value is validated with a user provided schema. An exception is thrown if the validation fails. You can disable such validation by setting <code class="docutils literal notranslate"><span class="pre">spark.sql.sources.validatePartitionColumns</span></code> to <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p>In Spark version 2.4 and below, the parser of JSON data source treats empty strings as null for some data types such as <code class="docutils literal notranslate"><span class="pre">IntegerType</span></code>. For <code class="docutils literal notranslate"><span class="pre">FloatType</span></code>, <code class="docutils literal notranslate"><span class="pre">DoubleType</span></code>, <code class="docutils literal notranslate"><span class="pre">DateType</span></code> and <code class="docutils literal notranslate"><span class="pre">TimestampType</span></code>, it fails on empty strings and throws exceptions. Spark 3.0 disallows empty strings and will throw an exception for data types except for <code class="docutils literal notranslate"><span class="pre">StringType</span></code> and <code class="docutils literal notranslate"><span class="pre">BinaryType</span></code>. The previous behavior of allowing an empty string can be restored by setting <code class="docutils literal notranslate"><span class="pre">spark.sql.legacy.json.allowEmptyString.enabled</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p></li>
<li><p>In Spark 3.0, if files or subdirectories disappear during recursive directory listing (that is, they appear in an intermediate listing but then cannot be read or listed during later phases of the recursive directory listing, due to either concurrent file deletions or object store consistency issues) then the listing will fail with an exception unless <code class="docutils literal notranslate"><span class="pre">spark.sql.files.ignoreMissingFiles</span></code> is <code class="docutils literal notranslate"><span class="pre">true</span></code> (default false). In previous versions, these missing files or subdirectories would be ignored. Note that this change of behavior only applies during initial table file listing (or during <code class="docutils literal notranslate"><span class="pre">REFRESH</span> <span class="pre">TABLE</span></code>), not during query execution: the net change is that <code class="docutils literal notranslate"><span class="pre">spark.sql.files.ignoreMissingFiles</span></code> is now obeyed during table file listing and query planning, not only at query execution time.</p></li>
<li><p>In Spark version 2.4 and below, CSV datasource converts a malformed CSV string to a row with all nulls in the PERMISSIVE mode. In Spark 3.0, the returned row can contain non-null fields if some of CSV column values were parsed and converted to desired types successfully.</p></li>
<li><p>In Spark 3.0, parquet logical type <code class="docutils literal notranslate"><span class="pre">TIMESTAMP_MICROS</span></code> is used by default while saving <code class="docutils literal notranslate"><span class="pre">TIMESTAMP</span></code> columns. In Spark version 2.4 and below, <code class="docutils literal notranslate"><span class="pre">TIMESTAMP</span></code> columns are saved as <code class="docutils literal notranslate"><span class="pre">INT96</span></code> in parquet files. Note that some SQL systems such as Hive 1.x and Impala 2.x can only read INT96 timestamps. You can set <code class="docutils literal notranslate"><span class="pre">spark.sql.parquet.outputTimestampType</span></code> as <code class="docutils literal notranslate"><span class="pre">INT96</span></code> to restore the previous behavior and keep interoperability.</p></li>
<li><p>In Spark 3.0, when Avro files are written with user provided schema, the fields are matched by field names between catalyst schema and Avro schema instead of positions.</p></li>
</ul>
</div>
<div class="section" id="query-engine">
<h4>Query Engine<a class="headerlink" href="#query-engine" title="Permalink to this headline"> </a></h4>
<ul class="simple">
<li><p>In Spark 3.0, Dataset query fails if it contains ambiguous column reference that is caused by self join. A typical example: <code class="docutils literal notranslate"><span class="pre">val</span> <span class="pre">df1</span> <span class="pre">=</span> <span class="pre">...;</span> <span class="pre">val</span> <span class="pre">df2</span> <span class="pre">=</span> <span class="pre">df1.filter(...);,</span> <span class="pre">then</span> <span class="pre">df1.join(df2,</span> <span class="pre">df1(&quot;a&quot;)</span> <span class="pre">&gt;</span> <span class="pre">df2(&quot;a&quot;))</span></code> returns an empty result which is quite confusing. This is because Spark cannot resolve Dataset column references that point to tables being self joined, and <code class="docutils literal notranslate"><span class="pre">df1(&quot;a&quot;)</span></code> is exactly the same as <code class="docutils literal notranslate"><span class="pre">df2(&quot;a&quot;)</span></code> in Spark. To restore the behavior before Spark 3.0, you can set <code class="docutils literal notranslate"><span class="pre">spark.sql.analyzer.failAmbiguousSelfJoin</span></code> to <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p>In Spark 3.0, numbers written in scientific notation (for example, <code class="docutils literal notranslate"><span class="pre">1E2</span></code>) are parsed as <code class="docutils literal notranslate"><span class="pre">Double</span></code>. In Spark version 2.4 and below, they’re parsed as <code class="docutils literal notranslate"><span class="pre">Decimal</span></code>. To restore the pre-Spark 3.0 behavior, you can set <code class="docutils literal notranslate"><span class="pre">spark.sql.legacy.exponentLiteralAsDecimal.enabled</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p></li>
<li><p>In Spark 3.0, configuration <code class="docutils literal notranslate"><span class="pre">spark.sql.crossJoin.enabled</span></code> becomes an internal configuration and is true by default. By default Spark won’t raise exceptions on SQL with implicit cross joins.</p></li>
<li><p>In Spark version 2.4 and below, float/double -0.0 is semantically equal to 0.0, but -0.0 and 0.0 are considered as different values when used in aggregate grouping keys, window partition keys, and join keys. In Spark 3.0, this bug is fixed. For example, <code class="docutils literal notranslate"><span class="pre">Seq(-0.0,</span> <span class="pre">0.0).toDF(&quot;d&quot;).groupBy(&quot;d&quot;).count()</span></code> returns <code class="docutils literal notranslate"><span class="pre">[(0.0,</span> <span class="pre">2)]</span></code> in Spark 3.0, and <code class="docutils literal notranslate"><span class="pre">[(0.0,</span> <span class="pre">1),</span> <span class="pre">(-0.0,</span> <span class="pre">1)]</span></code> in Spark 2.4 and below.</p></li>
<li><p>In Spark 3.0, <code class="docutils literal notranslate"><span class="pre">TIMESTAMP</span></code> literals are converted to strings using the SQL config <code class="docutils literal notranslate"><span class="pre">spark.sql.session.timeZone</span></code>. In Spark version 2.4 and below, the conversion uses the default time zone of the Java virtual machine.</p></li>
<li><p>In Spark 3.0, Spark casts <code class="docutils literal notranslate"><span class="pre">String</span></code> to <code class="docutils literal notranslate"><span class="pre">Date/Timestamp</span></code> in binary comparisons with dates/timestamps. The previous behavior of casting <code class="docutils literal notranslate"><span class="pre">Date/Timestamp</span></code> to <code class="docutils literal notranslate"><span class="pre">String</span></code> can be restored by setting <code class="docutils literal notranslate"><span class="pre">spark.sql.legacy.typeCoercion.datetimeToString.enabled</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p></li>
<li><p>In Spark version 2.4 and below, invalid time zone ids are silently ignored and replaced by GMT time zone, for example, in the <code class="docutils literal notranslate"><span class="pre">from_utc_timestamp</span></code> function. In Spark 3.0, such time zone ids are rejected, and Spark throws <code class="docutils literal notranslate"><span class="pre">java.time.DateTimeException</span></code>.</p></li>
<li><p>In Spark 3.0, Proleptic Gregorian calendar is used in parsing, formatting, and converting dates and timestamps as well as in extracting sub-components like years, days and so on. Spark 3.0 uses Java 8 API classes from the java.time packages that are based on <a class="reference external" href="https://docs.oracle.com/javase/8/docs/api/java/time/chrono/IsoChronology.html">ISO chronology</a>. In Spark version 2.4 and below, those operations are performed using the hybrid calendar (<a class="reference external" href="https://docs.oracle.com/javase/7/docs/api/java/util/GregorianCalendar.html">Julian + Gregorian</a>). The changes impact the results for dates before October 15, 1582 (Gregorian) and affect the following Spark 3.0 API:</p>
<ul>
<li><p>Parsing/formatting of timestamp/date strings. This effects on CSV/JSON datasources and on the <code class="docutils literal notranslate"><span class="pre">unix_timestamp</span></code>, <code class="docutils literal notranslate"><span class="pre">date_format</span></code>, <code class="docutils literal notranslate"><span class="pre">to_unix_timestamp</span></code>, <code class="docutils literal notranslate"><span class="pre">from_unixtime</span></code>, <code class="docutils literal notranslate"><span class="pre">to_date</span></code>, <code class="docutils literal notranslate"><span class="pre">to_timestamp</span></code> functions when patterns specified by users is used for parsing and formatting. In Spark 3.0, we define our own pattern strings in <code class="docutils literal notranslate"><span class="pre">sql-ref-datetime-pattern.md</span></code>, which is implemented via <code class="docutils literal notranslate"><span class="pre">java.time.format.DateTimeFormatter</span></code> under the hood. The new implementation performs strict checking of its input. For example, the <code class="docutils literal notranslate"><span class="pre">2015-07-22</span> <span class="pre">10:00:00</span></code> timestamp cannot be parse if pattern is <code class="docutils literal notranslate"><span class="pre">yyyy-MM-dd</span></code> because the parser does not consume whole input. Another example is the <code class="docutils literal notranslate"><span class="pre">31/01/2015</span> <span class="pre">00:00</span></code> input cannot be parsed by the <code class="docutils literal notranslate"><span class="pre">dd/MM/yyyy</span> <span class="pre">hh:mm</span></code> pattern because <code class="docutils literal notranslate"><span class="pre">hh</span></code> presupposes hours in the range 1-12. In Spark version 2.4 and below, <code class="docutils literal notranslate"><span class="pre">java.text.SimpleDateFormat</span></code> is used for timestamp/date string conversions, and the supported patterns are described in <a class="reference external" href="https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html">simpleDateFormat</a>. The old behavior can be restored by setting <code class="docutils literal notranslate"><span class="pre">spark.sql.legacy.timeParserPolicy</span></code> to <code class="docutils literal notranslate"><span class="pre">LEGACY</span></code>.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">weekofyear</span></code>, <code class="docutils literal notranslate"><span class="pre">weekday</span></code>, <code class="docutils literal notranslate"><span class="pre">dayofweek</span></code>, <code class="docutils literal notranslate"><span class="pre">date_trunc</span></code>, <code class="docutils literal notranslate"><span class="pre">from_utc_timestamp</span></code>, <code class="docutils literal notranslate"><span class="pre">to_utc_timestamp</span></code>, and <code class="docutils literal notranslate"><span class="pre">unix_timestamp</span></code> functions use <code class="docutils literal notranslate"><span class="pre">java.time</span></code> API for calculating week number of year, day number of week as well for conversion from/to <code class="docutils literal notranslate"><span class="pre">TimestampType</span></code> values in UTC time zone.</p></li>
<li><p>The JDBC options <code class="docutils literal notranslate"><span class="pre">lowerBound</span></code> and <code class="docutils literal notranslate"><span class="pre">upperBound</span></code> are converted to TimestampType/DateType values in the same way as casting strings to TimestampType/DateType values. The conversion is based on Proleptic Gregorian calendar, and time zone defined by the SQL config <code class="docutils literal notranslate"><span class="pre">spark.sql.session.timeZone</span></code>. In Spark version 2.4 and below, the conversion is based on the hybrid calendar (Julian + Gregorian) and on default system time zone.</p></li>
<li><p>Formatting <code class="docutils literal notranslate"><span class="pre">TIMESTAMP</span></code> and <code class="docutils literal notranslate"><span class="pre">DATE</span></code> literals.</p></li>
<li><p>Creating typed <code class="docutils literal notranslate"><span class="pre">TIMESTAMP</span></code> and <code class="docutils literal notranslate"><span class="pre">DATE</span></code> literals from strings. In Spark 3.0, string conversion to typed <code class="docutils literal notranslate"><span class="pre">TIMESTAMP/DATE</span></code> literals is performed via casting to <code class="docutils literal notranslate"><span class="pre">TIMESTAMP/DATE</span></code> values. For example, <code class="docutils literal notranslate"><span class="pre">TIMESTAMP</span> <span class="pre">'2019-12-23</span> <span class="pre">12:59:30'</span></code> is semantically equal to <code class="docutils literal notranslate"><span class="pre">CAST('2019-12-23</span> <span class="pre">12:59:30'</span> <span class="pre">AS</span> <span class="pre">TIMESTAMP)</span></code>. When the input string does not contain information about time zone, the time zone from the SQL config <code class="docutils literal notranslate"><span class="pre">spark.sql.session.timeZone</span></code> is used in that case. In Spark version 2.4 and below, the conversion is based on JVM system time zone. The different sources of the default time zone may change the behavior of typed <code class="docutils literal notranslate"><span class="pre">TIMESTAMP</span></code> and <code class="docutils literal notranslate"><span class="pre">DATE</span></code> literals.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="apache-hive">
<h4>Apache Hive<a class="headerlink" href="#apache-hive" title="Permalink to this headline"> </a></h4>
<ul class="simple">
<li><p>In Spark 3.0, we upgraded the built-in Hive version from 1.2 to 2.3 which brings the following impacts:</p>
<ul>
<li><p>You may need to set <code class="docutils literal notranslate"><span class="pre">spark.sql.hive.metastore.version</span></code> and <code class="docutils literal notranslate"><span class="pre">spark.sql.hive.metastore.jars</span></code> according to the version of the Hive metastore you want to connect to. For example: set <code class="docutils literal notranslate"><span class="pre">spark.sql.hive.metastore.version</span></code> to <code class="docutils literal notranslate"><span class="pre">1.2.1</span></code> and <code class="docutils literal notranslate"><span class="pre">spark.sql.hive.metastore.jars</span></code> to <code class="docutils literal notranslate"><span class="pre">maven</span></code> if your Hive metastore version is 1.2.1.</p></li>
<li><p>You need to migrate your custom SerDes to Hive 2.3 or build your own Spark with <code class="docutils literal notranslate"><span class="pre">hive-1.2</span></code> profile. See <a class="reference external" href="https://issues.apache.org/jira/browse/HIVE-15167">HIVE-15167</a> for more details.</p></li>
<li><p>The decimal string representation can be different between Hive 1.2 and Hive 2.3 when using <code class="docutils literal notranslate"><span class="pre">TRANSFORM</span></code> operator in SQL for script transformation, which depends on hive’s behavior. In Hive 1.2, the string representation omits trailing zeroes. But in Hive 2.3, it is always padded to 18 digits with trailing zeroes if necessary.</p></li>
<li><p>In Databricks Runtime 7.x, when reading a Hive SerDe table, by default Spark disallows reading files under a subdirectory that is not a table partition. To enable it, set the configuration <code class="docutils literal notranslate"><span class="pre">spark.databricks.io.hive.scanNonpartitionedDirectory.enabled</span></code> as <code class="docutils literal notranslate"><span class="pre">true</span></code>. This does not affect Spark native table readers and file readers.</p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="mllib">
<h3>MLlib<a class="headerlink" href="#mllib" title="Permalink to this headline"> </a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">OneHotEncoder</span></code>, which is deprecated in 2.3, is removed in 3.0 and <code class="docutils literal notranslate"><span class="pre">OneHotEncoderEstimator</span></code> is now renamed to <code class="docutils literal notranslate"><span class="pre">OneHotEncoder</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">org.apache.spark.ml.image.ImageSchema.readImages</span></code>, which is deprecated in 2.3, is removed in 3.0. Use <code class="docutils literal notranslate"><span class="pre">spark.read.format('image')</span></code> instead.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">org.apache.spark.mllib.clustering.KMeans.train</span></code> with param Int <code class="docutils literal notranslate"><span class="pre">runs</span></code>, which is deprecated in 2.1, is removed in 3.0. Use train method without runs instead.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">org.apache.spark.mllib.classification.LogisticRegressionWithSGD</span></code>, which is deprecated in 2.0, is removed in 3.0, use <code class="docutils literal notranslate"><span class="pre">org.apache.spark.ml.classification.LogisticRegression</span></code> or <code class="docutils literal notranslate"><span class="pre">spark.mllib.classification.LogisticRegressionWithLBFGS</span></code> instead.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">org.apache.spark.mllib.feature.ChiSqSelectorModel.isSorted</span></code>, which is deprecated in 2.1, is removed in 3.0, is not intended for subclasses to use.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">org.apache.spark.mllib.regression.RidgeRegressionWithSGD</span></code>, which is deprecated in 2.0, is removed in 3.0. Use <code class="docutils literal notranslate"><span class="pre">org.apache.spark.ml.regression.LinearRegression</span></code> with <code class="docutils literal notranslate"><span class="pre">elasticNetParam</span> <span class="pre">=</span> <span class="pre">0.0</span></code>. Note the default <code class="docutils literal notranslate"><span class="pre">regParam</span></code> is 0.01 for <code class="docutils literal notranslate"><span class="pre">RidgeRegressionWithSGD</span></code>, but is 0.0 for <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">org.apache.spark.mllib.regression.LassoWithSGD</span></code>, which is deprecated in 2.0, is removed in 3.0. Use <code class="docutils literal notranslate"><span class="pre">org.apache.spark.ml.regression.LinearRegression</span></code> with <code class="docutils literal notranslate"><span class="pre">elasticNetParam</span> <span class="pre">=</span> <span class="pre">1.0</span></code>. Note the default <code class="docutils literal notranslate"><span class="pre">regParam</span></code> is 0.01 for <code class="docutils literal notranslate"><span class="pre">LassoWithSGD</span></code>, but is 0.0 for <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">org.apache.spark.mllib.regression.LinearRegressionWithSGD</span></code>, which is deprecated in 2.0, is removed in 3.0. Use <code class="docutils literal notranslate"><span class="pre">org.apache.spark.ml.regression.LinearRegression</span></code> or <code class="docutils literal notranslate"><span class="pre">LBFGS</span></code> instead.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">org.apache.spark.mllib.clustering.KMeans.getRuns</span></code> and <code class="docutils literal notranslate"><span class="pre">setRuns</span></code>, which are deprecated in 2.1, are removed in 3.0, and have had no effect since Spark 2.0.0.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">org.apache.spark.ml.LinearSVCModel.setWeightCol</span></code>, which is deprecated in 2.4, is removed in 3.0, and is not intended for users.</p></li>
<li><p>In 3.0, <code class="docutils literal notranslate"><span class="pre">org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel</span></code> extends <code class="docutils literal notranslate"><span class="pre">MultilayerPerceptronParams</span></code> to expose the training params. As a result, <code class="docutils literal notranslate"><span class="pre">layers</span></code> in <code class="docutils literal notranslate"><span class="pre">MultilayerPerceptronClassificationModel</span></code> has been changed from <code class="docutils literal notranslate"><span class="pre">Array[Int]</span></code> to <code class="docutils literal notranslate"><span class="pre">IntArrayParam</span></code>. You should use <code class="docutils literal notranslate"><span class="pre">MultilayerPerceptronClassificationModel.getLayers</span></code> instead of <code class="docutils literal notranslate"><span class="pre">MultilayerPerceptronClassificationModel.layers</span></code> to retrieve the size of layers.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">org.apache.spark.ml.classification.GBTClassifier.numTrees</span></code>, which is deprecated in 2.4.5, is removed in 3.0. Use <code class="docutils literal notranslate"><span class="pre">getNumTrees</span></code> instead.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">org.apache.spark.ml.clustering.KMeansModel.computeCost</span></code>, which is deprecated in 2.4, is removed in 3.0, use <code class="docutils literal notranslate"><span class="pre">ClusteringEvaluator</span></code> instead.</p></li>
<li><p>The member variable precision in <code class="docutils literal notranslate"><span class="pre">org.apache.spark.mllib.evaluation.MulticlassMetrics</span></code>, which is deprecated in 2.0, is removed in 3.0. Use accuracy instead.</p></li>
<li><p>The member variable recall in <code class="docutils literal notranslate"><span class="pre">org.apache.spark.mllib.evaluation.MulticlassMetrics</span></code>, which is deprecated in 2.0, is removed in 3.0. Use <code class="docutils literal notranslate"><span class="pre">accuracy</span></code> instead.</p></li>
<li><p>The member variable <code class="docutils literal notranslate"><span class="pre">fMeasure</span></code> in <code class="docutils literal notranslate"><span class="pre">org.apache.spark.mllib.evaluation.MulticlassMetrics</span></code>, which is deprecated in 2.0, is removed in 3.0. Use <code class="docutils literal notranslate"><span class="pre">accuracy</span></code> instead.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">org.apache.spark.ml.util.GeneralMLWriter.context</span></code>, which is deprecated in 2.0, is removed in 3.0. Use <code class="docutils literal notranslate"><span class="pre">session</span></code> instead.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">org.apache.spark.ml.util.MLWriter.context</span></code>, which is deprecated in 2.0, is removed in 3.0. Use <code class="docutils literal notranslate"><span class="pre">session</span></code> instead.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">org.apache.spark.ml.util.MLReader.context</span></code>, which is deprecated in 2.0, is removed in 3.0. Use <code class="docutils literal notranslate"><span class="pre">session</span></code> instead.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">abstract</span> <span class="pre">class</span> <span class="pre">UnaryTransformer[IN,</span> <span class="pre">OUT,</span> <span class="pre">T</span> <span class="pre">&lt;:</span> <span class="pre">UnaryTransformer[IN,</span> <span class="pre">OUT,</span> <span class="pre">T]]</span></code> is changed to <code class="docutils literal notranslate"><span class="pre">abstract</span> <span class="pre">class</span> <span class="pre">UnaryTransformer[IN:</span> <span class="pre">TypeTag,</span> <span class="pre">OUT:</span> <span class="pre">TypeTag,</span> <span class="pre">T</span> <span class="pre">&lt;:</span> <span class="pre">UnaryTransformer[IN,</span> <span class="pre">OUT,</span> <span class="pre">T]]</span></code> in 3.0.</p></li>
<li><p>In Spark 3.0, a multiclass logistic regression in Pyspark will now (correctly) return <code class="docutils literal notranslate"><span class="pre">LogisticRegressionSummary</span></code>, not the subclass <code class="docutils literal notranslate"><span class="pre">BinaryLogisticRegressionSummary</span></code>. The additional methods exposed by <code class="docutils literal notranslate"><span class="pre">BinaryLogisticRegressionSummary</span></code> would not work in this case anyway. (<a class="reference external" href="https://issues.apache.org/jira/browse/SPARK-31681">SPARK-31681</a>)</p></li>
<li><p>In Spark 3.0, <code class="docutils literal notranslate"><span class="pre">pyspark.ml.param.shared.Has*</span></code> mixins do not provide any <code class="docutils literal notranslate"><span class="pre">set*(self,</span> <span class="pre">value)</span></code> setter methods anymore, use the respective <code class="docutils literal notranslate"><span class="pre">self.set(self.*,</span> <span class="pre">value)</span></code> instead. See SPARK-29093 for details. (<a class="reference external" href="https://issues.apache.org/jira/browse/SPARK-29093">SPARK-29093</a>)</p></li>
</ul>
</div>
<div class="section" id="other-behavior-changes">
<h3>Other behavior changes<a class="headerlink" href="#other-behavior-changes" title="Permalink to this headline"> </a></h3>
<ul>
<li><p>The upgrade to Scala 2.12 involves the following changes:</p>
<ul>
<li><p>Package cell serialization is handled differently. The following example illustrates the behavior change and how to handle it.</p>
<p>Running <code class="docutils literal notranslate"><span class="pre">foo.bar.MyObjectInPackageCell.run()</span></code> as defined in the following package cell will trigger the error <code class="docutils literal notranslate"><span class="pre">java.lang.NoClassDefFoundError:</span> <span class="pre">Could</span> <span class="pre">not</span> <span class="pre">initialize</span> <span class="pre">class</span> <span class="pre">foo.bar.MyObjectInPackageCell$</span></code></p>
<div class="highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="k">package</span><span class="w"> </span><span class="nn">foo</span><span class="p">.</span><span class="n">bar</span>

<span class="k">case</span><span class="w"> </span><span class="k">class</span><span class="w"> </span><span class="nc">MyIntStruct</span><span class="p">(</span><span class="n">int</span><span class="p">:</span><span class="w"> </span><span class="nc">Int</span><span class="p">)</span>

<span class="k">import</span><span class="w"> </span><span class="nn">org</span><span class="p">.</span><span class="nn">apache</span><span class="p">.</span><span class="nn">spark</span><span class="p">.</span><span class="nn">sql</span><span class="p">.</span><span class="nc">SparkSession</span>
<span class="k">import</span><span class="w"> </span><span class="nn">org</span><span class="p">.</span><span class="nn">apache</span><span class="p">.</span><span class="nn">spark</span><span class="p">.</span><span class="nn">sql</span><span class="p">.</span><span class="nn">functions</span><span class="p">.</span><span class="n">_</span>
<span class="k">import</span><span class="w"> </span><span class="nn">org</span><span class="p">.</span><span class="nn">apache</span><span class="p">.</span><span class="nn">spark</span><span class="p">.</span><span class="nn">sql</span><span class="p">.</span><span class="nc">Column</span>

<span class="k">object</span><span class="w"> </span><span class="nc">MyObjectInPackageCell</span><span class="w"> </span><span class="k">extends</span><span class="w"> </span><span class="nc">Serializable</span><span class="w"> </span><span class="p">{</span>

<span class="w">  </span><span class="c1">// Because SparkSession cannot be created in Spark executors,</span>
<span class="w">  </span><span class="c1">// the following line triggers the error</span>
<span class="w">  </span><span class="c1">// Could not initialize class foo.bar.MyObjectInPackageCell$</span>
<span class="w">  </span><span class="kd">val</span><span class="w"> </span><span class="n">spark</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="w">  </span><span class="k">def</span><span class="w"> </span><span class="nf">foo</span><span class="p">:</span><span class="w"> </span><span class="nc">Int</span><span class="w"> </span><span class="o">=&gt;</span><span class="w"> </span><span class="nc">Option</span><span class="p">[</span><span class="nc">MyIntStruct</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">x</span><span class="p">:</span><span class="w"> </span><span class="nc">Int</span><span class="p">)</span><span class="w"> </span><span class="o">=&gt;</span><span class="w"> </span><span class="nc">Some</span><span class="p">(</span><span class="nc">MyIntStruct</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>

<span class="w">  </span><span class="kd">val</span><span class="w"> </span><span class="n">theUDF</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">udf</span><span class="p">(</span><span class="n">foo</span><span class="p">)</span>

<span class="w">  </span><span class="kd">val</span><span class="w"> </span><span class="n">df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kd">val</span><span class="w"> </span><span class="n">myUDFInstance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">theUDF</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">&quot;id&quot;</span><span class="p">))</span>
<span class="w">    </span><span class="n">spark</span><span class="p">.</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">).</span><span class="n">withColumn</span><span class="p">(</span><span class="s">&quot;u&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">myUDFInstance</span><span class="p">)</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">def</span><span class="w"> </span><span class="nf">run</span><span class="p">():</span><span class="w"> </span><span class="nc">Unit</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">df</span><span class="p">.</span><span class="n">collect</span><span class="p">().</span><span class="n">foreach</span><span class="p">(</span><span class="n">println</span><span class="p">)</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>To work around this error, you can wrap <code class="docutils literal notranslate"><span class="pre">MyObjectInPackageCell</span></code> inside a serializable class.</p>
</li>
<li><p>Certain cases using <code class="docutils literal notranslate"><span class="pre">DataStreamWriter.foreachBatch</span></code> will require a source code update.
This change is due to the fact that Scala 2.12 has automatic conversion from lambda expressions to SAM types and can cause ambiguity.</p>
<p>For example, the following Scala code can’t compile:</p>
<div class="highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="n">streams</span>
<span class="w">  </span><span class="p">.</span><span class="n">writeStream</span>
<span class="w">  </span><span class="p">.</span><span class="n">foreachBatch</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="w"> </span><span class="n">id</span><span class="p">)</span><span class="w"> </span><span class="o">=&gt;</span><span class="w"> </span><span class="n">myFunc</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="w"> </span><span class="n">id</span><span class="p">)</span><span class="w"> </span><span class="p">}</span>
</pre></div>
</div>
<p>To fix the compilation error, change <code class="docutils literal notranslate"><span class="pre">foreachBatch</span> <span class="pre">{</span> <span class="pre">(df,</span> <span class="pre">id)</span> <span class="pre">=&gt;</span> <span class="pre">myFunc(df,</span> <span class="pre">id)</span> <span class="pre">}</span></code> to <code class="docutils literal notranslate"><span class="pre">foreachBatch(myFunc</span> <span class="pre">_)</span></code> or use the Java API explicitly: <code class="docutils literal notranslate"><span class="pre">foreachBatch(new</span> <span class="pre">VoidFunction2</span> <span class="pre">...)</span></code>.</p>
</li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>With the AWS SDK upgrade to 1.11.655, the use of <code class="docutils literal notranslate"><span class="pre">org.apache.hadoop.fs.s3native.NativeS3FileSystem</span></code> requires AWS Signature v4 and bucket endpoint setup. A 403 Forbidden error may be thrown if a user has configured AWS Signature v2 to sign requests to S3 with the S3N file system or a user accesses an S3 path that contains “+” characters and uses the legacy S3N file system (for example s3n://bucket/path/+file).</p></li>
</ul>
<ul class="simple">
<li><p>Because the Apache Hive version used for handling Hive user-defined functions and Hive SerDes is upgraded to 2.3, two changes are required:</p>
<ul>
<li><p>Hive’s <code class="docutils literal notranslate"><span class="pre">SerDe</span></code> interface is replaced by an abstract class <code class="docutils literal notranslate"><span class="pre">AbstractSerDe</span></code>.
For any custom Hive <code class="docutils literal notranslate"><span class="pre">SerDe</span></code> implementation, migrating to <code class="docutils literal notranslate"><span class="pre">AbstractSerDe</span></code> is required.</p></li>
<li><p>Setting <code class="docutils literal notranslate"><span class="pre">spark.sql.hive.metastore.jars</span></code> to <code class="docutils literal notranslate"><span class="pre">builtin</span></code> means that the Hive 2.3 metastore client will be used to access metastores for Databricks Runtime 7.x. If you need to access Hive 1.2 based external metastores, set <code class="docutils literal notranslate"><span class="pre">spark.sql.hive.metastore.jars</span></code> to the folder that contains Hive 1.2 jars.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="deprecations-and-removals">
<h3>Deprecations and removals<a class="headerlink" href="#deprecations-and-removals" title="Permalink to this headline"> </a></h3>
<ul class="simple">
<li><p>Data skipping index was deprecated in Databricks Runtime 4.3 and removed in Databricks Runtime 7.x. We recommend that you use Delta tables instead, which offer <a class="reference internal" href="../../delta/data-skipping.html"><span class="doc">improved data skipping capabilities</span></a>.</p></li>
<li><p>In Databricks Runtime 7.x, the underlying version of Apache Spark uses Scala 2.12. Since libraries compiled against Scala 2.11 can disable Databricks Runtime 7.x clusters in unexpected ways, clusters running Databricks Runtime 7.x do not install <a class="reference internal" href="../../libraries/workspace-libraries.html#install-workspace-libraries"><span class="std std-ref">libraries configured to be installed on all clusters</span></a>. The cluster <a class="reference internal" href="../../libraries/cluster-libraries.html#view-the-libraries-installed-on-a-cluster"><span class="std std-ref">Libraries tab</span></a> shows a status <code class="docutils literal notranslate"><span class="pre">Skipped</span></code> and a deprecation message that explains the changes in library handling. However, if you have a cluster that was created on an earlier version of Databricks Runtime <em>before Databricks platform version 3.20 was released to your workspace</em>, and you now edit that cluster to use Databricks Runtime 7.x, any libraries that were configured to be installed on all clusters will be installed on that cluster. In this case, any incompatible JARs in the installed libraries can cause the cluster to be disabled. The workaround is either to clone the cluster or to create a new cluster.</p></li>
</ul>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">org.apache.hadoop.fs.s3native.NativeS3FileSystem</span></code> and <code class="docutils literal notranslate"><span class="pre">org.apache.hadoop.fs.s3.S3FileSystem</span></code> are no longer supported for accessing S3.</p>
<p>We strongly encourage you to use <code class="docutils literal notranslate"><span class="pre">com.databricks.s3a.S3AFileSystem</span></code>, which is the default for <code class="docutils literal notranslate"><span class="pre">s3a://</span></code>, <code class="docutils literal notranslate"><span class="pre">s3://</span></code>, and <code class="docutils literal notranslate"><span class="pre">s3n://</span></code> file system schemes in Databricks Runtime. If you need assistance with migration to <code class="docutils literal notranslate"><span class="pre">com.databricks.s3a.S3AFileSystem</span></code>, contact Databricks support or your Databricks account team.</p>
</li>
</ul>
</div>
<div class="section" id="known-issues">
<h3>Known issues<a class="headerlink" href="#known-issues" title="Permalink to this headline"> </a></h3>
<ul class="simple">
<li><p>Parsing day of year using pattern letter ‘D’ returns the wrong result if the year field is missing. This can happen in SQL functions like <code class="docutils literal notranslate"><span class="pre">to_timestamp</span></code> which parses datetime string to datetime values using a pattern string. (<a class="reference external" href="https://issues.apache.org/jira/browse/SPARK-31939">SPARK-31939</a>)</p></li>
<li><p>Join/Window/Aggregate inside subqueries may lead to wrong results if the keys have values -0.0 and 0.0. (<a class="reference external" href="https://issues.apache.org/jira/browse/SPARK-31958">SPARK-31958</a>)</p></li>
<li><p>A window query may fail with ambiguous self-join error unexpectedly. (<a class="reference external" href="https://issues.apache.org/jira/browse/SPARK-31956">SPARK-31956</a>)</p></li>
<li><p>Streaming queries with <code class="docutils literal notranslate"><span class="pre">dropDuplicates</span></code> operator may not be able to restart with the checkpoint written by Spark 2.x. (<a class="reference external" href="https://issues.apache.org/jira/browse/SPARK-31990">SPARK-31990</a>)</p></li>
</ul>
</div>
</div>
</div>


    
          </div>
        </div>
        <div  class="suapp-rating">
  <div id="suPageRateApp">
     <su-app></su-app>
   </div> 
 </div>
<hr> 
<footer>
  <div role="contentinfo">
      <p class="copyright">
          &copy; Databricks 2023. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the <a href="http://www.apache.org/">Apache Software Foundation</a>.
      </p>
      <p> 
        
          <a id='feedbacklink' href="mailto:doc-feedback@databricks.com?subject=Documentation Feedback">Send us feedback</a>
        
     | <a href="https://databricks.com/privacy-policy">Privacy Policy</a> | <a href="https://databricks.com/terms-of-use">Terms of Use</a></p>

  </div> 

</footer>
      </div>
    </div>
  </section>
</main>

  </page>
  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT: '../../',
      VERSION: '1.0',
      COLLAPSE_INDEX: false,
      FILE_SUFFIX: '.html',
      HAS_SOURCE: 'false'
    };
  </script>
  <script type="text/javascript" src="../../_static/jquery.js"></script>
  <script type="text/javascript" src="../../_static/underscore.js"></script>
  <script type="text/javascript" src="../../_static/doctools.js"></script>
  <script type="text/javascript" src="../../_static/language_data.js"></script>
  

  <script type="text/javascript" src="../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../_static/js/jquery.waypoints.min.js"></script>

  <!-- Select2 (https://select2.org/) -->
  <link href="https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/css/select2.min.css" rel="stylesheet" />
  <script src="https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/js/select2.min.js"></script>
  <!-- End Select2 -->

  
  
  <script type="text/javascript" src="../../_static/js/localized.js"></script>
  <script type="text/javascript" src="../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  
 



  <script>
  window.__searchunifyLoaderConfig = JSON.parse('{"clients": {"en": "02c2e804-27e9-11ee-aefb-0242ac120011", "ja": "6a42c3f2-2820-11ee-aefb-0242ac120011", "pt": "6a86badd-2821-11ee-aefb-0242ac120011"}}')
</script>
<script type="text/javascript" src="../../_static/js/search-loader.js"></script>
</body>
<script type='text/javascript'>
  window.onload = function () {
    var description = document.querySelector('meta[name="description"]').getAttribute("content");
    let titleText = document.querySelector('h1').textContent;
    document.querySelector('meta[property="og:title"]').setAttribute("content", titleText);
    document.querySelector('meta[property="og:description"]').setAttribute("content", description);
    document.querySelector('meta[property="twitter:description"]').setAttribute("content", description);
  };
</script>

</html>