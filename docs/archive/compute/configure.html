

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en-US" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en-US"> <!--<![endif]-->

<head>
  <!-- cookie consent -->
  
    <!-- Combined Onetrust and Rudderstack Implementation Scripts -->
    <!-- Onetrust Initialization -->
    <script type="text/javascript" src="https://cdn.cookielaw.org/consent/92466579-1717-44d3-809d-a05fb02843ed-test/OtAutoBlock.js"></script>
    <script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="92466579-1717-44d3-809d-a05fb02843ed-test"></script>
    <link rel="stylesheet" id="db-onetrust-style" href="https://www.databricks.com/wp-content/uploads/db_onetrust.css" media="all" />
    <!-- Setting Rudderstack Write Key -->
    <script>window.rudderstackKey = "2SOR9fvSr5Fi6tN2ihPbVHnX1SZ" </script>
    <!-- Rudderstack Initialization + Onetrust Integration + Rudderstack Custom Events -->
    <script type="text/javascript" src="https://www.databricks.com/sites/default/files/rudderstack/v1/db-rudderstack-events.js"></script>

  <!-- cookie consent -->

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="X-UA-Compatible" content="IE=9" />
  <meta content="Learn how to configure Databricks clusters, including cluster mode, runtime, instance types, size, pools, autoscaling preferences, termination schedule, Apache Spark options, custom tags, log delivery, and more." name="description" />
<meta content="noindex" name="robots" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0">
  <meta property="og:image" content="https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:title" content="Configure clusters">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://docs.databricks.com">
  <meta property="og:description" content="" id="og-description">
  <meta name="twitter:image" content="https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png">
  <meta name="twitter:site" content="@databricks">
  <meta name="twitter:creator" content="@databricks">
  <meta property="twitter:description" content="">
  
  <title>Configure clusters &#124; Databricks on AWS</title>
  
  
  <link rel="canonical" href="https://docs.databricks.com/en/archive/compute/configure.html">
  <!-- Start hreflang tag -->
  <link rel="alternate" hreflang="en" href="https://docs.databricks.com/en/archive/compute/configure.html" />
<link rel="alternate" hreflang="x-default" href="https://docs.databricks.com/en/archive/compute/configure.html" />
  <!-- End hreflang tag -->
  
  
  <link rel="shortcut icon" href="../../_static/favicon.ico" />
  

  

  

  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;
j.setAttributeNode(d.createAttribute('data-ot-ignore'));
f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-T85FQ33');</script>
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;
j.setAttributeNode(d.createAttribute('data-ot-ignore'));
f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-TWTKQQ');</script>
    
  <!-- End Google Tag Manager -->


  <!-- MaxMind / GEO IP -->
  <script src="//js.maxmind.com/js/apis/geoip2/v2.1/geoip2.js" type="text/javascript"></script>
  <!-- End MaxMind / GEO IP -->

  
  
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600&display=swap" rel="stylesheet">
  <link rel="preload" href="../../_static/fonts/DMSans-Bold.ttf" as="font">
  <link rel="preload" href="../../_static/fonts/DMSans-Regular.ttf" as="font">
  <link rel="preload" href="../../_static/fonts/DMMono-Regular.ttf" as="font">
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/cloud-provider-selector.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/translation-selector.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/searchunify/main.css" type="text/css" />

  
  <link rel="index" title="Index" href="../../genindex.html" />
  <link rel="search" title="Search" href="../../search.html" />
  <link rel="top" title="Databricks on AWS" href="../../index.html" /> 
</head>

<body class="wy-body-for-nav" role="document">

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T85FQ33"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TWTKQQ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  
  <nav class="wy-nav-top header su_header" role="navigation" aria-label="top navigation">
    
<nav class="wy-nav-top header su_header" role="navigation" aria-label="top navigation">
  <div class="container-logo">
    <ul class="mobile-menu-toggle">
        <li class="menu-toggle">
            <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
            
            <a href="https://www.databricks.com/" class="wy-nav-top-logo"><img src="../../_static/small-scale-lockup-full-color-rgb.svg" width="137" height="21"
              alt="Databricks" /></a>   
               
              </li>
    </ul>
    <ul class="su_nav-menu">
      <li class="menu-toggle">
        <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
        
          
        
        <a href="https://www.databricks.com/" class="wy-nav-top-logo"><img src="../../_static/small-scale-lockup-full-color-rgb.svg" width="137" height="21"
            alt="Databricks" /></a></li>
        <!-- 
<li><a href="https://help.databricks.com/s/">Help Center</a></li>
<li class="active"><a href="https://docs.databricks.com/en/">Documentation</a></li>
<li><a href="https://kb.databricks.com/">Knowledge Base</a></li>
 -->
    </ul>
  </div>
  <div class="su_nav-right">
    <ul class="su_link-mobile">
  <!-- Mobile header code can go here -->
</ul>
<ul class="right-try-list">
   
</ul>
  </div>
</nav>
  </nav>

  <div class="su_sub-header">
    <div class="container">
      <div class="su_sub-header-inner">
        <!-- <div class="su_subnav-menu-right">
  <div id="auto" style="width: 100%;">
    <div ng-controller="SearchautoController">
      <div bind-html-compile="autocompleteHtml">
        <form class="su__search-box-1" disabled="disabled">
          <input class="su__search-input" type="search" name="Search box" id="su__search-b" placeholder="Search Documentation" disabled="disabled"/>
          <button class="su__search-button" type="submit" class="button button-success" disabled="disabled">
            <svg width="24" height="24" viewBox="0 0 24 24">
              <path
                d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"
                fill="#333"></path>
            </svg>
          </button>
        </form>
      </div>
    </div>
  </div>
</div> -->
        <div class="search-lng-gap"></div>
        <div style="margin-left: 16px; margin-right: 16px;">
          <!-- <select name="lng selector" id="lng-selector">
    <option value="../../../en/archive/compute/configure.html" class="notranslate">English</option>
    <option value="../../../ja/archive/compute/configure.html" class="notranslate">日本語</option>
    <option value="../../../pt/archive/compute/configure.html" class="notranslate">Português (Brasil)</option>
</select> -->
        </div>
        <div class="cloud-selector-container">
          <!-- <select name="cloud provider selector" id="cloud-provider-selector">
    <option value="aws" selected class="notranslate">
        Amazon Web Services
    </option>
    <option value="azure"  class="notranslate">
        Microsoft Azure
    </option>
    <option value="gcp"  class="notranslate">
        Google Cloud Platform
    </option>
</select> -->
        </div>
      </div>
    </div>
  </div>
  <page class="js-page-container">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side su_nav-side">
<div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    

    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../index.html" class="main-navigation-home">Databricks on AWS</a>
    

    
      

      
        <p class="caption"><span class="caption-text">Load &amp; manage data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../rag-temp/index.html">RAG Studio</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    
  <p class="build_info notranslate"data-last-edit="December 23, 2023">
    Updated Jan 11, 2024
  </p>
<script>
  window.addEventListener('DOMContentLoaded',function(){
    var h1=document.querySelector('h1');
    var bi=document.querySelector('[data-last-edit]');
    if(h1 && bi){
      var ver = document.createElement('p');
      ver.className = 'version_info';
      ver.textContent = bi.getAttribute('data-last-edit');
      h1.parentElement.insertBefore(ver, h1.nextElementSibling);
    }
  });
</script>

    <p>
      
        <a id='feedbacklink' href="mailto:doc-feedback@databricks.com?subject=Documentation Feedback">Send us feedback</a>
      
    </p>
  </div>
</div>
</nav>
    
    
<main class="wy-grid-for-nav su_nav-grid">
  <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
    <div class="wy-nav-content su__nav_content">
      <div class="rst-content">
        





<div role="navigation" aria-label="breadcrumbs navigation" class="wy-breadcrumbs-wrapper">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>Configure clusters</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>
</div>
        
        <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
          <div itemprop="articleBody">
            
    
  <div class="section" id="configure-clusters">
<h1>Configure clusters<a class="headerlink" href="#configure-clusters" title="Permalink to this headline"> </a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>These are instructions for the legacy create cluster UI, and are included only for historical accuracy. All customers should be using the <a class="reference internal" href="../../compute/configure.html"><span class="doc">updated create cluster UI</span></a>.</p>
</div>
<p>This article explains the configuration options available when you create and edit Databricks clusters. It focuses on creating and editing clusters using the UI. For other methods, see the <a class="reference internal" href="../../dev-tools/cli/index.html"><span class="doc">Databricks CLI</span></a>, the <a class="reference external" href="https://docs.databricks.com/api/workspace/clusters">Clusters API</a>, and <a class="reference internal" href="../../dev-tools/terraform/index.html"><span class="doc">Databricks Terraform provider</span></a>.</p>
<p>For help deciding what combination of configuration options suits your needs best, see <a class="reference internal" href="../../compute/cluster-config-best-practices.html"><span class="doc">cluster configuration best practices</span></a>.</p>
<div class="figure align-default">
<img alt="Create cluster" src="../../_images/create-dialog-aws.png" />
</div>
<div class="section" id="cluster-policy">
<h2>Cluster policy<a class="headerlink" href="#cluster-policy" title="Permalink to this headline"> </a></h2>
<p>A <a class="reference internal" href="../../administration-guide/clusters/policies.html"><span class="doc">cluster policy</span></a> limits the ability to configure clusters based on a set of rules. The policy rules limit the attributes or attribute values available for cluster creation. Cluster policies have ACLs that limit their use to specific users and groups and thus limit which policies you can select when you create a cluster.</p>
<p>To configure a cluster policy, select the cluster policy in the <strong>Policy</strong> drop-down.</p>
<div class="figure align-default">
<img alt="Select cluster policy" src="../../_images/policy.png" />
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If no policies have been <a class="reference internal" href="../../administration-guide/clusters/policies.html"><span class="doc">created in the workspace</span></a>, the <strong>Policy</strong> drop-down does not display.</p>
</div>
<p>If you have:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../../security/auth-authz/access-control/cluster-acl.html#cluster-create-permission"><span class="std std-ref">Cluster create permission</span></a>, you can select the <strong>Unrestricted</strong> policy and create fully-configurable clusters. The <strong>Unrestricted</strong> policy does not limit any cluster attributes or attribute values.</p></li>
<li><p>Both cluster create permission and access to cluster policies, you can select the <strong>Unrestricted</strong> policy and the policies you have access to.</p></li>
<li><p>Access to cluster policies only, you can select the policies you have access to.</p></li>
</ul>
</div>
<div class="section" id="cluster-mode">
<h2>Cluster mode<a class="headerlink" href="#cluster-mode" title="Permalink to this headline"> </a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This article describes the legacy clusters UI. For information about the new clusters UI (in preview), see <a class="reference internal" href="../../compute/configure.html"><span class="doc">Create a cluster</span></a>. This includes some terminology changes for cluster access types and modes. For a comparison of the new and legacy cluster types, see <a class="reference internal" href="cluster-ui-preview.html"><span class="doc">Clusters UI changes and cluster access modes</span></a>. In the preview UI:</p>
<ul class="simple">
<li><p><em>Standard mode clusters</em> are now called <em>No Isolation Shared access mode clusters</em>.</p></li>
<li><p><em>High Concurrency with Tables ACLs</em> are now called <em>Shared access mode clusters</em>.</p></li>
</ul>
</div>
<p>Databricks supports three cluster modes: Standard, High Concurrency, and <a class="reference internal" href="../../compute/single-node.html"><span class="doc">Single Node</span></a>. The default cluster mode is Standard.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<ul class="simple">
<li><p>If your workspace is assigned to a <a class="reference internal" href="../../data-governance/unity-catalog/index.html"><span class="doc">Unity Catalog</span></a> metastore, High Concurrency clusters are not available. Instead, you use <a class="reference internal" href="../../compute/configure.html#access-mode"><span class="std std-ref">access mode</span></a> to ensure the integrity of access controls and enforce strong isolation guarantees. See also <a class="reference internal" href="../../compute/configure.html#access-mode"><span class="std std-ref">Access modes</span></a>.</p></li>
<li><p>You cannot change the cluster mode after a cluster is created. If you want a different cluster mode, you must create a new cluster.</p></li>
</ul>
</div>
<p>The cluster configuration includes an <a class="reference internal" href="../../compute/clusters-manage.html#automatic-termination"><span class="std std-ref">auto terminate</span></a> setting whose <em>default value</em> depends on cluster mode:</p>
<ul class="simple">
<li><p>Standard and Single Node clusters terminate automatically after 120 minutes by default.</p></li>
<li><p>High Concurrency clusters <em>do not</em> terminate automatically by default.</p></li>
</ul>
<div class="section" id="standard-clusters">
<span id="standard"></span><h3>Standard clusters<a class="headerlink" href="#standard-clusters" title="Permalink to this headline"> </a></h3>
<p></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Standard mode clusters (sometimes called No Isolation Shared clusters) can be shared by multiple users, with no isolation between users. If you use the High Concurrency cluster mode <em>without additional security settings such as Table ACLs or Credential Passthrough</em>, the same settings are used as Standard mode clusters. Account admins can <a class="reference internal" href="../../administration-guide/account-settings/no-isolation-shared.html"><span class="doc">prevent internal credentials from being automatically generated for Databricks workspace admins</span></a> on these types of cluster. For more secure options, Databricks recommends alternatives such as high concurrency clusters with Table ACLs.</p>
</div>
<p>A Standard cluster is recommended for single users only. Standard clusters can run workloads developed in Python, SQL, R, and Scala.</p>
</div>
<div class="section" id="high-concurrency-clusters">
<span id="high-concurrency"></span><h3>High Concurrency clusters<a class="headerlink" href="#high-concurrency-clusters" title="Permalink to this headline"> </a></h3>
<p>A High Concurrency cluster is a managed cloud resource. The key benefits of High Concurrency clusters are that they provide fine-grained sharing for maximum resource utilization and minimum query latencies.</p>
<p>High Concurrency clusters can run workloads developed in SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala.</p>
<p>In addition, only High Concurrency clusters support <a class="reference internal" href="../../data-governance/table-acls/index.html"><span class="doc">table access control</span></a>.</p>
<p>To create a High Concurrency cluster, set <strong>Cluster Mode</strong> to <strong>High Concurrency</strong>.</p>
<div class="figure align-default">
<img alt="High Concurrency cluster mode" src="../../_images/high-concurrency.png" />
</div>
</div>
<div class="section" id="single-node-clusters">
<span id="single-node"></span><h3>Single Node clusters<a class="headerlink" href="#single-node-clusters" title="Permalink to this headline"> </a></h3>
<p>A Single Node cluster has no workers and runs Spark jobs on the driver node.</p>
<p>In contrast, a Standard cluster requires <em>at least one</em> Spark worker node in addition to the driver node to execute Spark jobs.</p>
<p>To create a Single Node cluster, set <strong>Cluster Mode</strong> to <strong>Single Node</strong>.</p>
<div class="figure align-default">
<img alt="Single Node cluster mode" src="../../_images/single-node.png" />
</div>
<p>To learn more about working with Single Node clusters, see <a class="reference internal" href="../../compute/single-node.html"><span class="doc">Single node compute</span></a>.</p>
</div>
</div>
<div class="section" id="pools">
<span id="instance-pools"></span><span id="pool"></span><h2>Pools<a class="headerlink" href="#pools" title="Permalink to this headline"> </a></h2>
<p>To reduce cluster start time, you can attach a cluster to a predefined <a class="reference internal" href="../../compute/pools.html"><span class="doc">pool</span></a> of idle instances, for the driver and worker nodes. The cluster is created using instances in the pools. If a pool does not have sufficient idle resources to create the requested  driver or worker nodes, the pool expands by allocating new instances from the instance provider. When an attached cluster is terminated, the instances it used are returned to the pools and can be reused by a different cluster.</p>
<p>If you select a pool for worker nodes but not for the driver node, the driver node inherit the pool from the worker node configuration.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If you attempt to select a pool for the driver node but not for worker nodes, an error occurs and your cluster isn’t created. This requirement prevents a situation where the driver node has to wait for worker nodes to be created, or vice versa.</p>
</div>
<p>See <a class="reference internal" href="../../compute/pools.html"><span class="doc">Create a pool</span></a> to learn more about working with pools in Databricks.</p>
</div>
<div class="section" id="databricks-runtime">
<span id="dbr"></span><span id="runtime"></span><h2>Databricks Runtime<a class="headerlink" href="#databricks-runtime" title="Permalink to this headline"> </a></h2>
<p>Databricks runtimes are the set of core components that run on your <a class="reference internal" href="../../compute/index.html"><span class="doc">clusters</span></a>. All Databricks runtimes include Apache Spark and add components and updates that improve usability, performance, and security. For details, see <a class="reference internal" href="../../release-notes/runtime/index.html"><span class="doc">Databricks Runtime release notes versions and compatibility</span></a>.</p>
<p>Databricks offers several types of runtimes and several versions of those runtime types in the <strong>Databricks Runtime Version</strong> drop-down when you create or edit a cluster.</p>
<div class="figure align-default">
<img alt="Select Runtime version" src="../../_images/runtime-version.png" />
</div>
<div class="section" id="photon-acceleration">
<span id="photon-image"></span><h3>Photon acceleration<a class="headerlink" href="#photon-acceleration" title="Permalink to this headline"> </a></h3>
<p><a class="reference internal" href="../../compute/photon.html"><span class="doc">Photon</span></a> is available for clusters running <a class="reference internal" href="../../release-notes/runtime/9.1lts.html"><span class="doc">Databricks Runtime 9.1 LTS</span></a> and above.</p>
<p>To enable Photon acceleration, select the <strong>Use Photon Acceleration</strong> checkbox.</p>
<p>If desired, you can specify the instance type in the Worker Type and Driver Type drop-down.</p>
<p>You can view Photon activity in the <a class="reference internal" href="../../compute/clusters-manage.html#spark-ui"><span class="std std-ref">Spark UI</span></a>. The following screenshot shows the query details DAG. There are two indications of Photon in the DAG. First, Photon operators start with “Photon”, for example, <code class="docutils literal notranslate"><span class="pre">PhotonGroupingAgg</span></code>. Second, in the DAG, Photon operators and stages are colored peach, while the non-Photon ones are blue.</p>
<div class="figure align-default">
<img alt="Photon DAG" src="../../_images/photon-dag.png" />
</div>
</div>
<div class="section" id="docker-images">
<span id="photon-acceleration"></span><h3>Docker images<a class="headerlink" href="#docker-images" title="Permalink to this headline"> </a></h3>
<p>For some Databricks Runtime versions, you can specify a Docker image when you create a cluster. Example use cases include library customization, a golden container environment that doesn’t change, and Docker CI/CD integration.</p>
<p>You can also use Docker images to create custom deep learning environments on clusters with GPU devices.</p>
<p>For instructions, see <a class="reference internal" href="../../compute/custom-containers.html"><span class="doc">Customize containers with Databricks Container Service</span></a> and <a class="reference internal" href="../../compute/gpu.html#databricks-container-services-on-gpu"><span class="std std-ref">Databricks Container Services on GPU clusters</span></a>.</p>
<p>
</p>
</div>
</div>
<div class="section" id="cluster-node-type">
<span id="node-types"></span><h2>Cluster node type<a class="headerlink" href="#cluster-node-type" title="Permalink to this headline"> </a></h2>
<p>A cluster consists of one driver node and zero or more worker nodes.</p>
<p>You can pick separate cloud provider instance types for the driver and worker nodes, although by default the driver node uses the same instance type as the worker node. Different families of instance types fit different use cases, such as memory-intensive or compute-intensive workloads.</p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#driver-node" id="id1">Driver node</a></p></li>
<li><p><a class="reference internal" href="#worker-node" id="id2">Worker node</a></p></li>
<li><p><a class="reference internal" href="#gpu-instance-types" id="id3">GPU instance types</a></p></li>
<li><p><a class="reference internal" href="#aws-graviton-instance-types" id="id4">AWS Graviton instance types</a></p></li>
</ul>
</div>
<div class="section" id="driver-node">
<h3><a class="toc-backref" href="#id1">Driver node</a><a class="headerlink" href="#driver-node" title="Permalink to this headline"> </a></h3>
<p>The driver node maintains state information of all notebooks attached to the cluster. The driver node also maintains the SparkContext and interprets all the commands you run from a notebook or a library on the cluster, and runs the Apache Spark master that coordinates with the Spark executors.</p>
<p>The default value of the driver node type is the same as the worker node type. You can choose a larger driver node type with more memory if you are planning to <code class="docutils literal notranslate"><span class="pre">collect()</span></code> a lot of data from Spark workers and analyze them in the notebook.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Since the driver node maintains all of the state information of the notebooks attached, make sure to detach unused notebooks from the driver node.</p>
</div>
</div>
<div class="section" id="worker-node">
<h3><a class="toc-backref" href="#id2">Worker node</a><a class="headerlink" href="#worker-node" title="Permalink to this headline"> </a></h3>
<p>Databricks worker nodes run the Spark executors and other services required for the proper functioning of the clusters. When you distribute your workload with Spark, all of the distributed processing happens on worker nodes. Databricks runs one executor per worker node; therefore the terms <em>executor</em> and <em>worker</em> are used interchangeably in the context of the Databricks architecture.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To run a Spark job, you need at least one worker node. If a cluster has zero workers, you can run non-Spark commands on the driver node, but Spark commands will fail.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Databricks launches worker nodes with two private IP addresses each. The node’s primary private IP address is used to host Databricks internal traffic. The secondary private IP address is used by the Spark container for intra-cluster communication. This model allows Databricks to provide isolation between multiple clusters in the same workspace.</p>
</div>
</div>
<div class="section" id="gpu-instance-types">
<h3><a class="toc-backref" href="#id3">GPU instance types</a><a class="headerlink" href="#gpu-instance-types" title="Permalink to this headline"> </a></h3>
<p>For computationally challenging tasks that demand high performance, like those associated with deep learning, Databricks supports clusters accelerated with graphics processing units (GPUs). For more information, see <a class="reference internal" href="../../compute/gpu.html"><span class="doc">GPU-enabled clusters</span></a>.</p>
</div>
<div class="section" id="aws-graviton-instance-types">
<span id="gpu-instance-types"></span><span id="spot-instances"></span><h3><a class="toc-backref" href="#id4">AWS Graviton instance types</a><a class="headerlink" href="#aws-graviton-instance-types" title="Permalink to this headline"> </a></h3>
<p>Databricks supports clusters with AWS Graviton processors. Arm-based AWS Graviton instances are designed by AWS to deliver better price performance over comparable current generation x86-based instances. See <a class="reference internal" href="../../compute/graviton.html"><span class="doc">AWS Graviton-enabled clusters</span></a>.</p>
</div>
</div>
<div class="section" id="cluster-size-and-autoscaling">
<span id="autoscaling"></span><h2>Cluster size and autoscaling<a class="headerlink" href="#cluster-size-and-autoscaling" title="Permalink to this headline"> </a></h2>
<p>When you create a Databricks cluster, you can either provide a fixed number of workers for the cluster or provide a minimum and maximum number of workers for the cluster.</p>
<p>When you provide a fixed size cluster, Databricks ensures that your cluster has the specified number of workers. When you provide a range for the number of workers, Databricks chooses the appropriate number of workers required to run your job. This is referred to as <em>autoscaling</em>.</p>
<p>With autoscaling, Databricks dynamically reallocates workers to account for the characteristics of your job. Certain parts of your pipeline may be more computationally demanding than others, and Databricks automatically adds additional workers during these phases of your job (and removes them when they’re no longer needed).</p>
<p>Autoscaling makes it easier to achieve high cluster utilization, because you don’t need to provision the cluster to match a workload. This applies especially to workloads whose requirements change over time (like exploring a dataset during the course of a day), but it can also apply to a one-time shorter workload whose provisioning requirements are unknown. Autoscaling thus offers two advantages:</p>
<ul class="simple">
<li><p>Workloads can run faster compared to a constant-sized under-provisioned cluster.</p></li>
<li><p>Autoscaling clusters can reduce overall costs compared to a statically-sized cluster.</p></li>
</ul>
<p>Depending on the constant size of the cluster and the workload, autoscaling gives you one or both of these benefits at the same time.  The cluster size can go below the minimum number of workers selected when the cloud provider terminates instances. In this case, Databricks continuously retries to re-provision instances in order to maintain the minimum number of workers.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Autoscaling is not available for <code class="docutils literal notranslate"><span class="pre">spark-submit</span></code> jobs.</p>
</div>
<div class="section" id="how-autoscaling-behaves">
<h3>How autoscaling behaves<a class="headerlink" href="#how-autoscaling-behaves" title="Permalink to this headline"> </a></h3>
<ul class="simple">
<li><p>Scales up from min to max in 2 steps.</p></li>
<li><p>Can scale down even if the cluster is not idle by looking at shuffle file state.</p></li>
<li><p>Scales down based on a percentage of current nodes.</p></li>
<li><p>On job clusters, scales down if the cluster is underutilized over the last 40 seconds.</p></li>
<li><p>On all-purpose clusters, scales down if the cluster is underutilized over the last 150 seconds.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">spark.databricks.aggressiveWindowDownS</span></code> Spark configuration property specifies in seconds how often a cluster makes down-scaling decisions. Increasing the value causes a cluster to scale down more slowly. The maximum value is 600.</p></li>
</ul>
</div>
<div class="section" id="enable-and-configure-autoscaling">
<h3>Enable and configure autoscaling<a class="headerlink" href="#enable-and-configure-autoscaling" title="Permalink to this headline"> </a></h3>
<p>To allow Databricks to resize your cluster automatically, you enable autoscaling for the cluster and provide the min and max range of workers.</p>
<ol class="arabic">
<li><p>Enable autoscaling.</p>
<ul>
<li><p>All-Purpose cluster - On the Create Cluster page, select the <strong>Enable autoscaling</strong> checkbox in the <strong>Autopilot Options</strong> box:</p>
<div class="figure align-default">
<img alt="Enable_autoscaling for interactive clusters" src="../../_images/autopilot-aws.png" />
</div>
</li>
<li><p>Job cluster - On the Configure Cluster page, select the <strong>Enable autoscaling</strong> checkbox in the <strong>Autopilot Options</strong> box:</p>
<div class="figure align-default">
<img alt="Enable autoscaling for job clusters" src="../../_images/autopilot-job-aws.png" />
</div>
</li>
</ul>
</li>
<li><p>Configure the min and max workers.</p>
<div class="figure align-default">
<img alt="Configure min and max workers" src="../../_images/workers-aws.png" />
</div>
<p>When the cluster is running, the cluster detail page displays the number of allocated workers. You can compare number of allocated workers with the worker configuration and make adjustments as needed.</p>
</li>
</ol>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If you are using an <a class="reference internal" href="../../compute/pools.html"><span class="doc">instance pool</span></a>:</p>
<ul class="simple">
<li><p>Make sure the cluster size requested is less than or equal to the <a class="reference internal" href="../../compute/pools.html#pool-min"><span class="std std-ref">minimum number of idle instances</span></a>
in the pool. If it is larger, cluster startup time will be equivalent to a cluster that doesn’t use a pool.</p></li>
<li><p>Make sure the maximum cluster size is less than or equal to the <a class="reference internal" href="../../compute/pools.html#pool-max"><span class="std std-ref">maximum capacity</span></a> of the pool. If it is larger, the cluster
creation will fail.</p></li>
</ul>
</div>
</div>
<div class="section" id="autoscaling-example">
<h3>Autoscaling example<a class="headerlink" href="#autoscaling-example" title="Permalink to this headline"> </a></h3>
<p>If you reconfigure a static cluster to be an autoscaling cluster, Databricks immediately resizes the cluster within the minimum and maximum bounds and then starts autoscaling. As an example, the following table demonstrates what happens to clusters with a certain initial size if you reconfigure a cluster to autoscale between 5 and 10 nodes.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 33%" />
<col style="width: 67%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Initial size</p></th>
<th class="head"><p>Size after reconfiguration</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>6</p></td>
<td><p>6</p></td>
</tr>
<tr class="row-odd"><td><p>12</p></td>
<td><p>10</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>5</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="local-disk-encryption">
<span id="local-disk-encrypt"></span><h2>Local disk encryption<a class="headerlink" href="#local-disk-encryption" title="Permalink to this headline"> </a></h2>
<div class="preview admonition">
<p class="admonition-title">Preview</p>
<p>This feature is in <a class="reference internal" href="../../release-notes/release-types.html"><span class="doc">Public Preview</span></a>.</p>
</div>
<p>Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or ephemeral data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data that is stored temporarily on your cluster’s local disks, you can enable local disk encryption.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes.</p>
</div>
<p>When local disk encryption is enabled, Databricks generates an encryption key locally that is unique to each cluster node and is used to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk.</p>
<p>To enable local disk encryption, you must use the <a class="reference external" href="https://docs.databricks.com/api/workspace/clusters">Clusters API</a>. During cluster creation or edit, set:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;enable_local_disk_encryption&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="p">}</span>
</pre></div>
</div>
<p>See the <a class="reference external" href="https://docs.databricks.com/api/workspace/clusters">Clusters API</a> for examples of how to invoke these APIs.</p>
<p>Here is an example of a cluster create call that enables local disk encryption:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;cluster_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;my-cluster&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;spark_version&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;7.3.x-scala2.12&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;node_type_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;r3.xlarge&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;enable_local_disk_encryption&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;spark_conf&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;spark.speculation&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="nt">&quot;num_workers&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">25</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="security-mode">
<span id="security-mode"></span><h2>Security mode<a class="headerlink" href="#security-mode" title="Permalink to this headline"> </a></h2>
<p>If your workspace is assigned to a <a class="reference internal" href="../../data-governance/unity-catalog/index.html"><span class="doc">Unity Catalog</span></a> metastore, you use security mode instead of <a class="reference internal" href="../../compute/configure.html#access-mode"><span class="std std-ref">High Concurrency cluster mode</span></a> to ensure the integrity of access controls and enforce strong isolation guarantees. High Concurrency cluster mode is not available with Unity Catalog.</p>
<p>Under <strong>Advanced options</strong>, select from the following cluster security modes:</p>
<ul class="simple">
<li><p><strong>None</strong>: No isolation. Does not enforce workspace-local table access control or credential passthrough. Cannot access Unity Catalog data.</p></li>
<li><p><strong>Single User</strong>: Can be used only by a single user (by default, the user who created the cluster). Other users cannot attach to the cluster. When accessing a view from a cluster with <strong>Single User</strong> security mode, the view is executed with the user’s permissions. Single-user clusters support workloads using Python, Scala, and R. Init scripts, library installation, and DBFS mounts are supported on single-user clusters. Automated jobs should use single-user clusters.</p></li>
<li><p><strong>User Isolation</strong>: Can be shared by multiple users. Only SQL workloads are supported. Library installation, init scripts, and DBFS mounts are disabled to enforce strict isolation among the cluster users.</p></li>
<li><p><strong>Table ACL only (Legacy)</strong>: Enforces workspace-local table access control, but cannot access Unity Catalog data.</p></li>
<li><p><strong>Passthrough only (Legacy)</strong>: Enforces workspace-local credential passthrough, but cannot access Unity Catalog data.</p></li>
</ul>
<p>The only security modes supported for Unity Catalog workloads are <strong>Single User</strong> and <strong>User Isolation</strong>.</p>
<p>For more information, see <a class="reference internal" href="../../compute/configure.html#access-mode"><span class="std std-ref">Access modes</span></a>.</p>
</div>
<div class="section" id="aws-configurations">
<span id="cluster-size-and-autoscaling"></span><span id="cluster-aws-config"></span><h2>AWS configurations<a class="headerlink" href="#aws-configurations" title="Permalink to this headline"> </a></h2>
<p>When you configure a cluster’s AWS instances you can choose the availability zone, the max spot price, EBS volume type and size, and instance profiles. To specify configurations,</p>
<ol class="arabic">
<li><p>On the cluster configuration page, click the <strong>Advanced Options</strong> toggle.</p></li>
<li><p>At the bottom of the page, click the <strong>Instances</strong> tab.</p>
<div class="figure align-default">
<img alt="Instances tab" src="../../_images/instances-aws.png" />
</div>
</li>
</ol>
<div class="section" id="availability-zones">
<h3>Availability zones<a class="headerlink" href="#availability-zones" title="Permalink to this headline"> </a></h3>
<p>This setting lets you specify which availability zone (AZ) you want the cluster to use. By default, this setting is set to <strong>auto</strong> (Auto-AZ), where the AZ is automatically selected based on available IPs in the workspace subnets. Auto-AZ retries in other availability zones if AWS returns insufficient capacity errors.</p>
<p>Choosing a specific AZ for a cluster is useful primarily if your organization has purchased reserved instances in specific availability zones. Read more about <a class="reference external" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html">AWS availability zones</a>.</p>
</div>
<div class="section" id="spot-instances">
<span id="spot-instances-1"></span><h3>Spot instances<a class="headerlink" href="#spot-instances" title="Permalink to this headline"> </a></h3>
<p>You can specify whether to use spot instances and the max spot price to use when launching spot instances as a percentage of the corresponding on-demand price. By default, the max price is 100% of the on-demand price. See <a class="reference external" href="https://aws.amazon.com/ec2/spot/">AWS spot pricing</a>.</p>
</div>
<div class="section" id="ebs-volumes">
<h3>EBS volumes<a class="headerlink" href="#ebs-volumes" title="Permalink to this headline"> </a></h3>
<p>This section describes the default EBS volume settings for worker nodes, how to add shuffle volumes, and how to configure a cluster so that Databricks automatically allocates EBS volumes.</p>
<p>To configure EBS volumes, click the <strong>Instances</strong> tab in the cluster configuration and select an option in the <strong>EBS Volume Type</strong> drop-down list.</p>
<div class="section" id="default-ebs-volumes">
<h4>Default EBS volumes<a class="headerlink" href="#default-ebs-volumes" title="Permalink to this headline"> </a></h4>
<p>Databricks provisions EBS volumes for every worker node as follows:</p>
<ul class="simple">
<li><p>A 30 GB encrypted EBS instance root volume used only by the host operating system and Databricks internal services.</p></li>
<li><p>A 150 GB encrypted EBS container root volume used by the Spark worker. This hosts Spark services and logs.</p></li>
<li><p>(HIPAA only) a 75 GB encrypted EBS worker log volume that stores logs for Databricks internal services.</p></li>
</ul>
</div>
<div class="section" id="add-ebs-shuffle-volumes">
<span id="user-configurable-ebs-volumes"></span><h4>Add EBS shuffle volumes<a class="headerlink" href="#add-ebs-shuffle-volumes" title="Permalink to this headline"> </a></h4>
<p>To add shuffle volumes, select <strong>General Purpose SSD</strong> in the EBS Volume Type drop-down list:</p>
<div class="figure align-default">
<img alt="EBS volume type" src="../../_images/add-ebs.png" />
</div>
<p>By default, Spark shuffle outputs go to the instance local disk. For instance types that do not have a local disk, or if you want to increase your Spark shuffle storage space, you can specify additional EBS volumes.
This is particularly useful to prevent out of disk space errors when you run Spark jobs that produce large shuffle outputs.</p>
<p>Databricks encrypts these EBS volumes for both on-demand and spot instances. Read more about <a class="reference external" href="https://aws.amazon.com/ebs/features/">AWS EBS volumes</a>.</p>
</div>
<div class="section" id="optionally-encrypt-databricks-ebs-volumes-with-a-customer-managed-key">
<h4>Optionally encrypt Databricks EBS volumes with a customer-managed key<a class="headerlink" href="#optionally-encrypt-databricks-ebs-volumes-with-a-customer-managed-key" title="Permalink to this headline"> </a></h4>
<p>You can optionally encrypt cluster EBS volumes with a customer-managed key.</p>
<p>See <a class="reference internal" href="../../security/keys/customer-managed-keys-storage-aws.html"><span class="doc">Customer-managed keys for workspace storage</span></a></p>
</div>
<div class="section" id="aws-ebs-limits">
<h4>AWS EBS limits<a class="headerlink" href="#aws-ebs-limits" title="Permalink to this headline"> </a></h4>
<p>Ensure that your AWS EBS limits are high enough to satisfy the runtime requirements for all workers in all clusters.
For information on the default EBS limits and how to change them, see <a class="reference external" href="https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html#limits_ebs">Amazon Elastic Block Store (EBS) Limits</a>.</p>
</div>
<div class="section" id="aws-ebs-ssd-volume-type">
<h4>AWS EBS SSD volume type<a class="headerlink" href="#aws-ebs-ssd-volume-type" title="Permalink to this headline"> </a></h4>
<p>You can select either gp2 or gp3 for your AWS EBS SSD volume type. To do this, see <a class="reference internal" href="../../administration-guide/clusters/manage-ssd.html"><span class="doc">Manage SSD storage</span></a>. Databricks recommends you switch to gp3 for its cost savings compared to gp2. For technical information about gp2 and gp3, see <a class="reference external" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html">Amazon EBS volume types</a>.</p>
</div>
<div class="section" id="autoscaling-local-storage">
<h4>Autoscaling local storage<a class="headerlink" href="#autoscaling-local-storage" title="Permalink to this headline"> </a></h4>
<p>If you don’t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance’s local storage).</p>
<p>To configure autoscaling storage, select <strong>Enable autoscaling local storage</strong> in the Autopilot Options box:</p>
<div class="figure align-default">
<img alt="Enable autoscaling local storage" src="../../_images/autoscaling-local-storage.png" />
</div>
<p>The EBS volumes attached to an instance are detached only when the instance is returned to AWS. That is, EBS volumes are never detached from an instance as long as it is part of a running cluster. To scale down EBS usage, Databricks recommends using this feature in a cluster configured with <a class="reference internal" href="#autoscaling"><span class="std std-ref">Cluster size and autoscaling</span></a> or <a class="reference internal" href="../../compute/clusters-manage.html#automatic-termination"><span class="std std-ref">Unexpected termination</span></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Databricks uses Throughput Optimized HDD (st1) to extend the local storage of an instance. The <a class="reference external" href="https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html#limits_ebs">default AWS capacity limit</a> for these volumes is 20 TiB. To avoid hitting this limit, administrators should request an increase in this limit based on their usage requirements.</p>
</div>
<div class="admonition note" id="autoscaling-local-storage-iam-roles">
<p class="admonition-title">Note</p>
<p>If you created your Databricks account prior to version 2.44 (before Apr 27, 2017) and want to use autoscaling local storage (enabled by default in <a class="reference internal" href="#high-concurrency"><span class="std std-ref">High Concurrency clusters</span></a>), you must add volume permissions to the IAM role or keys used to create your account. In particular, you must add the permissions <code class="docutils literal notranslate"><span class="pre">ec2:AttachVolume</span></code>, <code class="docutils literal notranslate"><span class="pre">ec2:CreateVolume</span></code>, <code class="docutils literal notranslate"><span class="pre">ec2:DeleteVolume</span></code>, and <code class="docutils literal notranslate"><span class="pre">ec2:DescribeVolumes</span></code>. For the complete list of permissions and instructions on how to update your existing IAM role or keys, see <a class="reference internal" href="../../administration-guide/account-settings-e2/credentials.html"><span class="doc">Create an IAM role for workspace deployment</span></a>.</p>
</div>
</div>
</div>
<div class="section" id="instance-profiles">
<h3>Instance profiles<a class="headerlink" href="#instance-profiles" title="Permalink to this headline"> </a></h3>
<p>To securely access AWS resources without using AWS keys, you can launch Databricks clusters with instance profiles. See <a class="reference internal" href="../../connect/storage/tutorial-s3-instance-profile.html"><span class="doc">Tutorial: Configure S3 access with an instance profile</span></a> for information about how to create and configure instance profiles. Once you have created an instance profile, you select it in the Instance Profile drop-down list:</p>
<div class="figure align-default">
<img alt="Instance profile" src="../../_images/create-cluster-with-instance-profile.png" />
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Once a cluster launches with an instance profile, anyone who has attach permissions to this cluster can access the underlying resources controlled by this role. To guard against unwanted access, you can use <a class="reference internal" href="../../security/auth-authz/access-control/cluster-acl.html"><span class="doc">Cluster access control</span></a> to restrict permissions to the cluster.</p>
</div>
</div>
</div>
<div class="section" id="spark-configuration">
<h2>Spark configuration<a class="headerlink" href="#spark-configuration" title="Permalink to this headline"> </a></h2>
<p>To fine tune Spark jobs, you can provide custom <a class="reference external" href="https://spark.apache.org/docs/latest/configuration.html">Spark configuration properties</a> in a cluster configuration.</p>
<ol class="arabic">
<li><p>On the cluster configuration page, click the <strong>Advanced Options</strong> toggle.</p></li>
<li><p>Click the <strong>Spark</strong> tab.</p>
<div class="figure align-default">
<img alt="Spark configuration" src="../../_images/spark-config-aws.png" />
</div>
<p>In <strong>Spark config</strong>, enter the configuration properties as one key-value pair per line.</p>
</li>
</ol>
<p>When you configure a cluster using the <a class="reference external" href="https://docs.databricks.com/api/workspace/clusters">Cluster API</a>, set Spark properties in the <code class="docutils literal notranslate"><span class="pre">spark_conf</span></code> field in the <a class="reference external" href="https://docs.databricks.com/api/workspace/clusters/create">Create new cluster API</a> or <a class="reference external" href="https://docs.databricks.com/api/workspace/clusters/edit">Update cluster configuration API</a>.</p>
<p>Databricks does not recommend using global init scripts.</p>
<p>To set Spark properties for all clusters, create a <a class="reference internal" href="../../init-scripts/global.html"><span class="doc">global init script</span></a>:</p>
<div class="highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="n">dbutils</span><span class="p">.</span><span class="n">fs</span><span class="p">.</span><span class="n">put</span><span class="p">(</span><span class="s">&quot;dbfs:/databricks/init/set_spark_params.sh&quot;</span><span class="p">,</span><span class="s">&quot;&quot;&quot;</span>
<span class="s">  |#!/bin/bash</span>
<span class="s">  |</span>
<span class="s">  |cat &lt;&lt; &#39;EOF&#39; &gt; /databricks/driver/conf/00-custom-spark-driver-defaults.conf</span>
<span class="s">  |[driver] {</span>
<span class="s">  |  &quot;spark.sql.sources.partitionOverwriteMode&quot; = &quot;DYNAMIC&quot;</span>
<span class="s">  |}</span>
<span class="s">  |EOF</span>
<span class="s">  &quot;&quot;&quot;</span><span class="p">.</span><span class="n">stripMargin</span><span class="p">,</span><span class="w"> </span><span class="kc">true</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="retrieve-a-spark-configuration-property-from-a-secret">
<h2>Retrieve a Spark configuration property from a secret<a class="headerlink" href="#retrieve-a-spark-configuration-property-from-a-secret" title="Permalink to this headline"> </a></h2>
<p>Databricks recommends storing sensitive information, such as passwords, in a <a class="reference internal" href="../../security/secrets/secrets.html"><span class="doc">secret</span></a> instead of plaintext. To reference a secret in the Spark configuration, use the following syntax:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="na">spark.&lt;property-name&gt; {{secrets/&lt;scope-name&gt;/&lt;secret-name&gt;}}</span>
</pre></div>
</div>
<p>For example, to set a Spark configuration property called <code class="docutils literal notranslate"><span class="pre">password</span></code> to the value of the secret stored in <code class="docutils literal notranslate"><span class="pre">secrets/acme_app/password</span></code>:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="na">spark.password {{secrets/acme-app/password}}</span>
</pre></div>
</div>
<p>For more information, see <a class="reference internal" href="../../security/secrets/secrets.html#path-value"><span class="std std-ref">Syntax for referencing secrets in a Spark configuration property or environment variable</span></a>.</p>
</div>
<div class="section" id="environment-variables">
<span id="env-var"></span><h2>Environment variables<a class="headerlink" href="#environment-variables" title="Permalink to this headline"> </a></h2>
<p>You can configure custom environment variables that you can access from <a class="reference internal" href="../../init-scripts/index.html"><span class="doc">init scripts</span></a> running on a cluster. Databricks also provides predefined <a class="reference internal" href="../../init-scripts/environment-variables.html"><span class="doc">environment variables</span></a> that you can use in init scripts. You cannot override these predefined environment variables.</p>
<ol class="arabic">
<li><p>On the cluster configuration page, click the <strong>Advanced Options</strong> toggle.</p></li>
<li><p>Click the <strong>Spark</strong> tab.</p></li>
<li><p>Set the environment variables in the <strong>Environment Variables</strong> field.</p>
<div class="figure align-default">
<img alt="Environment Variables field" src="../../_images/environment-variables.png" />
</div>
</li>
</ol>
<p>You can also set environment variables using the  <code class="docutils literal notranslate"><span class="pre">spark_env_vars</span></code> field in the <a class="reference external" href="https://docs.databricks.com/api/workspace/clusters/create">Create new cluster API</a> or <a class="reference external" href="https://docs.databricks.com/api/workspace/clusters/edit">Update cluster configuration API</a>.</p>
</div>
<div class="section" id="cluster-tags">
<span id="tags"></span><h2>Cluster tags<a class="headerlink" href="#cluster-tags" title="Permalink to this headline"> </a></h2>
<p>Cluster tags allow you to easily monitor the cost of cloud resources used by various groups in your organization. You can specify tags as key-value pairs when you create a cluster, and Databricks applies these tags to cloud resources like VMs and disk volumes, as well as <a class="reference internal" href="../../administration-guide/account-settings/usage-detail-tags.html"><span class="doc">DBU usage reports</span></a>.</p>
<p>For clusters launched from pools, the custom cluster tags are only applied to DBU usage reports and do not propagate to cloud resources.</p>
<p>For detailed information about how pool and cluster tag types work together, see <a class="reference internal" href="../../administration-guide/account-settings/usage-detail-tags.html"><span class="doc">Monitor usage using cluster, pool, and workspace tags</span></a>.</p>
<p>For convenience, Databricks applies four default tags to each cluster: <code class="docutils literal notranslate"><span class="pre">Vendor</span></code>, <code class="docutils literal notranslate"><span class="pre">Creator</span></code>, <code class="docutils literal notranslate"><span class="pre">ClusterName</span></code>, and <code class="docutils literal notranslate"><span class="pre">ClusterId</span></code>.</p>
<p>In addition, on job clusters, Databricks applies two default tags: <code class="docutils literal notranslate"><span class="pre">RunName</span></code> and <code class="docutils literal notranslate"><span class="pre">JobId</span></code>.</p>
<p>On resources used by Databricks SQL, Databricks also applies the default tag <code class="docutils literal notranslate"><span class="pre">SqlWarehouseId</span></code>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Do not assign a custom tag with the key <code class="docutils literal notranslate"><span class="pre">Name</span></code> to a cluster. Every cluster has a tag <code class="docutils literal notranslate"><span class="pre">Name</span></code> whose value is set by Databricks. If you change the value associated with the key <code class="docutils literal notranslate"><span class="pre">Name</span></code>, the cluster can no longer be tracked by Databricks. As a consequence, the cluster might not be terminated after becoming idle and will continue to incur usage costs.</p>
</div>
<p>You can add custom tags when you create a cluster. To configure cluster tags:</p>
<ol class="arabic">
<li><p>On the cluster configuration page, click the <strong>Advanced Options</strong> toggle.</p></li>
<li><p>At the bottom of the page, click the <strong>Tags</strong> tab.</p>
<div class="figure align-default">
<img alt="Tags tab" src="../../_images/tags.png" />
</div>
</li>
<li><p>Add a key-value pair for each custom tag. You can add up to 45 custom tags.</p></li>
</ol>
<div class="section" id="enforce-mandatory-tags">
<h3>Enforce mandatory tags<a class="headerlink" href="#enforce-mandatory-tags" title="Permalink to this headline"> </a></h3>
<p>To ensure that certain tags are always populated when clusters are created, you can apply a specific IAM policy to your account’s primary IAM role (the one created during account setup; contact your AWS administrator if you need access). The IAM policy should include explicit <a class="reference external" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html#AccessPolicyLanguage_Interplay">Deny statements</a> for mandatory tag keys and optional values. <em>Cluster creation will fail</em> if required tags with one of the allowed values aren’t provided.</p>
<p>For example, if you want to enforce <code class="docutils literal notranslate"><span class="pre">Department</span></code> and <code class="docutils literal notranslate"><span class="pre">Project</span></code> tags, with only specified values allowed for the former and a free-form non-empty value for the latter, you could apply an IAM policy like this one:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;Version&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2012-10-17&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;Statement&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;Sid&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;MandateLaunchWithTag1&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;Effect&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Deny&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;Action&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="s2">&quot;ec2:RunInstances&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;ec2:CreateTags&quot;</span>
<span class="w">      </span><span class="p">],</span>
<span class="w">      </span><span class="nt">&quot;Resource&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;arn:aws:ec2:region:accountId:instance/*&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;Condition&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;StringNotEqualsIgnoreCase&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">          </span><span class="nt">&quot;aws:RequestTag/Department&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">              </span><span class="s2">&quot;Deptt1&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;Deptt2&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;Deptt3&quot;</span>
<span class="w">          </span><span class="p">]</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;Sid&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;MandateLaunchWithTag2&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;Effect&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Deny&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;Action&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="s2">&quot;ec2:RunInstances&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;ec2:CreateTags&quot;</span>
<span class="w">      </span><span class="p">],</span>
<span class="w">      </span><span class="nt">&quot;Resource&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;arn:aws:ec2:region:accountId:instance/*&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;Condition&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;StringNotLike&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">          </span><span class="nt">&quot;aws:RequestTag/Project&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;?*&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Both <code class="docutils literal notranslate"><span class="pre">ec2:RunInstances</span></code> and <code class="docutils literal notranslate"><span class="pre">ec2:CreateTags</span></code> actions are required for each tag for effective coverage of scenarios in which there are clusters that have only on-demand instances, only spot instances, or both.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Databricks recommends that you add a separate policy statement for each tag. The overall policy might become long, but it is easier to debug. See the <a class="reference external" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_condition_operators.html">IAM Policy Condition Operators Reference</a> for a list of operators that can be used in a policy.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Cluster creation errors due to an IAM policy show an <code class="docutils literal notranslate"><span class="pre">encoded</span> <span class="pre">error</span> <span class="pre">message</span></code>, starting with:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">Cloud Provider Launch Failure: A cloud provider error was encountered while setting up the cluster.</span>
</pre></div>
</div>
<p>The message is encoded because the details of the authorization status can constitute privileged information that the user who requested the action should not see. See <a class="reference external" href="https://docs.aws.amazon.com/STS/latest/APIReference/API_DecodeAuthorizationMessage.html">DecodeAuthorizationMessage API</a> (or <a class="reference external" href="https://docs.aws.amazon.com/cli/latest/reference/sts/decode-authorization-message.html">CLI</a>) for information about how to decode such messages.</p>
</div>
</div>
</div>
<div class="section" id="ssh-access-to-clusters">
<span id="ssh-access"></span><h2>SSH access to clusters<a class="headerlink" href="#ssh-access-to-clusters" title="Permalink to this headline"> </a></h2>
<p></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You cannot use SSH to log into a cluster that has <a class="reference internal" href="../../security/network/classic/secure-cluster-connectivity.html"><span class="doc">secure cluster connectivity</span></a> enabled.</p>
</div>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Secure_Shell">SSH</a> allows you to log into Apache Spark clusters remotely for advanced troubleshooting and installing custom software.</p>
<p>For a related feature, see <a class="reference internal" href="../../compute/web-terminal.html"><span class="doc">Web terminal</span></a>.</p>
<p>This section describes how to configure your AWS account to enable ingress access to your cluster with your public key, and how to open an SSH connection to cluster nodes.</p>
<p></p>
<div class="section" id="configure-security-group">
<h3>Configure security group<a class="headerlink" href="#configure-security-group" title="Permalink to this headline"> </a></h3>
<p>You must update the Databricks security group in your AWS account to give ingress access to the IP address from which you will initiate the SSH connection. You can set this for a single IP address or provide a range that represents your entire office IP range.</p>
<ol class="arabic">
<li><p>In your AWS console, find the Databricks security group. It will have a label similar to <code class="docutils literal notranslate"><span class="pre">&lt;databricks-instance&gt;-worker-unmanaged</span></code>. (Example: <code class="docutils literal notranslate"><span class="pre">dbc-fb3asdddd3-worker-unmanaged</span></code>)</p></li>
<li><p>Edit the security group and add an inbound TCP rule to allow port <code class="docutils literal notranslate"><span class="pre">2200</span></code> to worker machines. It can be a single IP address or a range.</p>
<div class="figure align-default">
<img alt="SSH security group" src="../../_images/ssh-security-group.png" />
</div>
</li>
<li><p>Make sure that your computer and office allow you to send TCP traffic on port <code class="docutils literal notranslate"><span class="pre">2200</span></code>.</p></li>
</ol>
</div>
<div class="section" id="generate-ssh-key-pair">
<h3>Generate SSH key pair<a class="headerlink" href="#generate-ssh-key-pair" title="Permalink to this headline"> </a></h3>
<p>Create an SSH key pair by running this command in a terminal session:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ssh-keygen<span class="w"> </span>-t<span class="w"> </span>rsa<span class="w"> </span>-b<span class="w"> </span><span class="m">4096</span><span class="w"> </span>-C<span class="w"> </span><span class="s2">&quot;email@example.com&quot;</span>
</pre></div>
</div>
<p>You must provide the path to the directory where you want to save the public and private key. The public key is saved with the extension <code class="docutils literal notranslate"><span class="pre">.pub</span></code>.</p>
</div>
<div class="section" id="configure-a-new-cluster-with-your-public-key">
<h3>Configure a new cluster with your public key<a class="headerlink" href="#configure-a-new-cluster-with-your-public-key" title="Permalink to this headline"> </a></h3>
<ol class="arabic">
<li><p>Copy the entire contents of the public key file.</p></li>
<li><p>On the cluster configuration page, click the <strong>Advanced Options</strong> toggle.</p></li>
<li><p>At the bottom of the page, click the <strong>SSH</strong> tab.</p></li>
<li><p>Paste the key you copied into the <strong>SSH Public Key</strong> field.</p>
<div class="figure align-default">
<img alt="SSH input" src="../../_images/ssh-input-aws.png" />
</div>
</li>
</ol>
</div>
<div class="section" id="configure-an-existing-cluster-with-your-public-key">
<h3>Configure an existing cluster with your public key<a class="headerlink" href="#configure-an-existing-cluster-with-your-public-key" title="Permalink to this headline"> </a></h3>
<p>If you have a cluster and didn’t provide the public key during cluster creation, you can inject the public key by running this code from any notebook attached to the cluster:</p>
<div class="highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="kd">val</span><span class="w"> </span><span class="n">publicKey</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot; put your public key here &quot;</span>

<span class="k">def</span><span class="w"> </span><span class="nf">addAuthorizedPublicKey</span><span class="p">(</span><span class="n">key</span><span class="p">:</span><span class="w"> </span><span class="nc">String</span><span class="p">):</span><span class="w"> </span><span class="nc">Unit</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="kd">val</span><span class="w"> </span><span class="n">fw</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">java</span><span class="p">.</span><span class="n">io</span><span class="p">.</span><span class="nc">FileWriter</span><span class="p">(</span><span class="s">&quot;/home/ubuntu/.ssh/authorized_keys&quot;</span><span class="p">,</span><span class="w"> </span><span class="cm">/* append */</span><span class="w"> </span><span class="kc">true</span><span class="p">)</span>
<span class="w">  </span><span class="n">fw</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">&quot;\n&quot;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">key</span><span class="p">)</span>
<span class="w">  </span><span class="n">fw</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>
<span class="p">}</span>

<span class="kd">val</span><span class="w"> </span><span class="n">numExecutors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sc</span><span class="p">.</span><span class="n">getExecutorMemoryStatus</span><span class="p">.</span><span class="n">keys</span><span class="p">.</span><span class="n">size</span>
<span class="n">sc</span><span class="p">.</span><span class="n">parallelize</span><span class="p">(</span><span class="mi">0</span><span class="w"> </span><span class="n">until</span><span class="w"> </span><span class="n">numExecutors</span><span class="p">,</span><span class="w"> </span><span class="n">numExecutors</span><span class="p">).</span><span class="n">foreach</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=&gt;</span>
<span class="w">  </span><span class="n">addAuthorizedPublicKey</span><span class="p">(</span><span class="n">publicKey</span><span class="p">)</span>
<span class="p">}</span>
<span class="n">addAuthorizedPublicKey</span><span class="p">(</span><span class="n">publicKey</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="ssh-into-the-spark-driver-node">
<h3>SSH into the Spark driver node<a class="headerlink" href="#ssh-into-the-spark-driver-node" title="Permalink to this headline"> </a></h3>
<ol class="arabic">
<li><p>On the cluster configuration page, click the <strong>Advanced Options</strong> toggle.</p></li>
<li><p>Click the <strong>SSH</strong> tab. Copy the driver node hostname.</p></li>
<li><p>Run the following command, replacing the hostname and private key file path.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ssh<span class="w"> </span>ubuntu@&lt;hostname&gt;<span class="w"> </span>-p<span class="w"> </span><span class="m">2200</span><span class="w"> </span>-i<span class="w"> </span>&lt;private-key-file-path&gt;
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="ssh-into-spark-worker-nodes">
<h3>SSH into Spark worker nodes<a class="headerlink" href="#ssh-into-spark-worker-nodes" title="Permalink to this headline"> </a></h3>
<p>You SSH into worker nodes the same way that you SSH into the driver node.</p>
<ol class="arabic">
<li><p>On the cluster details page, click the <strong>Spark Cluster UI - Master</strong> tab.</p></li>
<li><p>In the Workers table, click the worker that you want to SSH into. Copy the Hostname field.</p>
<div class="figure align-default">
<img alt="SSH hostname" src="../../_images/hostname.png" />
</div>
</li>
</ol>
</div>
</div>
<div class="section" id="cluster-log-delivery">
<h2>Cluster log delivery<a class="headerlink" href="#cluster-log-delivery" title="Permalink to this headline"> </a></h2>
<p>When you create a cluster, you can specify a location to deliver the logs for the Spark driver node, worker nodes, and events. Logs are delivered every five minutes to your chosen destination. When a cluster is terminated, Databricks guarantees to deliver all logs generated up until the cluster was terminated.</p>
<p>The destination of the logs depends on the cluster ID. If the specified destination is
<code class="docutils literal notranslate"><span class="pre">dbfs:/cluster-log-delivery</span></code>, cluster logs for <code class="docutils literal notranslate"><span class="pre">0630-191345-leap375</span></code> are delivered to
<code class="docutils literal notranslate"><span class="pre">dbfs:/cluster-log-delivery/0630-191345-leap375</span></code>.</p>
<p>To configure the log delivery location:</p>
<ol class="arabic">
<li><p>On the cluster configuration page, click the <strong>Advanced Options</strong> toggle.</p></li>
<li><p>Click the <strong>Logging</strong> tab.</p>
<div class="figure align-default">
<img alt="Cluster log delivery" src="../../_images/log-delivery-aws.png" />
</div>
<p></p>
</li>
<li><p>Select a destination type.</p></li>
<li><p>Enter the cluster log path.</p></li>
</ol>
<div class="section" id="s3-bucket-destinations">
<h3>S3 bucket destinations<a class="headerlink" href="#s3-bucket-destinations" title="Permalink to this headline"> </a></h3>
<p>If you choose an S3 destination, you must configure the cluster with an instance profile that can access the bucket.
This instance profile must have both the <code class="docutils literal notranslate"><span class="pre">PutObject</span></code> and <code class="docutils literal notranslate"><span class="pre">PutObjectAcl</span></code> permissions. An example instance profile
has been included for your convenience. See <a class="reference internal" href="../../connect/storage/tutorial-s3-instance-profile.html"><span class="doc">Tutorial: Configure S3 access with an instance profile</span></a> for instructions on how to set up an instance profile.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;Version&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2012-10-17&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;Statement&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;Effect&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Allow&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;Action&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="s2">&quot;s3:ListBucket&quot;</span>
<span class="w">      </span><span class="p">],</span>
<span class="w">      </span><span class="nt">&quot;Resource&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="s2">&quot;arn:aws:s3:::&lt;my-s3-bucket&gt;&quot;</span>
<span class="w">      </span><span class="p">]</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;Effect&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Allow&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;Action&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="s2">&quot;s3:PutObject&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;s3:PutObjectAcl&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;s3:GetObject&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;s3:DeleteObject&quot;</span>
<span class="w">      </span><span class="p">],</span>
<span class="w">      </span><span class="nt">&quot;Resource&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="s2">&quot;arn:aws:s3:::&lt;my-s3-bucket&gt;/*&quot;</span>
<span class="w">      </span><span class="p">]</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This feature is also available in the REST API. See the <a class="reference external" href="https://docs.databricks.com/api/workspace/clusters">Clusters API</a>.</p>
</div>
</div>
<div class="section" id="init-scripts">
<h2>Init scripts<a class="headerlink" href="#init-scripts" title="Permalink to this headline"> </a></h2>
<p>A cluster node initialization—or init—script is a shell script that runs during startup for each cluster node <em>before</em> the Spark driver or worker JVM starts. You can use init scripts to install packages and libraries not included in the Databricks runtime, modify the JVM system classpath, set system properties and environment variables used by the JVM, or modify Spark configuration parameters, among other configuration tasks.</p>
<p>You can attach init scripts to a cluster by expanding the <strong>Advanced Options</strong> section and clicking the <strong>Init Scripts</strong> tab.</p>
<p>For detailed instructions, see <a class="reference internal" href="../../init-scripts/index.html"><span class="doc">What are init scripts?</span></a>.</p>
</div>
</div>


    
          </div>
        </div>
        <div  class="suapp-rating">
  <div id="suPageRateApp">
     <su-app></su-app>
   </div> 
 </div>
<hr> 
<footer>
  <div role="contentinfo">
      <p class="copyright">
          &copy; Databricks 2023. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the <a href="http://www.apache.org/">Apache Software Foundation</a>.
      </p>
      <p> 
        
          <a id='feedbacklink' href="mailto:doc-feedback@databricks.com?subject=Documentation Feedback">Send us feedback</a>
        
     | <a href="https://databricks.com/privacy-policy">Privacy Policy</a> | <a href="https://databricks.com/terms-of-use">Terms of Use</a></p>

  </div> 

</footer>
      </div>
    </div>
  </section>
</main>

  </page>
  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT: '../../',
      VERSION: '1.0',
      COLLAPSE_INDEX: false,
      FILE_SUFFIX: '.html',
      HAS_SOURCE: 'false'
    };
  </script>
  <script type="text/javascript" src="../../_static/jquery.js"></script>
  <script type="text/javascript" src="../../_static/underscore.js"></script>
  <script type="text/javascript" src="../../_static/doctools.js"></script>
  <script type="text/javascript" src="../../_static/language_data.js"></script>
  

  <script type="text/javascript" src="../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../_static/js/jquery.waypoints.min.js"></script>

  <!-- Select2 (https://select2.org/) -->
  <link href="https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/css/select2.min.css" rel="stylesheet" />
  <script src="https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/js/select2.min.js"></script>
  <!-- End Select2 -->

  
  
  <script type="text/javascript" src="../../_static/js/localized.js"></script>
  <script type="text/javascript" src="../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  
 



  <script>
  window.__searchunifyLoaderConfig = JSON.parse('{"clients": {"en": "02c2e804-27e9-11ee-aefb-0242ac120011", "ja": "6a42c3f2-2820-11ee-aefb-0242ac120011", "pt": "6a86badd-2821-11ee-aefb-0242ac120011"}}')
</script>
<script type="text/javascript" src="../../_static/js/search-loader.js"></script>
</body>
<script type='text/javascript'>
  window.onload = function () {
    var description = document.querySelector('meta[name="description"]').getAttribute("content");
    let titleText = document.querySelector('h1').textContent;
    document.querySelector('meta[property="og:title"]').setAttribute("content", titleText);
    document.querySelector('meta[property="og:description"]').setAttribute("content", description);
    document.querySelector('meta[property="twitter:description"]').setAttribute("content", description);
  };
</script>

</html>