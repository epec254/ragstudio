

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en-US" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en-US"> <!--<![endif]-->

<head>
  <!-- cookie consent -->
  
    <!-- Combined Onetrust and Rudderstack Implementation Scripts -->
    <!-- Onetrust Initialization -->
    <script type="text/javascript" src="https://cdn.cookielaw.org/consent/92466579-1717-44d3-809d-a05fb02843ed-test/OtAutoBlock.js"></script>
    <script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="92466579-1717-44d3-809d-a05fb02843ed-test"></script>
    <link rel="stylesheet" id="db-onetrust-style" href="https://www.databricks.com/wp-content/uploads/db_onetrust.css" media="all" />
    <!-- Setting Rudderstack Write Key -->
    <script>window.rudderstackKey = "2SOR9fvSr5Fi6tN2ihPbVHnX1SZ" </script>
    <!-- Rudderstack Initialization + Onetrust Integration + Rudderstack Custom Events -->
    <script type="text/javascript" src="https://www.databricks.com/sites/default/files/rudderstack/v1/db-rudderstack-events.js"></script>

  <!-- cookie consent -->

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="X-UA-Compatible" content="IE=9" />
  <meta content="Learn how to read and write data to Azure Synapse Analytics using Databricks." name="description" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0">
  <meta property="og:image" content="https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:title" content="Query data in Azure Synapse Analytics">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://docs.databricks.com">
  <meta property="og:description" content="" id="og-description">
  <meta name="twitter:image" content="https://www.databricks.com/wp-content/uploads/2020/04/og-databricks.png">
  <meta name="twitter:site" content="@databricks">
  <meta name="twitter:creator" content="@databricks">
  <meta property="twitter:description" content="">
  
  <title>Query data in Azure Synapse Analytics &#124; Databricks on AWS</title>
  
  
  <link rel="canonical" href="https://docs.databricks.com/en/connect/external-systems/synapse-analytics.html">
  <!-- Start hreflang tag -->
  <link rel="alternate" hreflang="en" href="https://docs.databricks.com/en/connect/external-systems/synapse-analytics.html" />
<link rel="alternate" hreflang="x-default" href="https://docs.databricks.com/en/connect/external-systems/synapse-analytics.html" />
  <!-- End hreflang tag -->
  
  
  <link rel="shortcut icon" href="../../_static/favicon.ico" />
  

  

  

  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;
j.setAttributeNode(d.createAttribute('data-ot-ignore'));
f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-T85FQ33');</script>
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;
j.setAttributeNode(d.createAttribute('data-ot-ignore'));
f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-TWTKQQ');</script>
    
  <!-- End Google Tag Manager -->


  <!-- MaxMind / GEO IP -->
  <script src="//js.maxmind.com/js/apis/geoip2/v2.1/geoip2.js" type="text/javascript"></script>
  <!-- End MaxMind / GEO IP -->

  
  
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600&display=swap" rel="stylesheet">
  <link rel="preload" href="../../_static/fonts/DMSans-Bold.ttf" as="font">
  <link rel="preload" href="../../_static/fonts/DMSans-Regular.ttf" as="font">
  <link rel="preload" href="../../_static/fonts/DMMono-Regular.ttf" as="font">
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/cloud-provider-selector.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/translation-selector.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/searchunify/main.css" type="text/css" />

  
  <link rel="index" title="Index" href="../../genindex.html" />
  <link rel="search" title="Search" href="../../search.html" />
  <link rel="top" title="Databricks on AWS" href="../../index.html" /> 
</head>

<body class="wy-body-for-nav" role="document">

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T85FQ33"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TWTKQQ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  
  <nav class="wy-nav-top header su_header" role="navigation" aria-label="top navigation">
    
<nav class="wy-nav-top header su_header" role="navigation" aria-label="top navigation">
  <div class="container-logo">
    <ul class="mobile-menu-toggle">
        <li class="menu-toggle">
            <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
            
            <a href="https://www.databricks.com/" class="wy-nav-top-logo"><img src="../../_static/small-scale-lockup-full-color-rgb.svg" width="137" height="21"
              alt="Databricks" /></a>   
               
              </li>
    </ul>
    <ul class="su_nav-menu">
      <li class="menu-toggle">
        <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
        
          
        
        <a href="https://www.databricks.com/" class="wy-nav-top-logo"><img src="../../_static/small-scale-lockup-full-color-rgb.svg" width="137" height="21"
            alt="Databricks" /></a></li>
        <!-- 
<li><a href="https://help.databricks.com/s/">Help Center</a></li>
<li class="active"><a href="https://docs.databricks.com/en/">Documentation</a></li>
<li><a href="https://kb.databricks.com/">Knowledge Base</a></li>
 -->
    </ul>
  </div>
  <div class="su_nav-right">
    <ul class="su_link-mobile">
  <!-- Mobile header code can go here -->
</ul>
<ul class="right-try-list">
   
</ul>
  </div>
</nav>
  </nav>

  <div class="su_sub-header">
    <div class="container">
      <div class="su_sub-header-inner">
        <!-- <div class="su_subnav-menu-right">
  <div id="auto" style="width: 100%;">
    <div ng-controller="SearchautoController">
      <div bind-html-compile="autocompleteHtml">
        <form class="su__search-box-1" disabled="disabled">
          <input class="su__search-input" type="search" name="Search box" id="su__search-b" placeholder="Search Documentation" disabled="disabled"/>
          <button class="su__search-button" type="submit" class="button button-success" disabled="disabled">
            <svg width="24" height="24" viewBox="0 0 24 24">
              <path
                d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"
                fill="#333"></path>
            </svg>
          </button>
        </form>
      </div>
    </div>
  </div>
</div> -->
        <div class="search-lng-gap"></div>
        <div style="margin-left: 16px; margin-right: 16px;">
          <!-- <select name="lng selector" id="lng-selector">
    <option value="../../../en/connect/external-systems/synapse-analytics.html" class="notranslate">English</option>
    <option value="../../../ja/connect/external-systems/synapse-analytics.html" class="notranslate">日本語</option>
    <option value="../../../pt/connect/external-systems/synapse-analytics.html" class="notranslate">Português (Brasil)</option>
</select> -->
        </div>
        <div class="cloud-selector-container">
          <!-- <select name="cloud provider selector" id="cloud-provider-selector">
    <option value="aws" selected class="notranslate">
        Amazon Web Services
    </option>
    <option value="azure"  class="notranslate">
        Microsoft Azure
    </option>
    <option value="gcp"  class="notranslate">
        Google Cloud Platform
    </option>
</select> -->
        </div>
      </div>
    </div>
  </div>
  <page class="js-page-container">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side su_nav-side">
<div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    

    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../../index.html" class="main-navigation-home">Databricks on AWS</a>
    

    
      

      
        <p class="caption"><span class="caption-text">Load &amp; manage data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../rag-temp/index.html">RAG Studio</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    
  <p class="build_info notranslate"data-last-edit="December 23, 2023">
    Updated Jan 11, 2024
  </p>
<script>
  window.addEventListener('DOMContentLoaded',function(){
    var h1=document.querySelector('h1');
    var bi=document.querySelector('[data-last-edit]');
    if(h1 && bi){
      var ver = document.createElement('p');
      ver.className = 'version_info';
      ver.textContent = bi.getAttribute('data-last-edit');
      h1.parentElement.insertBefore(ver, h1.nextElementSibling);
    }
  });
</script>

    <p>
      
        <a id='feedbacklink' href="mailto:doc-feedback@databricks.com?subject=Documentation Feedback">Send us feedback</a>
      
    </p>
  </div>
</div>
</nav>
    
    
<main class="wy-grid-for-nav su_nav-grid">
  <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
    <div class="wy-nav-content su__nav_content">
      <div class="rst-content">
        





<div role="navigation" aria-label="breadcrumbs navigation" class="wy-breadcrumbs-wrapper">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>Query data in Azure Synapse Analytics</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>
</div>
        
        <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
          <div itemprop="articleBody">
            
    
  <div class="section" id="query-data-in-azure-synapse-analytics">
<h1>Query data in Azure Synapse Analytics<a class="headerlink" href="#query-data-in-azure-synapse-analytics" title="Permalink to this headline"> </a></h1>
<p>You can access Azure Synapse from Databricks using the Azure Synapse connector, which uses the <code class="docutils literal notranslate"><span class="pre">COPY</span></code> statement in Azure Synapse to transfer large volumes of data efficiently between a Databricks cluster and an Azure Synapse instance using an Azure Data Lake Storage Gen2 storage account for temporary staging.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You may prefer Lakehouse Federation for managing queries on Azure Synapse or Azure Data Warehouse data. See <a class="reference internal" href="../../query-federation/index.html"><span class="doc">Run queries using Lakehouse Federation</span></a>.</p>
</div>
<p><a class="reference external" href="https://azure.microsoft.com/services/synapse-analytics/">Azure Synapse Analytics</a> is a cloud-based enterprise data warehouse that leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>This connector is for use with Synapse Dedicated Pool instances only and is not compatible with other Synapse components.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">COPY</span></code> is available only on Azure Data Lake Storage Gen2 instances. If you’re looking for details on working with Polybase, see <a class="reference internal" href="../../archive/azure/synapse-polybase.html"><span class="doc">Connecting Databricks and Azure Synapse with PolyBase (legacy)</span></a>.</p>
</div>
<div class="section" id="example-syntax-for-synapse">
<h2>Example syntax for Synapse<a class="headerlink" href="#example-syntax-for-synapse" title="Permalink to this headline"> </a></h2>
<p>You can query Synapse in Scala, Python, SQL, and R. The following code examples use storage account keys and forward the storage credentials from Databricks to Synapse.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Use the connection string provided by Azure portal, which enables Secure Sockets Layer (SSL) encryption for all data sent between the Spark driver and the Azure Synapse instance through the JDBC connection. To verify that the SSL encryption is enabled, you can search for <code class="docutils literal notranslate"><span class="pre">encrypt=true</span></code> in the connection string.</p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><a class="reference internal" href="../unity-catalog/external-locations.html"><span class="doc">External locations defined in Unity Catalog</span></a> are not supported as <code class="docutils literal notranslate"><span class="pre">tempDir</span></code> locations.</p>
</div>
<div class="js-code-language-tabs js-code-language-tabs--literal compound">
<div class="compound-first highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="c1">// Set up the storage account access key in the notebook session conf.</span>
<span class="n">spark</span><span class="p">.</span><span class="n">conf</span><span class="p">.</span><span class="n">set</span><span class="p">(</span>
<span class="w">  </span><span class="s">&quot;fs.azure.account.key.&lt;your-storage-account-name&gt;.dfs.core.windows.net&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="s">&quot;&lt;your-storage-account-access-key&gt;&quot;</span><span class="p">)</span>

<span class="c1">// Get some data from an Azure Synapse table. The following example applies to Databricks Runtime 11.3 LTS and above.</span>
<span class="kd">val</span><span class="w"> </span><span class="n">df</span><span class="p">:</span><span class="w"> </span><span class="nc">DataFrame</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">spark</span><span class="p">.</span><span class="n">read</span>
<span class="w">  </span><span class="p">.</span><span class="n">format</span><span class="p">(</span><span class="s">&quot;sqldw&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;host&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;hostname&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;port&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;port&quot;</span><span class="p">)</span><span class="w"> </span><span class="cm">/* Optional - will use default port 1433 if not specified. */</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;user&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;username&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;password&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;password&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;database&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;database-name&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;dbtable&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;schema-name.table-name&quot;</span><span class="p">)</span><span class="w"> </span><span class="cm">/* If schemaName not provided, default to &quot;dbo&quot;. */</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;tempDir&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;forwardSparkAzureStorageCredentials&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;true&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">load</span><span class="p">()</span>

<span class="c1">// Get some data from an Azure Synapse table. The following example applies to Databricks Runtime 10.4 LTS and below.</span>
<span class="kd">val</span><span class="w"> </span><span class="n">df</span><span class="p">:</span><span class="w"> </span><span class="nc">DataFrame</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">spark</span><span class="p">.</span><span class="n">read</span>
<span class="w">  </span><span class="p">.</span><span class="n">format</span><span class="p">(</span><span class="s">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;url&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;tempDir&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;forwardSparkAzureStorageCredentials&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;true&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;dbTable&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;your-table-name&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">load</span><span class="p">()</span>

<span class="c1">// Load data from an Azure Synapse query.</span>
<span class="kd">val</span><span class="w"> </span><span class="n">df</span><span class="p">:</span><span class="w"> </span><span class="nc">DataFrame</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">spark</span><span class="p">.</span><span class="n">read</span>
<span class="w">  </span><span class="p">.</span><span class="n">format</span><span class="p">(</span><span class="s">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;url&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;tempDir&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;forwardSparkAzureStorageCredentials&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;true&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;query&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;select x, count(*) as cnt from table group by x&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">load</span><span class="p">()</span>

<span class="c1">// Apply some transformations to the data, then use the</span>
<span class="c1">// Data Source API to write the data back to another table in Azure Synapse.</span>

<span class="n">df</span><span class="p">.</span><span class="n">write</span>
<span class="w">  </span><span class="p">.</span><span class="n">format</span><span class="p">(</span><span class="s">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;url&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;forwardSparkAzureStorageCredentials&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;true&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;dbTable&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;your-table-name&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">&quot;tempDir&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">.</span><span class="n">save</span><span class="p">()</span>
</pre></div>
</div>
<div class="compound-middle highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="c1"># Set up the storage account access key in the notebook session conf.</span>
<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
  <span class="s2">&quot;fs.azure.account.key.&lt;your-storage-account-name&gt;.dfs.core.windows.net&quot;</span><span class="p">,</span>
  <span class="s2">&quot;&lt;your-storage-account-access-key&gt;&quot;</span><span class="p">)</span>

<span class="c1"># Get some data from an Azure Synapse table. The following example applies to Databricks Runtime 11.3 LTS and above.</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span>
  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;sqldw&quot;</span><span class="p">)</span>
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;host&quot;</span><span class="p">,</span> <span class="s2">&quot;hostname&quot;</span><span class="p">)</span>
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;port&quot;</span><span class="p">,</span> <span class="s2">&quot;port&quot;</span><span class="p">)</span> <span class="c1"># Optional - will use default port 1433 if not specified.</span>
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;username&quot;</span><span class="p">)</span>
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;password&quot;</span><span class="p">,</span> <span class="s2">&quot;password&quot;</span><span class="p">)</span>
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;database&quot;</span><span class="p">,</span> <span class="s2">&quot;database-name&quot;</span><span class="p">)</span>
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;dbtable&quot;</span><span class="p">,</span> <span class="s2">&quot;schema-name.table-name&quot;</span><span class="p">)</span> <span class="c1"># If schemaName not provided, default to &quot;dbo&quot;.</span>
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;tempDir&quot;</span><span class="p">,</span> <span class="s2">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span>
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;forwardSparkAzureStorageCredentials&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span>
  <span class="o">.</span><span class="n">load</span><span class="p">()</span>

<span class="c1"># Get some data from an Azure Synapse table. The following example applies to Databricks Runtime 10.4 LTS and below.</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span> \
  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;url&quot;</span><span class="p">,</span> <span class="s2">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;tempDir&quot;</span><span class="p">,</span> <span class="s2">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;forwardSparkAzureStorageCredentials&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;dbTable&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;your-table-name&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">load</span><span class="p">()</span>

<span class="c1"># Load data from an Azure Synapse query.</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span> \
  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;url&quot;</span><span class="p">,</span> <span class="s2">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;tempDir&quot;</span><span class="p">,</span> <span class="s2">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;forwardSparkAzureStorageCredentials&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;query&quot;</span><span class="p">,</span> <span class="s2">&quot;select x, count(*) as cnt from table group by x&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">load</span><span class="p">()</span>

<span class="c1"># Apply some transformations to the data, then use the</span>
<span class="c1"># Data Source API to write the data back to another table in Azure Synapse.</span>

<span class="n">df</span><span class="o">.</span><span class="n">write</span> \
  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;url&quot;</span><span class="p">,</span> <span class="s2">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;forwardSparkAzureStorageCredentials&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;dbTable&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;your-table-name&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;tempDir&quot;</span><span class="p">,</span> <span class="s2">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">save</span><span class="p">()</span>
</pre></div>
</div>
<div class="compound-middle highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="c1">-- Set up the storage account access key in the notebook session conf.</span>
<span class="k">SET</span><span class="w"> </span><span class="n">fs</span><span class="p">.</span><span class="n">azure</span><span class="p">.</span><span class="n">account</span><span class="p">.</span><span class="k">key</span><span class="p">.</span><span class="o">&lt;</span><span class="n">your</span><span class="o">-</span><span class="k">storage</span><span class="o">-</span><span class="n">account</span><span class="o">-</span><span class="n">name</span><span class="o">&gt;</span><span class="p">.</span><span class="n">dfs</span><span class="p">.</span><span class="n">core</span><span class="p">.</span><span class="n">windows</span><span class="p">.</span><span class="n">net</span><span class="o">=&lt;</span><span class="n">your</span><span class="o">-</span><span class="k">storage</span><span class="o">-</span><span class="n">account</span><span class="o">-</span><span class="k">access</span><span class="o">-</span><span class="k">key</span><span class="o">&gt;</span><span class="p">;</span>

<span class="c1">-- Read data using SQL. The following example applies to Databricks Runtime 11.3 LTS and above.</span>
<span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">example_table_in_spark_read</span>
<span class="k">USING</span><span class="w"> </span><span class="n">sqldw</span>
<span class="k">OPTIONS</span><span class="w"> </span><span class="p">(</span>
<span class="w">  </span><span class="k">host</span><span class="w"> </span><span class="s1">&#39;&lt;hostname&gt;&#39;</span><span class="p">,</span>
<span class="w">  </span><span class="n">port</span><span class="w"> </span><span class="s1">&#39;&lt;port&gt;&#39;</span><span class="w"> </span><span class="cm">/* Optional - will use default port 1433 if not specified. */</span>
<span class="w">  </span><span class="k">user</span><span class="w"> </span><span class="s1">&#39;&lt;username&gt;&#39;</span><span class="p">,</span>
<span class="w">  </span><span class="n">password</span><span class="w"> </span><span class="s1">&#39;&lt;password&gt;&#39;</span><span class="p">,</span>
<span class="w">  </span><span class="k">database</span><span class="w"> </span><span class="s1">&#39;&lt;database-name&gt;&#39;</span>
<span class="w">  </span><span class="n">dbtable</span><span class="w"> </span><span class="s1">&#39;&lt;schema-name&gt;.&lt;table-name&gt;&#39;</span><span class="p">,</span><span class="w"> </span><span class="cm">/* If schemaName not provided, default to &quot;dbo&quot;. */</span>
<span class="w">  </span><span class="n">forwardSparkAzureStorageCredentials</span><span class="w"> </span><span class="s1">&#39;true&#39;</span><span class="p">,</span>
<span class="w">  </span><span class="n">tempDir</span><span class="w"> </span><span class="s1">&#39;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&#39;</span>
<span class="p">);</span>

<span class="c1">-- Read data using SQL. The following example applies to Databricks Runtime 10.4 LTS and below.</span>
<span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">example_table_in_spark_read</span>
<span class="k">USING</span><span class="w"> </span><span class="n">com</span><span class="p">.</span><span class="n">databricks</span><span class="p">.</span><span class="n">spark</span><span class="p">.</span><span class="n">sqldw</span>
<span class="k">OPTIONS</span><span class="w"> </span><span class="p">(</span>
<span class="w">  </span><span class="n">url</span><span class="w"> </span><span class="s1">&#39;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&#39;</span><span class="p">,</span>
<span class="w">  </span><span class="n">forwardSparkAzureStorageCredentials</span><span class="w"> </span><span class="s1">&#39;true&#39;</span><span class="p">,</span>
<span class="w">  </span><span class="n">dbtable</span><span class="w"> </span><span class="s1">&#39;&lt;your-table-name&gt;&#39;</span><span class="p">,</span>
<span class="w">  </span><span class="n">tempDir</span><span class="w"> </span><span class="s1">&#39;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&#39;</span>
<span class="p">);</span>

<span class="c1">-- Write data using SQL.</span>
<span class="c1">-- Create a new table, throwing an error if a table with the same name already exists:</span>

<span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">example_table_in_spark_write</span>
<span class="k">USING</span><span class="w"> </span><span class="n">com</span><span class="p">.</span><span class="n">databricks</span><span class="p">.</span><span class="n">spark</span><span class="p">.</span><span class="n">sqldw</span>
<span class="k">OPTIONS</span><span class="w"> </span><span class="p">(</span>
<span class="w">  </span><span class="n">url</span><span class="w"> </span><span class="s1">&#39;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&#39;</span><span class="p">,</span>
<span class="w">  </span><span class="n">forwardSparkAzureStorageCredentials</span><span class="w"> </span><span class="s1">&#39;true&#39;</span><span class="p">,</span>
<span class="w">  </span><span class="n">dbTable</span><span class="w"> </span><span class="s1">&#39;&lt;your-table-name&gt;&#39;</span><span class="p">,</span>
<span class="w">  </span><span class="n">tempDir</span><span class="w"> </span><span class="s1">&#39;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&#39;</span>
<span class="p">)</span>
<span class="k">AS</span><span class="w"> </span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">table_to_save_in_spark</span><span class="p">;</span>
</pre></div>
</div>
<div class="compound-last highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load SparkR</span>
<span class="nf">library</span><span class="p">(</span><span class="n">SparkR</span><span class="p">)</span>

<span class="c1"># Set up the storage account access key in the notebook session conf.</span>
<span class="n">conf</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="nf">sparkR.session</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;conf&quot;</span><span class="p">)</span>
<span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;set&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;fs.azure.account.key.&lt;your-storage-account-name&gt;.dfs.core.windows.net&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;your-storage-account-access-key&gt;&quot;</span><span class="p">)</span>

<span class="c1"># Get some data from an Azure Synapse table.</span>
<span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">read.df</span><span class="p">(</span>
<span class="w">   </span><span class="n">source</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">url</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">forward_spark_azure_storage_credentials</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;true&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">dbTable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&lt;your-table-name&gt;&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">tempDir</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span>

<span class="c1"># Load data from an Azure Synapse query.</span>
<span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">read.df</span><span class="p">(</span>
<span class="w">   </span><span class="n">source</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">url</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">forward_spark_azure_storage_credentials</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;true&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">query</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;select x, count(*) as cnt from table group by x&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="n">tempDir</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span>

<span class="c1"># Apply some transformations to the data, then use the</span>
<span class="c1"># Data Source API to write the data back to another table in Azure Synapse.</span>

<span class="nf">write.df</span><span class="p">(</span>
<span class="w">  </span><span class="n">df</span><span class="p">,</span>
<span class="w">  </span><span class="n">source</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;com.databricks.spark.sqldw&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="n">url</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="n">forward_spark_azure_storage_credentials</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;true&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="n">dbTable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&lt;your-table-name&gt;&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="n">tempDir</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;abfss://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.dfs.core.windows.net/&lt;your-directory-name&gt;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="how-does-authentication-between-databricks-and-synapse-work">
<span id="authentication"></span><h2>How does authentication between Databricks and Synapse work?<a class="headerlink" href="#how-does-authentication-between-databricks-and-synapse-work" title="Permalink to this headline"> </a></h2>
<p>The Azure Synapse connector uses three types of network connections:</p>
<ul class="simple">
<li><p>Spark driver to Azure Synapse</p></li>
<li><p>Spark cluster to Azure storage account</p></li>
<li><p>Azure Synapse to Azure storage account</p></li>
</ul>
</div>
<div class="section" id="configuring-access-to-azure-storage">
<h2>Configuring access to Azure storage<a class="headerlink" href="#configuring-access-to-azure-storage" title="Permalink to this headline"> </a></h2>
<p>Both Databricks and Synapse need privileged access to an Azure storage account to be used for temporary data storage.</p>
<p>Azure Synapse does not support using SAS for storage account access. You can configure access for both services by doing one of the following:</p>
<ul class="simple">
<li><p>Use the account key and secret for the storage account and set <code class="docutils literal notranslate"><span class="pre">forwardSparkAzureStorageCredentials</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code>. See <a class="reference internal" href="../storage/azure-storage.html#set-spark-properties-to-configure-azure-credentials-to-access-azure-storage"><span class="std std-ref">Set Spark properties to configure Azure credentials to access Azure storage</span></a>.</p></li>
<li><p>Use Azure Data Lake Storage Gen2 with <a class="reference internal" href="#service-principal"><span class="std std-ref">OAuth 2.0 authentication</span></a>.</p></li>
<li><p>Configure your Azure Synapse instance to have a Managed Service Identity.</p></li>
</ul>
</div>
<div class="section" id="required-azure-synapse-permissions">
<h2>Required Azure Synapse permissions<a class="headerlink" href="#required-azure-synapse-permissions" title="Permalink to this headline"> </a></h2>
<p>Because it uses <code class="docutils literal notranslate"><span class="pre">COPY</span></code> in the background, the Azure Synapse connector requires the JDBC connection user to have permission to run the following commands in the connected Azure Synapse instance:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://learn.microsoft.com/sql/t-sql/statements/copy-into-transact-sql">COPY INTO</a></p></li>
</ul>
<p>If the destination table does not exist in Azure Synapse, permission to run the following command is required in addition to the command above:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://learn.microsoft.com/sql/t-sql/statements/create-table-azure-sql-data-warehouse">CREATE TABLE</a></p></li>
</ul>
<p>The following table summarizes the permissions required for writes with <code class="docutils literal notranslate"><span class="pre">COPY</span></code>:</p>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Permissions (insert into an existing table)</p></th>
<th class="head"><p>Permissions (insert into a new table)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ADMINISTER DATABASE BULK OPERATIONS</p>
<p>INSERT</p>
</td>
<td><p>ADMINISTER DATABASE BULK OPERATIONS</p>
<p>INSERT</p>
<p>CREATE TABLE</p>
<p>ALTER ON SCHEMA :: dbo</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="configure-connection-from-databricks-to-synapse-with-oauth-20-with-a-service-principal">
<span id="service-principal"></span><h2>Configure connection from Databricks to Synapse with OAuth 2.0 with a service principal<a class="headerlink" href="#configure-connection-from-databricks-to-synapse-with-oauth-20-with-a-service-principal" title="Permalink to this headline"> </a></h2>
<p>You can authenticate to Azure Synapse Analytics using a service principal with access to the underlying storage account. For more information on using service principal credentials to access an Azure storage account, see <a class="reference internal" href="../storage/azure-storage.html"><span class="doc">Connect to Azure Data Lake Storage Gen2 and Blob Storage</span></a>. You must set the <code class="docutils literal notranslate"><span class="pre">enableServicePrincipalAuth</span></code> option to <code class="docutils literal notranslate"><span class="pre">true</span></code> in the connection configuration <a class="reference internal" href="#parameters"><span class="std std-ref">Databricks Synapse connector options reference</span></a> to enable the connector to authenticate with a service principal.</p>
<p>You can optionally use a different service principal for the Azure Synapse Analytics connection. The following example configures service principal credentials for the storage account and optional service principal credentials for Synapse:</p>
<div class="js-code-language-tabs js-code-language-tabs--literal compound">
<div class="compound-first highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="c1">; Defining the Service Principal credentials for the Azure storage account</span>
<span class="na">fs.azure.account.auth.type OAuth</span>
<span class="na">fs.azure.account.oauth.provider.type org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider</span>
<span class="na">fs.azure.account.oauth2.client.id &lt;application-id&gt;</span>
<span class="na">fs.azure.account.oauth2.client.secret &lt;service-credential&gt;</span>
<span class="na">fs.azure.account.oauth2.client.endpoint https</span><span class="o">:</span><span class="s">//login.microsoftonline.com/&lt;directory-id&gt;/oauth2/token</span>

<span class="c1">; Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials)</span>
<span class="na">spark.databricks.sqldw.jdbc.service.principal.client.id &lt;application-id&gt;</span>
<span class="na">spark.databricks.sqldw.jdbc.service.principal.client.secret &lt;service-credential&gt;</span>
</pre></div>
</div>
<div class="compound-middle highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="c1">// Defining the Service Principal credentials for the Azure storage account</span>
<span class="n">spark</span><span class="p">.</span><span class="n">conf</span><span class="p">.</span><span class="n">set</span><span class="p">(</span><span class="s">&quot;fs.azure.account.auth.type&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;OAuth&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="p">.</span><span class="n">conf</span><span class="p">.</span><span class="n">set</span><span class="p">(</span><span class="s">&quot;fs.azure.account.oauth.provider.type&quot;</span><span class="p">,</span><span class="w">  </span><span class="s">&quot;org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="p">.</span><span class="n">conf</span><span class="p">.</span><span class="n">set</span><span class="p">(</span><span class="s">&quot;fs.azure.account.oauth2.client.id&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;application-id&gt;&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="p">.</span><span class="n">conf</span><span class="p">.</span><span class="n">set</span><span class="p">(</span><span class="s">&quot;fs.azure.account.oauth2.client.secret&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;service-credential&gt;&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="p">.</span><span class="n">conf</span><span class="p">.</span><span class="n">set</span><span class="p">(</span><span class="s">&quot;fs.azure.account.oauth2.client.endpoint&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;https://login.microsoftonline.com/&lt;directory-id&gt;/oauth2/token&quot;</span><span class="p">)</span>

<span class="c1">// Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials)</span>
<span class="n">spark</span><span class="p">.</span><span class="n">conf</span><span class="p">.</span><span class="n">set</span><span class="p">(</span><span class="s">&quot;spark.databricks.sqldw.jdbc.service.principal.client.id&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;application-id&gt;&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="p">.</span><span class="n">conf</span><span class="p">.</span><span class="n">set</span><span class="p">(</span><span class="s">&quot;spark.databricks.sqldw.jdbc.service.principal.client.secret&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;service-credential&gt;&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="compound-middle highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Defining the service principal credentials for the Azure storage account</span>
<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;fs.azure.account.auth.type&quot;</span><span class="p">,</span> <span class="s2">&quot;OAuth&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;fs.azure.account.oauth.provider.type&quot;</span><span class="p">,</span>  <span class="s2">&quot;org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;fs.azure.account.oauth2.client.id&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;application-id&gt;&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;fs.azure.account.oauth2.client.secret&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;service-credential&gt;&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;fs.azure.account.oauth2.client.endpoint&quot;</span><span class="p">,</span> <span class="s2">&quot;https://login.microsoftonline.com/&lt;directory-id&gt;/oauth2/token&quot;</span><span class="p">)</span>

<span class="c1"># Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.databricks.sqldw.jdbc.service.principal.client.id&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;application-id&gt;&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.databricks.sqldw.jdbc.service.principal.client.secret&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;service-credential&gt;&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="compound-last highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load SparkR</span>
<span class="nf">library</span><span class="p">(</span><span class="n">SparkR</span><span class="p">)</span>
<span class="n">conf</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="nf">sparkR.session</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;conf&quot;</span><span class="p">)</span>

<span class="c1"># Defining the service principal credentials for the Azure storage account</span>
<span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;set&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;fs.azure.account.auth.type&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;OAuth&quot;</span><span class="p">)</span>
<span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;set&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;fs.azure.account.oauth.provider.type&quot;</span><span class="p">,</span><span class="w">  </span><span class="s">&quot;org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider&quot;</span><span class="p">)</span>
<span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;set&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;fs.azure.account.oauth2.client.id&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;application-id&gt;&quot;</span><span class="p">)</span>
<span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;set&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;fs.azure.account.oauth2.client.secret&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;service-credential&gt;&quot;</span><span class="p">)</span>
<span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;set&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;fs.azure.account.oauth2.client.endpoint&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;https://login.microsoftonline.com/&lt;directory-id&gt;/oauth2/token&quot;</span><span class="p">)</span>

<span class="c1"># Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials)</span>
<span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;set&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;spark.databricks.sqldw.jdbc.service.principal.client.id&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;application-id&gt;&quot;</span><span class="p">)</span>
<span class="nf">sparkR.callJMethod</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;set&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;spark.databricks.sqldw.jdbc.service.principal.client.secret&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;service-credential&gt;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="supported-save-modes-for-batch-writes">
<h2>Supported save modes for batch writes<a class="headerlink" href="#supported-save-modes-for-batch-writes" title="Permalink to this headline"> </a></h2>
<p>The Azure Synapse connector supports <code class="docutils literal notranslate"><span class="pre">ErrorIfExists</span></code>, <code class="docutils literal notranslate"><span class="pre">Ignore</span></code>, <code class="docutils literal notranslate"><span class="pre">Append</span></code>, and <code class="docutils literal notranslate"><span class="pre">Overwrite</span></code> save modes with the default mode being <code class="docutils literal notranslate"><span class="pre">ErrorIfExists</span></code>. For more information on supported save modes in Apache Spark, see <a class="reference external" href="https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes">Spark SQL documentation on Save Modes</a>.</p>
</div>
<div class="section" id="databricks-synapse-connector-options-reference">
<span id="parameters"></span><h2>Databricks Synapse connector options reference<a class="headerlink" href="#databricks-synapse-connector-options-reference" title="Permalink to this headline"> </a></h2>
<p>
The <code class="docutils literal notranslate"><span class="pre">OPTIONS</span></code> provided in Spark SQL support the following settings:</p>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 33%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 44%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Required</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">dbTable</span></code></p></td>
<td><p>Yes, unless <code class="docutils literal notranslate"><span class="pre">query</span></code> is specified</p></td>
<td><p>No default</p></td>
<td><p>The table to create or read from in Azure Synapse. This parameter is required
when saving data back to Azure Synapse.</p>
<p>You can also use <code class="docutils literal notranslate"><span class="pre">{SCHEMA</span> <span class="pre">NAME}.{TABLE</span> <span class="pre">NAME}</span></code> to access a table in a given
schema. If schema name is not provided, the default schema associated with the
JDBC user is used.</p>
<p>The previously supported <code class="docutils literal notranslate"><span class="pre">dbtable</span></code> variant is deprecated and will be ignored in future releases.
Use the “camel case” name instead.</p>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">query</span></code></p></td>
<td><p>Yes, unless <code class="docutils literal notranslate"><span class="pre">dbTable</span></code> is specified</p></td>
<td><p>No default</p></td>
<td><p>The query to read from in Azure Synapse.</p>
<p>For tables referred in the query, you can also use <code class="docutils literal notranslate"><span class="pre">{SCHEMA</span> <span class="pre">NAME}.{TABLE</span> <span class="pre">NAME}</span></code>
to access a table in a given schema.
If schema name is not provided, the default schema associated with the
JDBC user is used.</p>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">user</span></code></p></td>
<td><p>No</p></td>
<td><p>No default</p></td>
<td><p>The Azure Synapse username. Must be used in tandem with <code class="docutils literal notranslate"><span class="pre">password</span></code> option.
Can only be used if the user and password are not passed in the URL.
Passing both will result in an error.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">password</span></code></p></td>
<td><p>No</p></td>
<td><p>No default</p></td>
<td><p>The Azure Synapse password. Must be used in tandem with <code class="docutils literal notranslate"><span class="pre">user</span></code> option.
Can only be used if the user and password are not passed in the URL.
Passing both will result in an error.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">url</span></code></p></td>
<td><p>Yes</p></td>
<td><p>No default</p></td>
<td><p>A JDBC URL with <code class="docutils literal notranslate"><span class="pre">sqlserver</span></code> set as the subprotocol. It is recommended
to use the connection string provided by Azure portal. Setting
<code class="docutils literal notranslate"><span class="pre">encrypt=true</span></code> is strongly recommended, because it enables SSL
encryption of the JDBC connection. If <code class="docutils literal notranslate"><span class="pre">user</span></code> and <code class="docutils literal notranslate"><span class="pre">password</span></code>
are set separately, you do not need to include them in the URL.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">jdbcDriver</span></code></p></td>
<td><p>No</p></td>
<td><p>Determined by the JDBC URL’s subprotocol</p></td>
<td><p>The class name of the JDBC driver to use. This class must be on the classpath. In most cases,
it should not be necessary to specify this option, as the appropriate driver classname should
automatically be determined by the JDBC URL’s subprotocol.</p>
<p>The previously supported <code class="docutils literal notranslate"><span class="pre">jdbc_driver</span></code> variant is deprecated and will be ignored in future releases.
Use the “camel case” name instead.</p>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">tempDir</span></code></p></td>
<td><p>Yes</p></td>
<td><p>No default</p></td>
<td><p>A <code class="docutils literal notranslate"><span class="pre">abfss</span></code> URI. We recommend you use a dedicated Blob storage container for the Azure Synapse.</p>
<p>The previously supported <code class="docutils literal notranslate"><span class="pre">tempdir</span></code> variant is deprecated and will be ignored in future releases.
Use the “camel case” name instead.</p>
<p>You cannot use an <a class="reference internal" href="../unity-catalog/external-locations.html"><span class="doc">External location defined in Unity Catalog</span></a> as a <code class="docutils literal notranslate"><span class="pre">tempDir</span></code> location.</p>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">tempCompression</span></code></p></td>
<td><p>No</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">SNAPPY</span></code></p></td>
<td><p>The compression algorithm to be used to encode/decode temporary by both Spark and Azure Synapse. Currently supported values are: <code class="docutils literal notranslate"><span class="pre">UNCOMPRESSED</span></code>, <code class="docutils literal notranslate"><span class="pre">SNAPPY</span></code> and <code class="docutils literal notranslate"><span class="pre">GZIP</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">forwardSparkAzureStorageCredentials</span></code></p></td>
<td><p>No</p></td>
<td><p>false</p></td>
<td><p>If <code class="docutils literal notranslate"><span class="pre">true</span></code>, the library automatically discovers the credentials that Spark is using
to connect to the Blob storage container and forwards those credentials to Azure Synapse over JDBC. These credentials
are sent as part of the JDBC query. Therefore it is strongly recommended that you enable SSL
encryption of the JDBC connection when you use this option.</p>
<p>When configuring storage authentication, you must set exactly one of <code class="docutils literal notranslate"><span class="pre">useAzureMSI</span></code> and <code class="docutils literal notranslate"><span class="pre">forwardSparkAzureStorageCredentials</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code>.
Alternatively, you can set <code class="docutils literal notranslate"><span class="pre">enableServicePrincipalAuth</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code> and use service principal for both JDBC and storage authentication.</p>
<p>The previously supported <code class="docutils literal notranslate"><span class="pre">forward_spark_azure_storage_credentials</span></code> variant is deprecated and will be ignored in future releases.
Use the “camel case” name instead.</p>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">useAzureMSI</span></code></p></td>
<td><p>No</p></td>
<td><p>false</p></td>
<td><p>If <code class="docutils literal notranslate"><span class="pre">true</span></code>, the library will specify <code class="docutils literal notranslate"><span class="pre">IDENTITY</span> <span class="pre">=</span> <span class="pre">'Managed</span> <span class="pre">Service</span> <span class="pre">Identity'</span></code> and no <code class="docutils literal notranslate"><span class="pre">SECRET</span></code>
for the database scoped credentials it creates.</p>
<p>When configuring storage authentication, you must set exactly one of <code class="docutils literal notranslate"><span class="pre">useAzureMSI</span></code> and <code class="docutils literal notranslate"><span class="pre">forwardSparkAzureStorageCredentials</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code>.
Alternatively, you can set <code class="docutils literal notranslate"><span class="pre">enableServicePrincipalAuth</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code> and use service principal for both JDBC and storage authentication.</p>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">enableServicePrincipalAuth</span></code></p></td>
<td><p>No</p></td>
<td><p>false</p></td>
<td><p>If <code class="docutils literal notranslate"><span class="pre">true</span></code>, the library will use the provided service principal credentials to connect to the Azure storage account and Azure Synapse Analytics over JDBC.</p>
<p>If either <code class="docutils literal notranslate"><span class="pre">forward_spark_azure_storage_credentials</span></code> or <code class="docutils literal notranslate"><span class="pre">useAzureMSI</span></code> is set to <code class="docutils literal notranslate"><span class="pre">true</span></code>, that option would take precedence over service principal in storage authentication.</p>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">tableOptions</span></code></p></td>
<td><p>No</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CLUSTERED</span> <span class="pre">COLUMNSTORE</span> <span class="pre">INDEX</span></code>, <code class="docutils literal notranslate"><span class="pre">DISTRIBUTION</span> <span class="pre">=</span> <span class="pre">ROUND_ROBIN</span></code></p></td>
<td><p>A string used to specify <a class="reference external" href="https://learn.microsoft.com/sql/t-sql/statements/create-table-azure-sql-data-warehouse">table options</a>
when creating the Azure Synapse table set through <code class="docutils literal notranslate"><span class="pre">dbTable</span></code>.
This string is passed literally to the <code class="docutils literal notranslate"><span class="pre">WITH</span></code> clause of the <code class="docutils literal notranslate"><span class="pre">CREATE</span> <span class="pre">TABLE</span></code> SQL statement
that is issued against Azure Synapse.</p>
<p>The previously supported <code class="docutils literal notranslate"><span class="pre">table_options</span></code> variant is deprecated and will be ignored in future releases.
Use the “camel case” name instead.</p>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">preActions</span></code></p></td>
<td><p>No</p></td>
<td><p>No default (empty string)</p></td>
<td><p>A <code class="docutils literal notranslate"><span class="pre">;</span></code> separated list of SQL commands to be executed in Azure Synapse before writing data
to the Azure Synapse instance. These SQL commands are required to be valid commands accepted by Azure Synapse.</p>
<p>If any of these commands fail, it is treated as an error and the write operation
is not executed.</p>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">postActions</span></code></p></td>
<td><p>No</p></td>
<td><p>No default (empty string)</p></td>
<td><p>A <code class="docutils literal notranslate"><span class="pre">;</span></code> separated list of SQL commands to be executed in Azure Synapse
after the connector successfully writes data to the Azure Synapse instance.
These SQL commands are required to be valid commands accepted by Azure Synapse.</p>
<p>If any of these commands fail, it is treated as an error and
you’ll get an exception after the data is successfully written to the Azure Synapse instance.</p>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">maxStrLength</span></code></p></td>
<td><p>No</p></td>
<td><p>256</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">StringType</span></code> in Spark is mapped to the <code class="docutils literal notranslate"><span class="pre">NVARCHAR(maxStrLength)</span></code> type in Azure Synapse. You can use <code class="docutils literal notranslate"><span class="pre">maxStrLength</span></code>
to set the string length for all <code class="docutils literal notranslate"><span class="pre">NVARCHAR(maxStrLength)</span></code> type columns that are in the table with name
<code class="docutils literal notranslate"><span class="pre">dbTable</span></code> in Azure Synapse.</p>
<p>The previously supported <code class="docutils literal notranslate"><span class="pre">maxstrlength</span></code> variant is deprecated and will be ignored in future releases.
Use the “camel case” name instead.</p>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">applicationName</span></code></p></td>
<td><p>No</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Databricks-User-Query</span></code></p></td>
<td><p>The tag of the connection for each query. If not specified or the value is an empty string,
the default value of the tag is added the JDBC URL. The default value prevents the Azure DB
Monitoring tool from raising spurious SQL injection alerts against queries.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">maxbinlength</span></code></p></td>
<td><p>No</p></td>
<td><p>No default</p></td>
<td><p>Control the column length of <code class="docutils literal notranslate"><span class="pre">BinaryType</span></code> columns. This parameter is translated as <code class="docutils literal notranslate"><span class="pre">VARBINARY(maxbinlength)</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">identityInsert</span></code></p></td>
<td><p>No</p></td>
<td><p>false</p></td>
<td><p>Setting to <code class="docutils literal notranslate"><span class="pre">true</span></code> enables <code class="docutils literal notranslate"><span class="pre">IDENTITY_INSERT</span></code> mode, which inserts a DataFrame provided value in the identity column of the Azure Synapse table.</p>
<p>See <a class="reference external" href="https://learn.microsoft.com/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-identity#explicitly-inserting-values-into-an-identity-column">Explicitly inserting values into an IDENTITY column</a>.</p>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">externalDataSource</span></code></p></td>
<td><p>No</p></td>
<td><p>No default</p></td>
<td><p>A pre-provisioned external data source to read data from Azure Synapse. An external data source can only be used with PolyBase and removes the CONTROL permission requirement because the connector does not need to create a scoped credential and an external data source to load data.</p>
<p>For example usage and the list of permissions required when using an external data source, see <a class="reference internal" href="../../archive/azure/synapse-polybase.html#dw-polybase-external-data-source-permissions"><span class="std std-ref">Required Azure Synapse permissions for PolyBase with the external data source option</span></a>.</p>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">maxErrors</span></code></p></td>
<td><p>No</p></td>
<td><p>0</p></td>
<td><p>The maximum number of rows that can be rejected during reads and writes before the loading operation is cancelled. The rejected rows will be ignored. For example, if two out of ten records have errors, only eight records will be processed.</p>
<p>See <a class="reference external" href="https://learn.microsoft.com/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&amp;tabs=dedicated#reject_value--reject_value-1">REJECT_VALUE documentation in CREATE EXTERNAL TABLE</a> and <a class="reference external" href="https://learn.microsoft.com/sql/t-sql/statements/copy-into-transact-sql?view=azure-sqldw-latest#maxerrors--max_errors">MAXERRORS documentation in COPY</a>.</p>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">inferTimestampNTZType</span></code></p></td>
<td><p>No</p></td>
<td><p>false</p></td>
<td><p>If <code class="docutils literal notranslate"><span class="pre">true</span></code>, values of type Azure Synapse <code class="docutils literal notranslate"><span class="pre">TIMESTAMP</span></code> are interpreted as <code class="docutils literal notranslate"><span class="pre">TimestampNTZType</span></code> (timestamp without time zone) during reads. Otherwise, all timestamps are interpreted as <code class="docutils literal notranslate"><span class="pre">TimestampType</span></code> regardless of the type in the underlying Azure Synapse table.</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tableOptions</span></code>, <code class="docutils literal notranslate"><span class="pre">preActions</span></code>, <code class="docutils literal notranslate"><span class="pre">postActions</span></code>, and <code class="docutils literal notranslate"><span class="pre">maxStrLength</span></code> are relevant only when writing data from Databricks to a new table in Azure Synapse.</p></li>
<li><p>Even though all data source option names are case-insensitive, we recommend that you specify them in “camel case” for clarity.</p></li>
</ul>
</div>
<p></p>
</div>
<div class="section" id="query-pushdown-into-azure-synapse">
<span id="sql-dw-query-pushdown"></span><h2>Query pushdown into Azure Synapse<a class="headerlink" href="#query-pushdown-into-azure-synapse" title="Permalink to this headline"> </a></h2>
<p>The Azure Synapse connector implements a set of optimization rules
to push the following operators down into Azure Synapse:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Filter</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Project</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Limit</span></code></p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">Project</span></code> and <code class="docutils literal notranslate"><span class="pre">Filter</span></code> operators support the following expressions:</p>
<ul class="simple">
<li><p>Most boolean logic operators</p></li>
<li><p>Comparisons</p></li>
<li><p>Basic arithmetic operations</p></li>
<li><p>Numeric and string casts</p></li>
</ul>
<p>For the <code class="docutils literal notranslate"><span class="pre">Limit</span></code> operator, pushdown is supported only when there is no ordering specified. For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">SELECT</span> <span class="pre">TOP(10)</span> <span class="pre">*</span> <span class="pre">FROM</span> <span class="pre">table</span></code>, but not <code class="docutils literal notranslate"><span class="pre">SELECT</span> <span class="pre">TOP(10)</span> <span class="pre">*</span> <span class="pre">FROM</span> <span class="pre">table</span> <span class="pre">ORDER</span> <span class="pre">BY</span> <span class="pre">col</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Azure Synapse connector does not push down expressions operating on strings, dates, or timestamps.</p>
</div>
<p>Query pushdown built with the Azure Synapse connector is enabled by default. You can disable it by setting <code class="docutils literal notranslate"><span class="pre">spark.databricks.sqldw.pushdown</span></code> to <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p>
</div>
<div class="section" id="temporary-data-management">
<span id="temp-data-mgmt"></span><h2>Temporary data management<a class="headerlink" href="#temporary-data-management" title="Permalink to this headline"> </a></h2>
<p>The Azure Synapse connector <em>does not</em> delete the temporary files that it creates in the Azure storage container. Databricks recommends that you periodically delete temporary files under the user-supplied <code class="docutils literal notranslate"><span class="pre">tempDir</span></code> location.</p>
<p>To facilitate data cleanup, the Azure Synapse connector does not store data files directly under <code class="docutils literal notranslate"><span class="pre">tempDir</span></code>, but instead creates a subdirectory of the form: <code class="docutils literal notranslate"><span class="pre">&lt;tempDir&gt;/&lt;yyyy-MM-dd&gt;/&lt;HH-mm-ss-SSS&gt;/&lt;randomUUID&gt;/</span></code>. You can set up periodic jobs (using the Databricks <a class="reference internal" href="../../workflows/jobs/create-run-jobs.html"><span class="doc">jobs</span></a> feature or otherwise) to recursively delete any subdirectories that are older than a given threshold (for example, 2 days), with the assumption that there cannot be Spark jobs running longer than that threshold.</p>
<p>A simpler alternative is to periodically drop the whole container and create a new one with the same name. This requires that you use a dedicated container for the temporary data produced by the Azure Synapse connector and that you can find a time window in which you can guarantee that no queries involving the connector are running.</p>
</div>
<div class="section" id="temporary-object-management">
<span id="temp-object-mgmt"></span><h2>Temporary object management<a class="headerlink" href="#temporary-object-management" title="Permalink to this headline"> </a></h2>
<p>The Azure Synapse connector automates data transfer between a Databricks cluster and an Azure Synapse instance. For reading data from an Azure Synapse table or query or writing data to an Azure Synapse table, the Azure Synapse connector creates temporary objects, including <code class="docutils literal notranslate"><span class="pre">DATABASE</span> <span class="pre">SCOPED</span> <span class="pre">CREDENTIAL</span></code>, <code class="docutils literal notranslate"><span class="pre">EXTERNAL</span> <span class="pre">DATA</span> <span class="pre">SOURCE</span></code>, <code class="docutils literal notranslate"><span class="pre">EXTERNAL</span> <span class="pre">FILE</span> <span class="pre">FORMAT</span></code>, and <code class="docutils literal notranslate"><span class="pre">EXTERNAL</span> <span class="pre">TABLE</span></code> behind the scenes. These objects live only throughout the duration of the corresponding Spark job and are automatically dropped.</p>
<p>When a cluster is running a query using the Azure Synapse connector, if the Spark driver process crashes or is forcefully restarted, or if the cluster is forcefully terminated or restarted, temporary objects might not be dropped. To facilitate identification and manual deletion of these objects, the Azure Synapse connector prefixes the names of all intermediate temporary objects created in the Azure Synapse instance with a tag of the form: <code class="docutils literal notranslate"><span class="pre">tmp_databricks_&lt;yyyy_MM_dd_HH_mm_ss_SSS&gt;_&lt;randomUUID&gt;_&lt;internalObject&gt;</span></code>.</p>
<p>We recommend that you periodically look for leaked objects using queries such as the following:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">SELECT</span> <span class="pre">*</span> <span class="pre">FROM</span> <span class="pre">sys.database_scoped_credentials</span> <span class="pre">WHERE</span> <span class="pre">name</span> <span class="pre">LIKE</span> <span class="pre">'tmp_databricks_%'</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SELECT</span> <span class="pre">*</span> <span class="pre">FROM</span> <span class="pre">sys.external_data_sources</span> <span class="pre">WHERE</span> <span class="pre">name</span> <span class="pre">LIKE</span> <span class="pre">'tmp_databricks_%'</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SELECT</span> <span class="pre">*</span> <span class="pre">FROM</span> <span class="pre">sys.external_file_formats</span> <span class="pre">WHERE</span> <span class="pre">name</span> <span class="pre">LIKE</span> <span class="pre">'tmp_databricks_%'</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SELECT</span> <span class="pre">*</span> <span class="pre">FROM</span> <span class="pre">sys.external_tables</span> <span class="pre">WHERE</span> <span class="pre">name</span> <span class="pre">LIKE</span> <span class="pre">'tmp_databricks_%'</span></code></p></li>
</ul>
</div>
</div>


    
          </div>
        </div>
        <div  class="suapp-rating">
  <div id="suPageRateApp">
     <su-app></su-app>
   </div> 
 </div>
<hr> 
<footer>
  <div role="contentinfo">
      <p class="copyright">
          &copy; Databricks 2023. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the <a href="http://www.apache.org/">Apache Software Foundation</a>.
      </p>
      <p> 
        
          <a id='feedbacklink' href="mailto:doc-feedback@databricks.com?subject=Documentation Feedback">Send us feedback</a>
        
     | <a href="https://databricks.com/privacy-policy">Privacy Policy</a> | <a href="https://databricks.com/terms-of-use">Terms of Use</a></p>

  </div> 

</footer>
      </div>
    </div>
  </section>
</main>

  </page>
  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT: '../../',
      VERSION: '1.0',
      COLLAPSE_INDEX: false,
      FILE_SUFFIX: '.html',
      HAS_SOURCE: 'false'
    };
  </script>
  <script type="text/javascript" src="../../_static/jquery.js"></script>
  <script type="text/javascript" src="../../_static/underscore.js"></script>
  <script type="text/javascript" src="../../_static/doctools.js"></script>
  <script type="text/javascript" src="../../_static/language_data.js"></script>
  

  <script type="text/javascript" src="../../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../../_static/js/jquery.waypoints.min.js"></script>

  <!-- Select2 (https://select2.org/) -->
  <link href="https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/css/select2.min.css" rel="stylesheet" />
  <script src="https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/js/select2.min.js"></script>
  <!-- End Select2 -->

  
  
  <script type="text/javascript" src="../../_static/js/localized.js"></script>
  <script type="text/javascript" src="../../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  
 



  <script>
  window.__searchunifyLoaderConfig = JSON.parse('{"clients": {"en": "02c2e804-27e9-11ee-aefb-0242ac120011", "ja": "6a42c3f2-2820-11ee-aefb-0242ac120011", "pt": "6a86badd-2821-11ee-aefb-0242ac120011"}}')
</script>
<script type="text/javascript" src="../../_static/js/search-loader.js"></script>
</body>
<script type='text/javascript'>
  window.onload = function () {
    var description = document.querySelector('meta[name="description"]').getAttribute("content");
    let titleText = document.querySelector('h1').textContent;
    document.querySelector('meta[property="og:title"]').setAttribute("content", titleText);
    document.querySelector('meta[property="og:description"]').setAttribute("content", description);
    document.querySelector('meta[property="twitter:description"]').setAttribute("content", description);
  };
</script>

</html>